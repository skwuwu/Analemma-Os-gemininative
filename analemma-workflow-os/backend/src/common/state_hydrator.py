"""
ğŸš€ StateHydrator - Smart StateBag Architecture
================================================

Analemma OSì˜ í•µì‹¬ ì„±ëŠ¥ ìµœì í™” ëª¨ë“ˆ.
14ë§Œ ì¤„ ì»¤ë„ì—ì„œ ë°œìƒí•˜ëŠ” ì§ë ¬í™”/ì—­ì§ë ¬í™” ì˜¤ë²„í—¤ë“œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.

## í•µì‹¬ ì „ëµ

1. **Control Plane vs Data Plane ë¶„ë¦¬**
   - Control Plane (Small): ID, Status, Counters, Pointers â†’ SFN ì»¨í…ìŠ¤íŠ¸
   - Data Plane (Large): LLM ì‘ë‹µ, ëŒ€ìš©ëŸ‰ JSON â†’ S3 + Pointer

2. **On-demand Hydration (ìˆ˜ë¶„ ê³µê¸‰ íŒ¨í„´)**
   - Lambda ì…êµ¬ì—ì„œ í•„ìš”í•œ í•„ë“œê°€ 'í¬ì¸í„°'ë¼ë©´ S3ì—ì„œ ë¡œë“œ
   - ì²˜ë¦¬ ì™„ë£Œ í›„ S3ì— ë¤í”„í•˜ê³  í¬ì¸í„°ë§Œ ë°˜í™˜
   - SFN í˜ì´ë¡œë“œ í•­ìƒ 10KB ë¯¸ë§Œ ìœ ì§€

3. **Delta Updates (ë¸íƒ€ ì—…ë°ì´íŠ¸)**
   - ì „ì²´ StateBag ëŒ€ì‹  'ë³€ê²½ëœ í•„ë“œ'ë§Œ ë¦¬í„´
   - StateDataManagerê°€ ì¤‘ì•™ì—ì„œ ì·¨í•©í•˜ì—¬ ë§ˆìŠ¤í„° ì—…ë°ì´íŠ¸

Usage:
    from src.common.state_hydrator import StateHydrator, SmartStateBag
    
    # Lambda í•¸ë“¤ëŸ¬ ì‹œì‘
    hydrator = StateHydrator(bucket_name="execution-bucket")
    state = hydrator.hydrate(event)  # S3ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë§Œ ë¡œë“œ
    
    # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ìˆ˜í–‰
    state["llm_response"] = call_llm(...)
    
    # Lambda ë°˜í™˜ (ìë™ ì˜¤í”„ë¡œë“œ)
    return hydrator.dehydrate(
        state=state,
        owner_id=event.get('ownerId'),
        workflow_id=event.get('workflowId'),
        execution_id=event.get('execution_id')
    )  # í° í•„ë“œëŠ” S3ë¡œ, í¬ì¸í„°ë§Œ ë°˜í™˜

Author: Analemma OS Team
Version: 1.0.0
"""

from __future__ import annotations
import json
import hashlib
import logging
import os
import time
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Set, Tuple, Union, TypedDict, Literal
from functools import lru_cache

logger = logging.getLogger(__name__)


# ============================================================================
# ExecutionResult Interface (P0: ResultPath í†µì¼)
# ============================================================================
# ëª¨ë“  LambdaëŠ” ì´ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.
# Step Functionsì˜ ëª¨ë“  Taskì—ì„œ ResultPath: "$.execution_result"ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

class ThoughtSignature(TypedDict, total=False):
    """
    ğŸ§  Gemini 3 ì‚¬ê³  ê³¼ì • ì €ì¥ ê³µê°„
    
    ë¯¸ë˜ Gemini 3ì˜ thinking processë¥¼ ì´ êµ¬ì¡°ì— ì €ì¥í•©ë‹ˆë‹¤.
    ë°ì´í„° êµ¬ì¡°ê°€ ë¯¸ë¦¬ ì¤€ë¹„ë˜ì–´ ìˆì–´ í•„ë“œ ë¶ˆì¼ì¹˜ ë¬¸ì œë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    thinking_process: str  # ì‚¬ê³  ê³¼ì • í…ìŠ¤íŠ¸
    thought_steps: List[Dict[str, Any]]  # ë‹¨ê³„ë³„ ì‚¬ê³  ê³¼ì •
    confidence_score: float  # ì‹ ë¢°ë„ ì ìˆ˜ (0.0 ~ 1.0)
    reasoning_tokens: int  # ì‚¬ê³ ì— ì‚¬ìš©ëœ í† í° ìˆ˜
    model_version: str  # ì‚¬ìš©ëœ ëª¨ë¸ ë²„ì „
    timestamp: float  # ìƒì„± ì‹œê°„


class RoutingInfo(TypedDict, total=False):
    """
    ğŸš¦ Inter-segment ë¼ìš°íŒ… ì •ë³´
    
    partition_map.outgoing_edgesì—ì„œ ì¶”ì¶œí•œ ì •ë³´ë¥¼ ë‹´ìŠµë‹ˆë‹¤.
    """
    edge_type: str  # "normal", "hitp", "conditional", "loop_exit"
    is_loop_exit: bool
    is_back_edge: bool
    condition: Optional[str]
    target_node: str
    requires_hitp: bool


class ExecutionResult(TypedDict, total=False):
    """
    ğŸ“¦ í‘œì¤€ ì‹¤í–‰ ê²°ê³¼ ì¸í„°í˜ì´ìŠ¤ (P0: ëª¨ë“  Lambda í•„ìˆ˜ ì¤€ìˆ˜)
    
    Step Functionsì˜ ëª¨ë“  TaskëŠ” ì´ ì¸í„°í˜ì´ìŠ¤ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
    ResultPathëŠ” í•­ìƒ "$.execution_result"ë¡œ í†µì¼í•©ë‹ˆë‹¤.
    
    ì‚¬ìš© ì˜ˆì‹œ:
        return {
            "status": "CONTINUE",
            "final_state": {...},
            "final_state_s3_path": "s3://...",
            "next_segment_to_run": 3,
            "routing_info": {"requires_hitp": False}
        }
    """
    # í•„ìˆ˜ í•„ë“œ
    status: Literal[
        "CONTINUE",           # ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ê³„ì†
        "COMPLETE",           # ì›Œí¬í”Œë¡œìš° ì™„ë£Œ
        "PARALLEL_GROUP",     # ë³‘ë ¬ ê·¸ë£¹ ì‹¤í–‰ í•„ìš”
        "SEQUENTIAL_BRANCH",  # ìˆœì°¨ ë¸Œëœì¹˜ ì‹¤í–‰ í•„ìš”
        "PAUSE",              # ì¼ì‹œ ì •ì§€ (ì™¸ë¶€ ì´ë²¤íŠ¸ ëŒ€ê¸°)
        "PAUSED_FOR_HITP",    # HITP ëŒ€ê¸° ì¤‘
        "FAILED",             # ì‹¤íŒ¨
        "HALTED",             # ê°•ì œ ì¤‘ì§€
        "SIGKILL",            # ì¢…ë£Œ ì‹ í˜¸
        "SKIPPED"             # ìŠ¤í‚µë¨
    ]
    
    # ìƒíƒœ ë°ì´í„°
    final_state: Dict[str, Any]  # ì‹¤í–‰ í›„ ìƒíƒœ
    final_state_s3_path: Optional[str]  # S3 ì˜¤í”„ë¡œë“œ ì‹œ ê²½ë¡œ
    
    # ë¼ìš°íŒ… ì •ë³´
    next_segment_to_run: Optional[int]  # ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ID
    routing_info: Optional[RoutingInfo]  # Inter-segment ë¼ìš°íŒ… ì •ë³´
    
    # ë©”íƒ€ë°ì´í„°
    new_history_logs: List[str]  # ìƒˆ íˆìŠ¤í† ë¦¬ ë¡œê·¸
    error_info: Optional[Dict[str, Any]]  # ì—ëŸ¬ ì •ë³´
    segment_type: str  # "normal", "llm", "hitp", "aggregator" ë“±
    segment_id: Optional[int]  # í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ID
    total_segments: int  # ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜
    execution_time: float  # ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
    
    # ë³‘ë ¬ ì²˜ë¦¬ (Fork-Join íŒ¨í„´)
    branches: Optional[List[Dict[str, Any]]]  # PARALLEL_GROUPì¼ ë•Œ ë¸Œëœì¹˜ ì„¤ì •
    inner_partition_map: Optional[List[Dict[str, Any]]]  # SEQUENTIAL_BRANCHì¼ ë•Œ
    
    # Kernel ì•¡ì…˜
    kernel_actions: Optional[List[Dict[str, Any]]]  # ì¹´ë„ ì•¡ì…˜ ë¡œê·¸
    
    # ğŸ§  Gemini 3 ì‚¬ê³  ê³¼ì • (ë¯¸ë˜ ì˜ˆì•½)
    thought_signature: Optional[ThoughtSignature]  # LLM ì‚¬ê³  ê³¼ì • ì €ì¥


# ============================================================================
# Branch Result Interface (Fork-Join íŒ¨í„´)
# ============================================================================

class BranchResult(TypedDict, total=False):
    """
    ğŸ”€ ë³‘ë ¬ ë¸Œëœì¹˜ ê²°ê³¼ (Fork-Join íŒ¨í„´)
    
    parallel_groupì—ì„œ ê° ë¸Œëœì¹˜ëŠ” ì´ êµ¬ì¡°ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë³‘í•© ì¶©ëŒì„ ë°©ì§€í•˜ê¸° ìœ„í•´:
    1. ê° ë¸Œëœì¹˜ëŠ” branch_results ë°°ì—´ì— ê¸°ë¡
    2. Aggregator ë…¸ë“œì—ì„œ ë©”ì¸ state_bagì— ë³‘í•©
    """
    branch_id: str  # ë¸Œëœì¹˜ ì‹ë³„ì
    status: Literal["completed", "failed", "partial_failure"]
    final_state: Dict[str, Any]  # ë¸Œëœì¹˜ ìµœì¢… ìƒíƒœ
    final_state_s3_path: Optional[str]  # S3 ì˜¤í”„ë¡œë“œ ì‹œ
    loop_iterations: int  # ë£¨í”„ ë°˜ë³µ íšŸìˆ˜
    execution_time_ms: float  # ì‹¤í–‰ ì‹œê°„ (ms)
    error_info: Optional[Dict[str, Any]]  # ì—ëŸ¬ ì •ë³´
    thought_signature: Optional[ThoughtSignature]  # LLM ì‚¬ê³  ê³¼ì •

# ============================================================================
# Constants
# ============================================================================

# í˜ì´ë¡œë“œ ì„ê³„ê°’ (bytes)
CONTROL_PLANE_MAX_SIZE = 10 * 1024  # 10KB - SFN í˜ì´ë¡œë“œ ëª©í‘œ
DATA_PLANE_THRESHOLD = 50 * 1024   # 50KB - S3 ì˜¤í”„ë¡œë“œ íŠ¸ë¦¬ê±°
FIELD_OFFLOAD_THRESHOLD = 10 * 1024  # 10KB - ê°œë³„ í•„ë“œ ì˜¤í”„ë¡œë“œ íŠ¸ë¦¬ê±°

# Control Plane í•„ë“œ (í•­ìƒ SFN ì»¨í…ìŠ¤íŠ¸ì— ìœ ì§€)
CONTROL_PLANE_FIELDS = frozenset({
    # ì‹ë³„ì
    "ownerId", "workflowId", "idempotency_key", "execution_id",
    "quota_reservation_id",
    
    # ê²½ë¡œ í¬ì¸í„° (S3 ì°¸ì¡°)
    "workflow_config_s3_path", "state_s3_path", "partition_map_s3_path",
    "segment_manifest_s3_path", "final_state_s3_path",
    
    # ì¹´ìš´í„° ë° ìƒíƒœ
    "segment_to_run", "total_segments", "loop_counter",
    "max_loop_iterations", "max_branch_iterations", "max_concurrency",
    
    # ì „ëµ ë° ëª¨ë“œ
    "distributed_strategy", "distributed_mode", "MOCK_MODE",
    
    # ì„¸ê·¸ë¨¼íŠ¸ ë©”íƒ€ë°ì´í„°
    "llm_segments", "hitp_segments", "segment_type",
    
    # ë¼ì´íŠ¸ ì„¤ì • (ì¶•ì•½ëœ ë©”íƒ€ë°ì´í„°)
    "light_config"
})

# Data Plane í•„ë“œ (ìë™ S3 ì˜¤í”„ë¡œë“œ ëŒ€ìƒ)
DATA_PLANE_FIELDS = frozenset({
    "workflow_config", "partition_map", "segment_manifest",
    "current_state", "final_state", "state_history",
    "parallel_results", "branch_results", "callback_result",
    "llm_response", "query_results", "step_history", "messages",
    # ğŸ§  Gemini 3 ì‚¬ê³  ê³¼ì • (ëŒ€ìš©ëŸ‰ ê°€ëŠ¥)
    "thought_signature", "thinking_process", "thought_steps"
})

# í¬ì¸í„° ë§ˆì»¤
POINTER_MARKER = "__s3_pointer__"
DELTA_MARKER = "__delta_update__"


# ============================================================================
# Data Classes
# ============================================================================

@dataclass
class S3Pointer:
    """S3 ë°ì´í„° í¬ì¸í„° - ì‹¤ì œ ë°ì´í„° ëŒ€ì‹  ì°¸ì¡°ë§Œ ì €ì¥"""
    bucket: str
    key: str
    size_bytes: int
    checksum: str
    field_name: str
    created_at: float = field(default_factory=time.time)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            POINTER_MARKER: True,
            "bucket": self.bucket,
            "key": self.key,
            "size_bytes": self.size_bytes,
            "checksum": self.checksum,
            "field_name": self.field_name,
            "created_at": self.created_at
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> Optional["S3Pointer"]:
        if not data.get(POINTER_MARKER):
            return None
        return cls(
            bucket=data["bucket"],
            key=data["key"],
            size_bytes=data.get("size_bytes", 0),
            checksum=data.get("checksum", ""),
            field_name=data.get("field_name", ""),
            created_at=data.get("created_at", 0)
        )


@dataclass
class DeltaUpdate:
    """ë¸íƒ€ ì—…ë°ì´íŠ¸ - ë³€ê²½ëœ í•„ë“œë§Œ ì¶”ì """
    changed_fields: Dict[str, Any] = field(default_factory=dict)
    deleted_fields: Set[str] = field(default_factory=set)
    s3_pointers: Dict[str, S3Pointer] = field(default_factory=dict)
    timestamp: float = field(default_factory=time.time)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            DELTA_MARKER: True,
            "changed_fields": self.changed_fields,
            "deleted_fields": list(self.deleted_fields),
            "s3_pointers": {k: v.to_dict() for k, v in self.s3_pointers.items()},
            "timestamp": self.timestamp
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> Optional["DeltaUpdate"]:
        if not data.get(DELTA_MARKER):
            return None
        return cls(
            changed_fields=data.get("changed_fields", {}),
            deleted_fields=set(data.get("deleted_fields", [])),
            s3_pointers={
                k: S3Pointer.from_dict(v) 
                for k, v in data.get("s3_pointers", {}).items()
                if S3Pointer.from_dict(v) is not None
            },
            timestamp=data.get("timestamp", 0)
        )


# ============================================================================
# SmartStateBag - Hybrid Pointer Bag
# ============================================================================

class SmartStateBag(dict):
    """
    ğŸš€ Smart StateBag - Pointer-based State Management
    
    ì¼ë°˜ dictì²˜ëŸ¼ ì‚¬ìš©í•˜ë©´ì„œë„ ë‚´ë¶€ì ìœ¼ë¡œ:
    - í° í•„ë“œëŠ” ìë™ìœ¼ë¡œ í¬ì¸í„°ë¡œ ëŒ€ì²´
    - ë³€ê²½ ì‚¬í•­ ì¶”ì  (Delta Updates)
    - í•„ìš”í•  ë•Œë§Œ S3ì—ì„œ ë¡œë“œ (Lazy Loading)
    """
    
    def __init__(
        self, 
        initial_data: Optional[Dict[str, Any]] = None,
        hydrator: Optional["StateHydrator"] = None,
        track_changes: bool = True
    ):
        self._hydrator = hydrator
        self._track_changes = track_changes
        self._original_values: Dict[str, Any] = {}
        self._changed_fields: Set[str] = set()
        self._deleted_fields: Set[str] = set()
        self._lazy_fields: Dict[str, S3Pointer] = {}  # ì•„ì§ ë¡œë“œ ì•ˆ ëœ í•„ë“œ
        
        if initial_data:
            # í¬ì¸í„° í•„ë“œ ë¶„ë¦¬
            for key, value in initial_data.items():
                pointer = S3Pointer.from_dict(value) if isinstance(value, dict) else None
                if pointer:
                    self._lazy_fields[key] = pointer
                else:
                    super().__setitem__(key, self._wrap(value))
                    if track_changes:
                        self._original_values[key] = value
    
    def _wrap(self, value: Any) -> Any:
        """ì¤‘ì²© dictë¥¼ SmartStateBagìœ¼ë¡œ ë˜í•‘"""
        if isinstance(value, dict) and not isinstance(value, SmartStateBag):
            # í¬ì¸í„°ëŠ” ë˜í•‘í•˜ì§€ ì•ŠìŒ
            if value.get(POINTER_MARKER):
                return value
            return SmartStateBag(value, self._hydrator, track_changes=False)
        elif isinstance(value, list):
            return [self._wrap(item) for item in value if item is not None]
        return value
    
    def __setitem__(self, key: str, value: Any):
        # ë³€ê²½ ì¶”ì 
        if self._track_changes and key not in self._changed_fields:
            self._changed_fields.add(key)
        
        # lazy í•„ë“œì—ì„œ ì œê±° (ì‹¤ì œ ê°’ìœ¼ë¡œ ëŒ€ì²´ë¨)
        if key in self._lazy_fields:
            del self._lazy_fields[key]
        
        super().__setitem__(key, self._wrap(value))
    
    def __getitem__(self, key: str) -> Any:
        # Lazy Loading: í¬ì¸í„° í•„ë“œë©´ S3ì—ì„œ ë¡œë“œ
        if key in self._lazy_fields:
            pointer = self._lazy_fields[key]
            if self._hydrator:
                value = self._hydrator._load_from_s3(pointer)
                super().__setitem__(key, self._wrap(value))
                del self._lazy_fields[key]
                return super().__getitem__(key)
            else:
                # hydrator ì—†ìœ¼ë©´ í¬ì¸í„° ê·¸ëŒ€ë¡œ ë°˜í™˜
                return pointer.to_dict()
        
        return super().get(key)
    
    def get(self, key: str, default: Any = None) -> Any:
        """Safe get with lazy loading"""
        try:
            value = self.__getitem__(key)
            return value if value is not None else default
        except KeyError:
            return default
    
    def __contains__(self, key: object) -> bool:
        return super().__contains__(key) or key in self._lazy_fields
    
    def __delitem__(self, key: str):
        if self._track_changes:
            self._deleted_fields.add(key)
        if key in self._lazy_fields:
            del self._lazy_fields[key]
        else:
            super().__delitem__(key)
    
    def get_delta(self) -> DeltaUpdate:
        """ë³€ê²½ëœ í•„ë“œë§Œ ì¶”ì¶œ"""
        changed = {}
        for field_name in self._changed_fields:
            if field_name in self:
                changed[field_name] = self[field_name]
        
        return DeltaUpdate(
            changed_fields=changed,
            deleted_fields=self._deleted_fields.copy()
        )
    
    def has_changes(self) -> bool:
        """ë³€ê²½ ì‚¬í•­ ì¡´ì¬ ì—¬ë¶€"""
        return bool(self._changed_fields) or bool(self._deleted_fields)
    
    def get_control_plane(self) -> Dict[str, Any]:
        """Control Plane í•„ë“œë§Œ ì¶”ì¶œ (SFN í˜ì´ë¡œë“œìš©)"""
        result = {}
        for key in CONTROL_PLANE_FIELDS:
            if key in self:
                result[key] = super().get(key)
        return result
    
    def get_lazy_pointers(self) -> Dict[str, S3Pointer]:
        """ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì€ í¬ì¸í„° ëª©ë¡"""
        return self._lazy_fields.copy()
    
    def to_dict(self) -> Dict[str, Any]:
        """ì¼ë°˜ dictë¡œ ë³€í™˜ (lazy í•„ë“œëŠ” í¬ì¸í„°ë¡œ)"""
        result = dict(self)
        for key, pointer in self._lazy_fields.items():
            result[key] = pointer.to_dict()
        return result


# ============================================================================
# StateHydrator - Core Hydration Engine
# ============================================================================

class StateHydrator:
    """
    ğŸš€ State Hydrator - On-demand Data Loading & Smart Offloading
    
    Lambda í•¨ìˆ˜ì—ì„œ ì‚¬ìš©:
    1. hydrate(): ì´ë²¤íŠ¸ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë§Œ S3ì—ì„œ ë¡œë“œ
    2. dehydrate(): í° ë°ì´í„°ë¥¼ S3ë¡œ ì˜¤í”„ë¡œë“œí•˜ê³  í¬ì¸í„° ë°˜í™˜
    """
    
    def __init__(
        self,
        bucket_name: Optional[str] = None,
        s3_client: Optional[Any] = None,
        control_plane_max_size: int = CONTROL_PLANE_MAX_SIZE,
        field_offload_threshold: int = FIELD_OFFLOAD_THRESHOLD
    ):
        self._bucket = bucket_name or os.environ.get("EXECUTION_BUCKET", "")
        self._s3_client = s3_client
        self._control_plane_max_size = control_plane_max_size
        self._field_offload_threshold = field_offload_threshold
        self._cache: Dict[str, Any] = {}  # ì¸ë©”ëª¨ë¦¬ ìºì‹œ (Lambda ì¬ì‚¬ìš© ì‹œ)
    
    @property
    def s3_client(self):
        """Lazy S3 client initialization"""
        if self._s3_client is None:
            from src.common.aws_clients import get_s3_client
            self._s3_client = get_s3_client()
        return self._s3_client
    
    def hydrate(
        self,
        event: Dict[str, Any],
        fields_to_load: Optional[Set[str]] = None,
        eager_load: bool = False
    ) -> SmartStateBag:
        """
        ğŸš€ ì´ë²¤íŠ¸ë¥¼ SmartStateBagìœ¼ë¡œ ë³€í™˜ (On-demand Hydration)
        
        Args:
            event: Lambda ì´ë²¤íŠ¸ (SFN ë˜ëŠ” API Gateway)
            fields_to_load: ì¦‰ì‹œ ë¡œë“œí•  í•„ë“œ (Noneì´ë©´ lazy loading)
            eager_load: Trueë©´ ëª¨ë“  í¬ì¸í„° ì¦‰ì‹œ ë¡œë“œ
        
        Returns:
            SmartStateBag: Hydrated state bag
        """
        start_time = time.time()
        
        # state_data ì¶”ì¶œ (SFN ì»¨í…ìŠ¤íŠ¸)
        state_data = event.get("state_data", event)
        
        # SmartStateBag ìƒì„±
        bag = SmartStateBag(state_data, hydrator=self)
        
        # ì¦‰ì‹œ ë¡œë“œí•  í•„ë“œ ì²˜ë¦¬
        if fields_to_load or eager_load:
            fields = fields_to_load or set(bag.get_lazy_pointers().keys())
            for field_name in fields:
                if field_name in bag.get_lazy_pointers():
                    _ = bag[field_name]  # ì ‘ê·¼í•˜ë©´ ìë™ ë¡œë“œ
        
        elapsed = (time.time() - start_time) * 1000
        logger.debug(f"[StateHydrator] Hydrated in {elapsed:.2f}ms, "
                    f"lazy_fields={len(bag.get_lazy_pointers())}")
        
        return bag
    
    def dehydrate(
        self,
        state: SmartStateBag,
        owner_id: str,
        workflow_id: str,
        execution_id: str,
        segment_id: Optional[int] = None,
        force_offload_fields: Optional[Set[str]] = None,
        return_delta: bool = True
    ) -> Dict[str, Any]:
        """
        ğŸš€ SmartStateBagì„ SFN ë°˜í™˜ìš©ìœ¼ë¡œ ë³€í™˜ (Dehydration)
        
        í° í•„ë“œëŠ” S3ë¡œ ì˜¤í”„ë¡œë“œí•˜ê³  í¬ì¸í„°ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            state: SmartStateBag ì¸ìŠ¤í„´ìŠ¤
            owner_id: ì†Œìœ ì ID
            workflow_id: ì›Œí¬í”Œë¡œìš° ID
            execution_id: ì‹¤í–‰ ID
            segment_id: ì„¸ê·¸ë¨¼íŠ¸ ID (optional)
            force_offload_fields: ê°•ì œë¡œ S3 ì˜¤í”„ë¡œë“œí•  í•„ë“œ
            return_delta: Trueë©´ ë³€ê²½ëœ í•„ë“œë§Œ ë°˜í™˜
        
        Returns:
            Dict: SFN í˜ì´ë¡œë“œ (10KB ë¯¸ë§Œ ë³´ì¥)
        """
        start_time = time.time()
        result = {}
        offloaded_count = 0
        
        # Control Plane í•„ë“œëŠ” í•­ìƒ í¬í•¨
        result.update(state.get_control_plane())
        
        # Data Plane í•„ë“œ ì²˜ë¦¬
        all_fields = set(state.keys()) | set(state.get_lazy_pointers().keys())
        
        for field_name in all_fields:
            # Control Planeì€ ì´ë¯¸ ì²˜ë¦¬ë¨
            if field_name in CONTROL_PLANE_FIELDS:
                continue
            
            # Lazy í•„ë“œëŠ” í¬ì¸í„° ê·¸ëŒ€ë¡œ ìœ ì§€
            if field_name in state.get_lazy_pointers():
                result[field_name] = state.get_lazy_pointers()[field_name].to_dict()
                continue
            
            # Delta ëª¨ë“œì—ì„œ ë³€ê²½ë˜ì§€ ì•Šì€ í•„ë“œëŠ” ìŠ¤í‚µ
            if return_delta and field_name not in state._changed_fields:
                continue
            
            value = state.get(field_name)
            if value is None:
                continue
            
            # í¬ê¸° ì²´í¬ ë° ì˜¤í”„ë¡œë“œ ê²°ì •
            should_offload = (
                field_name in DATA_PLANE_FIELDS or
                (force_offload_fields and field_name in force_offload_fields)
            )
            
            if should_offload:
                value_json = json.dumps(value, ensure_ascii=False, default=str)
                value_size = len(value_json.encode('utf-8'))
                
                if value_size > self._field_offload_threshold:
                    # S3 ì˜¤í”„ë¡œë“œ
                    pointer = self._offload_to_s3(
                        value=value,
                        value_json=value_json,
                        owner_id=owner_id,
                        workflow_id=workflow_id,
                        execution_id=execution_id,
                        field_name=field_name,
                        segment_id=segment_id
                    )
                    result[field_name] = pointer.to_dict()
                    offloaded_count += 1
                    continue
            
            # ì‘ì€ í•„ë“œëŠ” ê·¸ëŒ€ë¡œ í¬í•¨
            result[field_name] = value
        
        # Delta ë©”íƒ€ë°ì´í„° ì¶”ê°€
        if return_delta and state.has_changes():
            result[DELTA_MARKER] = True
            result["__changed_fields__"] = list(state._changed_fields)
            result["__deleted_fields__"] = list(state._deleted_fields)
        
        # ìµœì¢… í¬ê¸° ê²€ì¦
        final_size = len(json.dumps(result, ensure_ascii=False, default=str).encode('utf-8'))
        
        elapsed = (time.time() - start_time) * 1000
        logger.info(f"[StateHydrator] Dehydrated in {elapsed:.2f}ms, "
                   f"final_size={final_size/1024:.2f}KB, offloaded={offloaded_count}")
        
        if final_size > self._control_plane_max_size:
            logger.warning(f"[StateHydrator] Payload exceeds target ({final_size/1024:.2f}KB > "
                          f"{self._control_plane_max_size/1024:.2f}KB)")
        
        return result
    
    def _offload_to_s3(
        self,
        value: Any,
        value_json: str,
        owner_id: str,
        workflow_id: str,
        execution_id: str,
        field_name: str,
        segment_id: Optional[int] = None
    ) -> S3Pointer:
        """S3ì— ë°ì´í„° ì˜¤í”„ë¡œë“œ"""
        # S3 í‚¤ ìƒì„±
        segment_suffix = f"_seg{segment_id}" if segment_id is not None else ""
        timestamp = int(time.time() * 1000)
        s3_key = f"workflows/{workflow_id}/executions/{execution_id}/{field_name}{segment_suffix}_{timestamp}.json"
        
        # ì²´í¬ì„¬ ê³„ì‚°
        checksum = hashlib.md5(value_json.encode('utf-8')).hexdigest()[:8]
        
        # S3 ì—…ë¡œë“œ
        self.s3_client.put_object(
            Bucket=self._bucket,
            Key=s3_key,
            Body=value_json.encode('utf-8'),
            ContentType="application/json"
        )
        
        logger.debug(f"[StateHydrator] Offloaded {field_name} to s3://{self._bucket}/{s3_key}")
        
        return S3Pointer(
            bucket=self._bucket,
            key=s3_key,
            size_bytes=len(value_json.encode('utf-8')),
            checksum=checksum,
            field_name=field_name
        )
    
    def _load_from_s3(self, pointer: S3Pointer) -> Any:
        """
        S3ì—ì„œ ë°ì´í„° ë¡œë“œ (ìºì‹œ ì‚¬ìš© + Checksum ê²€ì¦)
        
        ğŸ›¡ï¸ [P0 ê°•í™”] ë¶„ì‚° í™˜ê²½ì—ì„œ ë°ì´í„° ì˜¤ì—¼ ë°©ì§€ë¥¼ ìœ„í•œ
        Checksum ê²€ì¦ ë¡œì§ ì¶”ê°€
        """
        cache_key = f"{pointer.bucket}/{pointer.key}"
        
        # ìºì‹œ í™•ì¸
        if cache_key in self._cache:
            logger.debug(f"[StateHydrator] Cache hit for {pointer.field_name}")
            return self._cache[cache_key]
        
        # S3ì—ì„œ ë¡œë“œ
        try:
            response = self.s3_client.get_object(
                Bucket=pointer.bucket,
                Key=pointer.key
            )
            raw_bytes = response['Body'].read()
            raw_str = raw_bytes.decode('utf-8')
            
            # ğŸ›¡ï¸ [P0] Checksum ê²€ì¦ - ë°ì´í„° ì˜¤ì—¼ ë°©ì§€
            if pointer.checksum:
                calculated_checksum = hashlib.md5(raw_str.encode('utf-8')).hexdigest()[:8]
                if calculated_checksum != pointer.checksum:
                    logger.error(
                        f"[StateHydrator] ğŸš¨ CHECKSUM MISMATCH for {pointer.field_name}! "
                        f"Expected: {pointer.checksum}, Got: {calculated_checksum}. "
                        f"Data may be corrupted. S3 Key: {pointer.key}"
                    )
                    # ì—„ê²© ëª¨ë“œ: ì˜¤ì—¼ëœ ë°ì´í„° ê±°ë¶€
                    raise ValueError(
                        f"Data integrity check failed for {pointer.field_name}. "
                        f"Checksum mismatch: expected {pointer.checksum}, got {calculated_checksum}"
                    )
                else:
                    logger.debug(f"[StateHydrator] Checksum verified for {pointer.field_name}")
            
            data = json.loads(raw_str)
            
            # ìºì‹œ ì €ì¥
            self._cache[cache_key] = data
            
            logger.debug(f"[StateHydrator] Loaded {pointer.field_name} from S3 "
                        f"({pointer.size_bytes/1024:.2f}KB, checksum={pointer.checksum or 'N/A'})")
            return data
            
        except json.JSONDecodeError as e:
            logger.error(f"[StateHydrator] JSON decode failed for {pointer.field_name}: {e}")
            raise
        except Exception as e:
            logger.error(f"[StateHydrator] Failed to load {pointer.field_name}: {e}")
            raise
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self._cache.clear()


# ============================================================================
# Helper Functions
# ============================================================================

def check_inter_segment_edges(
    segment_config: Dict[str, Any],
    next_segment_config: Optional[Dict[str, Any]] = None
) -> Optional[Dict[str, Any]]:
    """
    ğŸš€ Inter-segment edge ì •ë³´ ì¶”ì¶œ (O(1) ì¡°íšŒ)
    
    partition_mapì˜ outgoing_edges í•„ë“œë¥¼ í™œìš©í•˜ì—¬
    workflow_config ì „ì²´ ìŠ¤ìº” ì—†ì´ ì—£ì§€ íƒ€ì… í™•ì¸
    
    Args:
        segment_config: í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ì„¤ì •
        next_segment_config: ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì„¤ì • (optional)
    
    Returns:
        Dict with edge_type, is_loop_exit, condition, metadata
        None if no inter-segment edge found
    """
    outgoing_edges = segment_config.get("outgoing_edges", [])
    
    if not outgoing_edges:
        return None
    
    if next_segment_config is None:
        # ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ outgoing edge ë°˜í™˜
        if outgoing_edges:
            edge = outgoing_edges[0]
            return {
                "edge_type": edge.get("edge_type", "normal"),
                "is_loop_exit": edge.get("is_loop_exit", False),
                "is_back_edge": edge.get("is_back_edge", False),
                "condition": edge.get("condition"),
                "router_func": edge.get("router_func"),
                "target_node": edge.get("target_node"),
                "metadata": edge.get("metadata", {})
            }
        return None
    
    # ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ì˜ node_ids ì§‘í•©
    next_node_ids = set(next_segment_config.get("node_ids", []))
    
    # outgoing_edgesì—ì„œ ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ê°€ëŠ” ì—£ì§€ ì°¾ê¸°
    for edge in outgoing_edges:
        if edge.get("target_node") in next_node_ids:
            return {
                "edge_type": edge.get("edge_type", "normal"),
                "is_loop_exit": edge.get("is_loop_exit", False),
                "is_back_edge": edge.get("is_back_edge", False),
                "condition": edge.get("condition"),
                "router_func": edge.get("router_func"),
                "target_node": edge.get("target_node"),
                "metadata": edge.get("metadata", {})
            }
    
    return None


def is_hitp_edge(edge_info: Optional[Dict[str, Any]]) -> bool:
    """HITP ì—£ì§€ì¸ì§€ í™•ì¸"""
    if not edge_info:
        return False
    edge_type = edge_info.get("edge_type", "").lower()
    return edge_type in {"hitp", "human_in_the_loop", "pause", "approval"}


def is_loop_exit_edge(edge_info: Optional[Dict[str, Any]]) -> bool:
    """Loop exit ì—£ì§€ì¸ì§€ í™•ì¸"""
    if not edge_info:
        return False
    return edge_info.get("is_loop_exit", False)


def prepare_response_with_offload(
    final_state: Dict[str, Any],
    output_s3_path: Optional[str],
    threshold_kb: int = 250
) -> Dict[str, Any]:
    """
    ğŸš€ S3 ì˜¤í”„ë¡œë“œ í—¬í¼ (DRY ì›ì¹™)
    
    4ê³³ì—ì„œ ì¤‘ë³µë˜ë˜ ë¡œì§ì„ ë‹¨ì¼ í•¨ìˆ˜ë¡œ í†µí•©
    
    Args:
        final_state: ìµœì¢… ìƒíƒœ ë°ì´í„°
        output_s3_path: S3 ì €ì¥ ê²½ë¡œ (ì´ë¯¸ ì €ì¥ëœ ê²½ìš°)
        threshold_kb: ì˜¤í”„ë¡œë“œ ì„ê³„ê°’ (KB)
    
    Returns:
        Dict: ì›ë³¸ ë˜ëŠ” S3 í¬ì¸í„° ë©”íƒ€ë°ì´í„°
    """
    if not output_s3_path:
        return final_state
    
    try:
        state_json = json.dumps(final_state, ensure_ascii=False, default=str)
        state_size = len(state_json.encode('utf-8'))
    except Exception:
        # ì§ë ¬í™” ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        return final_state
    
    if state_size < threshold_kb * 1024:
        return final_state
    
    # í° ìƒíƒœëŠ” ë©”íƒ€ë°ì´í„°ë§Œ
    return {
        "__s3_offloaded": True,
        "__s3_path": output_s3_path,
        "__original_size_kb": round(state_size / 1024, 2)
    }


def merge_delta_into_state(
    master_state: Dict[str, Any],
    delta: Dict[str, Any]
) -> Dict[str, Any]:
    """
    ë¸íƒ€ ì—…ë°ì´íŠ¸ë¥¼ ë§ˆìŠ¤í„° ìƒíƒœì— ë³‘í•©
    
    Args:
        master_state: ê¸°ì¡´ ë§ˆìŠ¤í„° ìƒíƒœ
        delta: ë¸íƒ€ ì—…ë°ì´íŠ¸ (changed_fields, deleted_fields í¬í•¨)
    
    Returns:
        Dict: ë³‘í•©ëœ ìƒíƒœ
    """
    if not delta.get(DELTA_MARKER):
        # ë¸íƒ€ê°€ ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return delta
    
    result = master_state.copy()
    
    # ë³€ê²½ëœ í•„ë“œ ì ìš©
    changed_fields = delta.get("changed_fields", {})
    for key, value in changed_fields.items():
        result[key] = value
    
    # ì‚­ì œëœ í•„ë“œ ì œê±°
    deleted_fields = delta.get("deleted_fields", [])
    for key in deleted_fields:
        result.pop(key, None)
    
    # S3 í¬ì¸í„° ì ìš©
    s3_pointers = delta.get("s3_pointers", {})
    for key, pointer_data in s3_pointers.items():
        result[key] = pointer_data
    
    return result


def validate_execution_result(result: Dict[str, Any]) -> Tuple[bool, List[str]]:
    """
    ğŸ›¡ï¸ ExecutionResult ì¸í„°í˜ì´ìŠ¤ ì¤€ìˆ˜ ê²€ì¦
    
    Lambdaê°€ ë°˜í™˜í•˜ëŠ” ê²°ê³¼ê°€ ExecutionResult ì¸í„°í˜ì´ìŠ¤ë¥¼
    ì¤€ìˆ˜í•˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Args:
        result: Lambda ë°˜í™˜ ê²°ê³¼
    
    Returns:
        Tuple[bool, List[str]]: (ìœ íš¨ì„± ì—¬ë¶€, ì˜¤ë¥˜ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸)
    """
    errors = []
    
    # í•„ìˆ˜ í•„ë“œ ê²€ì¦
    if "status" not in result:
        errors.append("Missing required field: status")
    else:
        valid_statuses = {
            "CONTINUE", "COMPLETE", "PARALLEL_GROUP", "SEQUENTIAL_BRANCH",
            "PAUSE", "PAUSED_FOR_HITP", "FAILED", "HALTED", "SIGKILL", "SKIPPED"
        }
        if result["status"] not in valid_statuses:
            errors.append(f"Invalid status: {result['status']}. Valid: {valid_statuses}")
    
    # final_state ê²€ì¦
    if "final_state" not in result and "final_state_s3_path" not in result:
        errors.append("Must have either 'final_state' or 'final_state_s3_path'")
    
    # ë¼ìš°íŒ… ì •ë³´ ê²€ì¦ (CONTINUE ìƒíƒœì¼ ë•Œ)
    if result.get("status") == "CONTINUE" and "next_segment_to_run" not in result:
        errors.append("CONTINUE status requires 'next_segment_to_run'")
    
    # ë³‘ë ¬ ê·¸ë£¹ ê²€ì¦
    if result.get("status") == "PARALLEL_GROUP" and "branches" not in result:
        errors.append("PARALLEL_GROUP status requires 'branches'")
    
    return len(errors) == 0, errors


def aggregate_branch_results(
    branch_results: List[Dict[str, Any]],
    merge_strategy: Literal["last_wins", "deep_merge", "collect"] = "deep_merge"
) -> Dict[str, Any]:
    """
    ğŸ”€ Fork-Join íŒ¨í„´: ë³‘ë ¬ ë¸Œëœì¹˜ ê²°ê³¼ ë³‘í•©
    
    ë³‘ë ¬ ì‹¤í–‰ëœ ë¸Œëœì¹˜ë“¤ì˜ ê²°ê³¼ë¥¼ ì•ˆì „í•˜ê²Œ ë³‘í•©í•©ë‹ˆë‹¤.
    ë³‘í•© ì¶©ëŒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì „ëµì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    Args:
        branch_results: ê° ë¸Œëœì¹˜ì˜ BranchResult ë°°ì—´
        merge_strategy:
            - "last_wins": ë§ˆì§€ë§‰ ë¸Œëœì¹˜ê°€ ë®ì–´ì“€ (ë¶ˆê¶Œì¥)
            - "deep_merge": ê¹Šì€ ë³‘í•© (ê¸°ë³¸ê°’)
            - "collect": ëª¨ë“  ë¸Œëœì¹˜ ê²°ê³¼ë¥¼ ë°°ì—´ë¡œ ìˆ˜ì§‘
    
    Returns:
        Dict: ë³‘í•©ëœ ìƒíƒœ
    """
    if not branch_results:
        return {}
    
    if merge_strategy == "collect":
        # ëª¨ë“  ë¸Œëœì¹˜ ê²°ê³¼ë¥¼ ë°°ì—´ë¡œ ìˆ˜ì§‘
        return {
            "branch_results": branch_results,
            "total_branches": len(branch_results),
            "successful_branches": len([b for b in branch_results if b.get("status") == "completed"]),
            "failed_branches": len([b for b in branch_results if b.get("status") == "failed"])
        }
    
    if merge_strategy == "last_wins":
        # ë§ˆì§€ë§‰ ë¸Œëœì¹˜ê°€ ë®ì–´ì“€ (ë³‘í•© ì¶©ëŒ ìœ„í—˜!)
        logger.warning("[Fork-Join] Using 'last_wins' strategy - potential merge conflicts!")
        result = {}
        for branch in branch_results:
            final_state = branch.get("final_state", {})
            result.update(final_state)
        return result
    
    # deep_merge: ê¹Šì€ ë³‘í•© (ê¸°ë³¸)
    def deep_merge(base: Dict, update: Dict) -> Dict:
        result = base.copy()
        for key, value in update.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = deep_merge(result[key], value)
            elif key in result and isinstance(result[key], list) and isinstance(value, list):
                # ë¦¬ìŠ¤íŠ¸ëŠ” í™•ì¥
                result[key] = result[key] + value
            else:
                result[key] = value
        return result
    
    merged = {}
    for branch in branch_results:
        final_state = branch.get("final_state", {})
        merged = deep_merge(merged, final_state)
    
    # ë¸Œëœì¹˜ ë©”íƒ€ë°ì´í„° ì¶”ê°€
    merged["__branch_metadata__"] = {
        "total_branches": len(branch_results),
        "successful_branches": len([b for b in branch_results if b.get("status") == "completed"]),
        "failed_branches": len([b for b in branch_results if b.get("status") == "failed"]),
        "branch_ids": [b.get("branch_id") for b in branch_results],
        "total_execution_time_ms": sum(b.get("execution_time_ms", 0) for b in branch_results)
    }
    
    return merged


def create_thought_signature(
    thinking_process: str,
    thought_steps: Optional[List[Dict[str, Any]]] = None,
    confidence_score: float = 0.0,
    reasoning_tokens: int = 0,
    model_version: str = "unknown"
) -> ThoughtSignature:
    """
    ğŸ§  Gemini 3 ì‚¬ê³  ê³¼ì • ì„œëª… ìƒì„±
    
    LLMì˜ ì‚¬ê³  ê³¼ì •ì„ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        thinking_process: ì‚¬ê³  ê³¼ì • í…ìŠ¤íŠ¸
        thought_steps: ë‹¨ê³„ë³„ ì‚¬ê³  ê³¼ì •
        confidence_score: ì‹ ë¢°ë„ ì ìˆ˜ (0.0 ~ 1.0)
        reasoning_tokens: ì‚¬ê³ ì— ì‚¬ìš©ëœ í† í° ìˆ˜
        model_version: ì‚¬ìš©ëœ ëª¨ë¸ ë²„ì „
    
    Returns:
        ThoughtSignature: ì‚¬ê³  ê³¼ì • ì„œëª…
    """
    return {
        "thinking_process": thinking_process,
        "thought_steps": thought_steps or [],
        "confidence_score": max(0.0, min(1.0, confidence_score)),
        "reasoning_tokens": reasoning_tokens,
        "model_version": model_version,
        "timestamp": time.time()
    }


# ============================================================================
# Backward Compatibility
# ============================================================================

def ensure_smart_state_bag(state: Any, hydrator: Optional[StateHydrator] = None) -> SmartStateBag:
    """ê¸°ì¡´ StateBag ë˜ëŠ” dictë¥¼ SmartStateBagìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ"""
    if isinstance(state, SmartStateBag):
        return state
    return SmartStateBag(state if isinstance(state, dict) else {}, hydrator=hydrator)


# ============================================================================
# Module-level Singleton (Lambda ì¬ì‚¬ìš© ìµœì í™”)
# ============================================================================

_default_hydrator: Optional[StateHydrator] = None


def get_default_hydrator() -> StateHydrator:
    """Default StateHydrator ì‹±ê¸€í†¤"""
    global _default_hydrator
    if _default_hydrator is None:
        _default_hydrator = StateHydrator()
    return _default_hydrator


def hydrate_event(event: Dict[str, Any], **kwargs) -> SmartStateBag:
    """ì´ë²¤íŠ¸ë¥¼ SmartStateBagìœ¼ë¡œ ë³€í™˜ (ì‹±ê¸€í†¤ ì‚¬ìš©)"""
    return get_default_hydrator().hydrate(event, **kwargs)


def dehydrate_state(
    state: SmartStateBag,
    owner_id: str,
    workflow_id: str,
    execution_id: str,
    **kwargs
) -> Dict[str, Any]:
    """SmartStateBagì„ SFN ë°˜í™˜ìš©ìœ¼ë¡œ ë³€í™˜ (ì‹±ê¸€í†¤ ì‚¬ìš©)"""
    return get_default_hydrator().dehydrate(
        state, owner_id, workflow_id, execution_id, **kwargs
    )
