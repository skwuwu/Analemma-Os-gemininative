import os
import json
import time
import logging
import boto3

# [1ìˆœìœ„ ìµœì í™”] Pre-compilation: DBì—ì„œ partition_map ë¡œë“œ
# ëŸ°íƒ€ì„ íŒŒí‹°ì…”ë‹ì€ fallbackìœ¼ë¡œë§Œ ì‚¬ìš©
try:
    from src.services.workflow.partition_service import partition_workflow_advanced
    _HAS_PARTITION = True
except ImportError:
    try:
        from src.services.workflow.partition_service import partition_workflow_advanced
        _HAS_PARTITION = True
    except ImportError:
        _HAS_PARTITION = False
        partition_workflow_advanced = None

# DynamoDB í´ë¼ì´ì–¸íŠ¸ (ì›œ ìŠ¤íƒ€íŠ¸ ìµœì í™”)
try:
    from src.common.aws_clients import get_dynamodb_resource
    _dynamodb = get_dynamodb_resource()
except ImportError:
    _dynamodb = boto3.resource('dynamodb')

WORKFLOWS_TABLE = os.environ.get('WORKFLOWS_TABLE', 'Workflows')

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


def _calculate_dynamic_concurrency(
    total_segments: int,
    llm_segments: int,
    hitp_segments: int,
    partition_map: list,
    owner_id: str
) -> int:
    """
    ì›Œí¬í”Œë¡œìš° ë³µì¡ë„ì™€ ì‚¬ìš©ì í‹°ì–´ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ë™ì‹œì„± ê³„ì‚°
    
    ğŸš¨ [Critical] ê·œê²© ëª…í™•í™”:
    ì´ í•¨ìˆ˜ê°€ ë°˜í™˜í•˜ëŠ” max_concurrencyëŠ” **ì²­í¬ ë‚´ë¶€ì˜ ë³‘ë ¬ ì²˜ë¦¬ëŸ‰**ì„ ê²°ì •í•©ë‹ˆë‹¤.
    
    - Distributed Mapì˜ MaxConcurrencyëŠ” ASLì—ì„œ 1ë¡œ ê³ ì • (ìƒíƒœ ì—°ì†ì„± ë³´ì¥)
    - ì´ ê°’ì€ ê° ì²­í¬ ë‚´ì—ì„œ ë³‘ë ¬ ë¸Œëœì¹˜ ì‹¤í–‰ ì‹œ ì‚¬ìš©ë¨
    - ProcessSegmentChunk ë‚´ë¶€ì˜ ë³‘ë ¬ ê·¸ë£¹ ì²˜ë¦¬ì— ì ìš©
    
    Args:
        total_segments: ì´ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜
        llm_segments: LLM ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜
        hitp_segments: HITP ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜
        partition_map: íŒŒí‹°ì…˜ ë§µ
        owner_id: ì‚¬ìš©ì ID
        
    Returns:
        int: ê³„ì‚°ëœ ì²­í¬ ë‚´ë¶€ MaxConcurrency ê°’ (5-50 ë²”ìœ„)
             â€» Distributed Map ìì²´ì˜ MaxConcurrencyì™€ëŠ” ë³„ê°œ
    """
    # ê¸°ë³¸ê°’
    base_concurrency = 15
    
    # 1. ë³‘ë ¬ ë¸Œëœì¹˜ ìˆ˜ ê³„ì‚°
    max_parallel_branches = 0
    if partition_map:
        for segment in partition_map:
            if segment.get('type') == 'parallel_group':
                branches = segment.get('branches', [])
                max_parallel_branches = max(max_parallel_branches, len(branches))
    
    # 2. ì›Œí¬í”Œë¡œìš° ë³µì¡ë„ ê¸°ë°˜ ì¡°ì •
    calculated_concurrency = base_concurrency # [Safety] Initialize default
    
    if max_parallel_branches == 0:
        # ë³‘ë ¬ ë¸Œëœì¹˜ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        calculated_concurrency = base_concurrency
    elif max_parallel_branches <= 5:

        # ì†Œê·œëª¨ ë³‘ë ¬ (5ê°œ ì´í•˜): ëª¨ë‘ ë™ì‹œ ì‹¤í–‰
        calculated_concurrency = max_parallel_branches
    elif max_parallel_branches <= 10:
        # ì¤‘ê·œëª¨ ë³‘ë ¬ (6-10ê°œ): 80% ë™ì‹œ ì‹¤í–‰
        calculated_concurrency = int(max_parallel_branches * 0.8)
    elif max_parallel_branches <= 20:
        # ëŒ€ê·œëª¨ ë³‘ë ¬ (11-20ê°œ): 60% ë™ì‹œ ì‹¤í–‰
        calculated_concurrency = int(max_parallel_branches * 0.6)
    else:
        # ì´ˆëŒ€ê·œëª¨ ë³‘ë ¬ (21ê°œ ì´ìƒ): ìµœëŒ€ 30ê°œë¡œ ì œí•œ
        calculated_concurrency = min(30, int(max_parallel_branches * 0.5))
    
    # 3. ì‚¬ìš©ì í‹°ì–´ ê¸°ë°˜ ì¡°ì • (ì„ íƒì )
    try:
        user_table = _dynamodb.Table(os.environ.get('USERS_TABLE', 'Users'))
        user_response = user_table.get_item(Key={'userId': owner_id})
        user_item = user_response.get('Item')
        
        if user_item:
            subscription_plan = user_item.get('subscription_plan', 'free')
            
            # í‹°ì–´ë³„ ë°°ìˆ˜ ì ìš©
            tier_multipliers = {
                'free': 0.5,      # 50% ì œí•œ
                'basic': 0.75,    # 75%
                'pro': 1.0,       # 100%
                'enterprise': 1.5 # 150% (ìµœëŒ€ 50ê¹Œì§€)
            }
            
            multiplier = tier_multipliers.get(subscription_plan, 1.0)
            calculated_concurrency = int(calculated_concurrency * multiplier)
            
            logger.info(f"User tier '{subscription_plan}' applied multiplier {multiplier}")
    except Exception as e:
        logger.warning(f"Failed to load user tier for concurrency calculation: {e}")
    
    # 4. ğŸ›¡ï¸ [Concurrency Protection] OS ë ˆë²¨ ìƒí•œ í´ë¨í•‘
    # ê³„ì • ë™ì‹œì„± í•œë„(10)ì— ë§ì¶° ì „ì²´ ì‹œìŠ¤í…œ ì•ˆì •ì„± ë³´ì¥
    MAX_OS_LIMIT = 2  # ê³„ì • Concurrency 10 ì œí•œ ëŒ€ì‘ (ìƒí–¥ ìš”ì²­ ì™„ë£Œ ì‹œ ì¦ê°€ ì˜ˆì •)
    clamped_concurrency = min(calculated_concurrency, MAX_OS_LIMIT)
    
    if calculated_concurrency > MAX_OS_LIMIT:
        logger.warning(
            f"[Concurrency Protection] Clamping requested concurrency {calculated_concurrency} "
            f"to OS limit {MAX_OS_LIMIT}"
        )
    
    # 5. ìµœì¢… ë²”ìœ„ ì œí•œ (1 ~ MAX_OS_LIMIT)
    final_concurrency = max(1, clamped_concurrency)
    
    logger.info(
        f"Chunk-internal concurrency calculated: {final_concurrency} "
        f"(branches: {max_parallel_branches}, segments: {total_segments}, "
        f"llm: {llm_segments}, hitp: {hitp_segments}, OS_LIMIT: {MAX_OS_LIMIT}) "
        f"â€» Distributed Map MaxConcurrency remains 1 for state continuity"
    )
    
    return final_concurrency


def _load_workflow_config(owner_id: str, workflow_id: str) -> dict:
    """
    Workflows í…Œì´ë¸”ì—ì„œ workflow configë¥¼ ë¡œë“œ.
    subgraphsë¥¼ í¬í•¨í•œ ì „ì²´ configë¥¼ ê°€ì ¸ì˜´.
    """
    if not owner_id or not workflow_id:
        return None
    
    try:
        table = _dynamodb.Table(WORKFLOWS_TABLE)
        response = table.get_item(
            Key={'ownerId': owner_id, 'workflowId': workflow_id},
            ProjectionExpression='config, partition_map, total_segments, llm_segments_count, hitp_segments_count'
        )
        item = response.get('Item')
        if item and item.get('config'):
            config = item.get('config')
            # JSON stringì´ë©´ íŒŒì‹±
            if isinstance(config, str):
                import json
                config = json.loads(config)
            logger.info(f"Loaded workflow config from src.DB: {workflow_id}")
            return {
                'config': config,
                'partition_map': item.get('partition_map'),
                'total_segments': item.get('total_segments'),
                'llm_segments_count': item.get('llm_segments_count'),
                'hitp_segments_count': item.get('hitp_segments_count')
            }
        return None
    except Exception as e:
        logger.warning(f"Failed to load workflow config: {e}")
        return None


def _load_precompiled_partition(owner_id: str, workflow_id: str) -> dict:
    """
    Workflows í…Œì´ë¸”ì—ì„œ pre-compiled partition_map ë¡œë“œ.
    ì €ì¥ ì‹œì ì— ê³„ì‚°ëœ partition_mapì„ ê°€ì ¸ì˜´.
    """
    if not owner_id or not workflow_id:
        return None
    
    try:
        table = _dynamodb.Table(WORKFLOWS_TABLE)
        response = table.get_item(
            Key={'ownerId': owner_id, 'workflowId': workflow_id},
            ProjectionExpression='partition_map, total_segments, llm_segments_count, hitp_segments_count'
        )
        item = response.get('Item')
        if item and item.get('partition_map'):
            logger.info(f"Loaded pre-compiled partition_map from src.DB: {item.get('total_segments', 0)} segments")
            return item
        return None
    except Exception as e:
        logger.warning(f"Failed to load pre-compiled partition_map: {e}")
        return None


def lambda_handler(event, context):
    """
    Initializes state data.
    [1ìˆœìœ„ ìµœì í™”] Pre-compiled partition_mapì„ DBì—ì„œ ë¡œë“œ.
    Fallback: ëŸ°íƒ€ì„ ê³„ì‚° (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
    
    [ì„œë¸Œê·¸ë˜í”„ ì§€ì›] workflow_configì— subgraphsê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´
    DynamicWorkflowBuilderì—ì„œ ì¬ê·€ì ìœ¼ë¡œ ë¹Œë“œë¨.
    """
    logger.info("Initializing state data")
    
    # [FIX] 1. Move initialization to top (prevent NameError in S3 Metadata)
    current_time = int(time.time())
    
    # 1. ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
    raw_input = event.get('input', event)
    if not isinstance(raw_input, dict):
        raw_input = {}
        
    # [FIX] ë³€ìˆ˜ ëª…ì‹œì  ì´ˆê¸°í™” (UnboundLocalError ë° Missing Field ë°©ì§€)
    # SFNì—ì„œ $.field_name ì°¸ì¡° ì‹œ ì—ëŸ¬ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ í‚¤ê°€ í•­ìƒ ì¡´ì¬í•´ì•¼ í•¨
    partition_map_s3_path = ""
    manifest_s3_path = ""
    state_s3_path = ""
    
    # 2. Config ê²°ì • ìš°ì„ ìˆœìœ„ ì •êµí™”
    workflow_config = raw_input.get('test_workflow_config') or raw_input.get('workflow_config')
    owner_id = raw_input.get('ownerId', "")
    workflow_id = raw_input.get('workflowId', "")
    idempotency_key = raw_input.get('idempotency_key', "")
    quota_reservation_id = raw_input.get('quota_reservation_id', "")
    
    # [Fix] MOCK_MODE ì¶”ì¶œ - ì‹œë®¬ë ˆì´í„° E2E í…ŒìŠ¤íŠ¸ìš©
    mock_mode = raw_input.get('MOCK_MODE', 'false')
    
    # [Robustness Fix] workflow_configê°€ ì—†ìœ¼ë©´ DBì—ì„œ ë¡œë“œ (test_config ì—†ì„ ë•Œ)
    if not workflow_config and workflow_id and owner_id:
        logger.info(f"workflow_config missing in input, loading from src.DB: {workflow_id}")
        db_data = _load_workflow_config(owner_id, workflow_id)
        if db_data and db_data.get('config'):
            workflow_config = db_data.get('config')
            # DBì—ì„œ partition_mapë„ í•¨ê»˜ ë¡œë“œëœ ê²½ìš° ì‚¬ìš©
            if db_data.get('partition_map'):
                event['_db_partition_map'] = db_data.get('partition_map')
                event['_db_total_segments'] = db_data.get('total_segments')
                event['_db_llm_segments'] = db_data.get('llm_segments_count')
                event['_db_hitp_segments'] = db_data.get('hitp_segments_count')
        else:
            logger.warning(f"Workflow not found in DB: {workflow_id}")

    # workflow_configê°€ ì—¬ì „íˆ ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì„¤ì •í•˜ì—¬ crash ë°©ì§€
    if not workflow_config:
        workflow_config = {}
        logger.warning("Proceeding with empty workflow_config (risk of later failure)")

    # 3. MOCK_MODE: ê°•ì œ íŒŒí‹°ì…”ë‹ (DB ë°ì´í„° ì—†ìŒ ëŒ€ë¹„)
    if raw_input.get('test_workflow_config'):
         if not _HAS_PARTITION:
             logger.warning("MOCK_MODE but partition logic unavailable!")
         else:
             logger.info("MOCK_MODE detected: Forcing runtime partition calculation")
             try:
                 partition_result = partition_workflow_advanced(workflow_config)
                 # Override raw inputs/event cache with fresh calculation
                 raw_input['partition_map'] = partition_result.get('partition_map', [])
                 raw_input['total_segments'] = partition_result.get('total_segments', 0)
                 raw_input['llm_segments'] = partition_result.get('llm_segments', 0)
                 raw_input['hitp_segments'] = partition_result.get('hitp_segments', 0)
             except Exception as e:
                 logger.error(f"MOCK_MODE partitioning failed: {e}")

    # 2. íŒŒí‹°ì…”ë‹ (ìš°ì„ ìˆœìœ„: ì…ë ¥ > DB pre-compiled > ëŸ°íƒ€ì„ ê³„ì‚°)
    partition_map = None
    total_segments = 0
    llm_segments = 0
    hitp_segments = 0
    
    # 2a. ì…ë ¥ì— ì´ë¯¸ partition_mapì´ ìˆë‹¤ë©´ ì‚¬ìš© (ì¬ì‹œë„/Child Workflow)
    if raw_input.get('partition_map'):
        logger.info("Using provided partition_map from src.input")
        partition_map = raw_input.get('partition_map')
        total_segments = raw_input.get('total_segments', len(partition_map))
        llm_segments = raw_input.get('llm_segments', 0)
        hitp_segments = raw_input.get('hitp_segments', 0)
    
    # 2a-1. config ë¡œë“œ ì‹œ í•¨ê»˜ ê°€ì ¸ì˜¨ partition_map ì‚¬ìš©
    if not partition_map and event.get('_db_partition_map'):
        logger.info("Using partition_map from src.DB config load")
        partition_map = event.get('_db_partition_map')
        total_segments = event.get('_db_total_segments', len(partition_map) if partition_map else 0)
        llm_segments = event.get('_db_llm_segments', 0)
        hitp_segments = event.get('_db_hitp_segments', 0)
    
    # 2b. DBì—ì„œ pre-compiled partition_map ë¡œë“œ (1ìˆœìœ„ ìµœì í™”)
    if not partition_map:
        precompiled = _load_precompiled_partition(owner_id, workflow_id)
        if precompiled:
            partition_map = precompiled.get('partition_map')
            total_segments = precompiled.get('total_segments', len(partition_map) if partition_map else 0)
            llm_segments = precompiled.get('llm_segments_count', 0)
            hitp_segments = precompiled.get('hitp_segments_count', 0)
    
    # 2c. Fallback: ëŸ°íƒ€ì„ ê³„ì‚° (ê¸°ì¡´ ë¡œì§, í•˜ìœ„ í˜¸í™˜ì„±)
    if not partition_map:
        if not _HAS_PARTITION:
            raise RuntimeError("partition_workflow_advanced not available and no pre-compiled partition_map found")
        
        logger.info("Calculating partition_map at runtime (fallback)...")
        try:
            partition_result = partition_workflow_advanced(workflow_config)
            partition_map = partition_result.get('partition_map', [])
            total_segments = partition_result.get('total_segments', 0)
            llm_segments = partition_result.get('llm_segments', 0)
            hitp_segments = partition_result.get('hitp_segments', 0)
        except Exception as e:
            logger.error(f"Partitioning failed: {e}")
            raise RuntimeError(f"Failed to partition workflow: {str(e)}")

    # ğŸ¯ [ë‹¨ì¼í™” ì „ëµ] ì‹¤í–‰í•  ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸(segment_manifest) ìƒì„±
    segment_manifest = []
    for idx, segment in enumerate(partition_map):
        segment_manifest.append({
            "segment_id": idx,
            "segment_config": segment,
            "execution_order": idx,
            "dependencies": segment.get("dependencies", []),
            "type": segment.get("type", "normal")
        })
    
    # ğŸš¨ [Critical Fix] ë¶„ì‚° ëª¨ë“œ ê°ì§€ ë° ëŒ€ìš©ëŸ‰ ë°ì´í„° S3 ì˜¤í”„ë¡œë”©
    is_distributed_mode = total_segments > 300  # ë¶„ì‚° ëª¨ë“œ ì„ê³„ê°’
    partition_map_for_return = partition_map  # ê¸°ë³¸ê°’: ì „ì²´ ë°˜í™˜
    
    # AWS Clients
    s3_client = None
    bucket = os.environ.get('WORKFLOW_STATE_BUCKET')
    
    # S3 ì—…ë¡œë“œ í•„ìš” ì‹œì—ë§Œ boto3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    if bucket and owner_id and workflow_id:
        if len(segment_manifest) > 50 or is_distributed_mode:
            import boto3
            s3_client = boto3.client('s3')

    # 1. Manifest Offloading (50ê°œ ì´ˆê³¼ ì‹œ)
    if len(segment_manifest) > 50 and s3_client:
        try:
            manifest_key = f"workflow-manifests/{owner_id}/{workflow_id}/segment_manifest.json"
            manifest_data = json.dumps(segment_manifest, ensure_ascii=False)
            s3_client.put_object(
                Bucket=bucket,
                Key=manifest_key,
                Body=manifest_data.encode('utf-8')
            )
            manifest_s3_path = f"s3://{bucket}/{manifest_key}"
            logger.info(f"Segment manifest uploaded to S3: {manifest_s3_path}")
        except Exception as e:
            logger.warning(f"Failed to upload segment manifest to S3: {e}")
            # ì‹¤íŒ¨ ì‹œ manifest_s3_pathëŠ” ì´ˆê¸°ê°’ "" ìœ ì§€ë¨

    # 2. Partition Map Offloading (ë¶„ì‚° ëª¨ë“œ)
    # ğŸš¨ [Critical Fix] ë¶„ì‚° ëª¨ë“œì—ì„œ partition_map í¬ê¸° ìµœì í™”
    if is_distributed_mode:
        partition_map_json = json.dumps(partition_map, ensure_ascii=False)
        partition_map_size_kb = len(partition_map_json.encode('utf-8')) / 1024
        
        logger.info(f"Distributed mode detected: {total_segments} segments, partition_map size: {partition_map_size_kb:.1f}KB")
        
        # 100KB ì´ìƒì˜ partition_mapì€ S3ë¡œ ì˜¤í”„ë¡œë”©
        if partition_map_size_kb > 100 and s3_client:
            try:
                partition_map_key = f"workflow-partitions/{owner_id}/{workflow_id}/partition_map.json"
                s3_client.put_object(
                    Bucket=bucket,
                    Key=partition_map_key,
                    Body=partition_map_json.encode('utf-8'),
                    ContentType='application/json',
                    Metadata={
                        'total_segments': str(total_segments),
                        'size_kb': str(int(partition_map_size_kb)),
                        'created_at': str(current_time)
                    }
                )
                
                partition_map_s3_path = f"s3://{bucket}/{partition_map_key}"
                logger.info(f"Large partition_map offloaded to S3: {partition_map_s3_path}")
                
                # ğŸš¨ [Critical] ë¶„ì‚° ëª¨ë“œì—ì„œëŠ” partition_map ì œì™¸
                partition_map_for_return = None
                
            except Exception as e:
                logger.warning(f"Failed to offload partition_map to S3: {e}")
                # S3 ì‹¤íŒ¨ ì‹œ ê²½ê³ í•˜ê³  ì¸ë¼ì¸ìœ¼ë¡œ í´ë°±
                logger.warning(f"Proceeding with inline partition_map ({partition_map_size_kb:.1f}KB)")
        else:
            logger.info(f"partition_map size acceptable for inline return: {partition_map_size_kb:.1f}KB")

    # 3. ì´ˆê¸° ìƒíƒœ êµ¬ì„±
    current_time = int(time.time())
    
    # S3 ê²½ë¡œê°€ ìˆìœ¼ë©´ current_stateë¥¼ nullë¡œ, ì•„ë‹ˆë©´ initial_state ì‚¬ìš©
    initial_state_s3_path = workflow_config.get('initial_state_s3_path')
    if initial_state_s3_path:
        # Large payload: S3ì—ì„œ ë¡œë“œ (segment_runnerì—ì„œ ì²˜ë¦¬)
        current_state = None
        state_s3_path = initial_state_s3_path
        input_value = initial_state_s3_path
    else:
        # Inline state
        current_state = workflow_config.get('initial_state', {})
        # state_s3_path = "" # ìœ„ì—ì„œ ì´ë¯¸ ì´ˆê¸°í™”ë¨
        input_value = current_state
    
    # 4. ë™ì  ë™ì‹œì„± ê³„ì‚° (Map ìƒíƒœ ìµœì í™”)
    max_concurrency = _calculate_dynamic_concurrency(
        total_segments=total_segments,
        llm_segments=llm_segments,
        hitp_segments=hitp_segments,
        partition_map=partition_map,
        owner_id=owner_id
    )
    
    logger.info(f"[FIXED_ROBUST] Returning state data. partition_map_s3_path: '{partition_map_s3_path}'")

    return {
        "workflow_config": workflow_config,
        "current_state": current_state,
        "input": input_value,
        "state_s3_path": state_s3_path,
        "state_history": [],
        "ownerId": owner_id,
        "workflowId": workflow_id,
        "segment_to_run": 0,
        "idempotency_key": idempotency_key,
        "quota_reservation_id": quota_reservation_id,
        
        # ğŸš¨ [Critical Fix] ë¶„ì‚° ëª¨ë“œì—ì„œ partition_map ì¡°ê±´ë¶€ ë°˜í™˜
        "total_segments": total_segments,
        "partition_map": partition_map_for_return,
        "partition_map_s3_path": partition_map_s3_path,  # í•­ìƒ ì¡´ì¬ (ìµœì†Œ "")
        
        # manifest_s3_pathê°€ ìˆìœ¼ë©´ segment_manifestëŠ” Noneìœ¼ë¡œ (í˜ì´ë¡œë“œ ìµœì í™”)
        "segment_manifest": segment_manifest if manifest_s3_path == "" else None,
        "segment_manifest_s3_path": manifest_s3_path,    # í•­ìƒ ì¡´ì¬ (ìµœì†Œ "")
        
        # Metadata
        "state_durations": {},
        "last_update_time": current_time,
        "start_time": current_time,
        "loop_counter": 0,
        "llm_segments": llm_segments,
        "hitp_segments": hitp_segments,
        
        # Distributed Mode Flag
        "distributed_mode": is_distributed_mode,
        "max_concurrency": max_concurrency,
        
        # [Fix] MOCK_MODE ì „ë‹¬ - ì‹œë®¬ë ˆì´í„° E2E í…ŒìŠ¤íŠ¸ìš© (HITP ìë™ resume ë“±)
        "MOCK_MODE": mock_mode,
        
        # ğŸš¨ [Critical Fix] Loop Control Parameters
        "max_loop_iterations": workflow_config.get("max_loop_iterations", 100),
        "max_branch_iterations": workflow_config.get("max_branch_iterations", 100)
    }

