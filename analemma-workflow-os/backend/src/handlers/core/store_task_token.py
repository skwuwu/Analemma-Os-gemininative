import json
import os
import time
import logging
import boto3
from botocore.exceptions import ClientError
import uuid

# Í≥µÌÜµ Î™®ÎìàÏóêÏÑú AWS ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Í∞ÄÏ†∏Ïò§Í∏∞
try:
    from src.common.aws_clients import get_dynamodb_resource, get_stepfunctions_client
    from src.common.constants import DynamoDBConfig
    dynamodb = get_dynamodb_resource()
    sfn_client = get_stepfunctions_client()
except ImportError:
    dynamodb = boto3.resource('dynamodb')
    sfn_client = boto3.client('stepfunctions')

# [v3.17] Kernel Protocol ÏÇ¨Ïö© - ÏïàÏ†ÑÌïú state bag Ï∂îÏ∂ú
try:
    from src.common.kernel_protocol import open_state_bag
    KERNEL_PROTOCOL_AVAILABLE = True
except ImportError:
    KERNEL_PROTOCOL_AVAILABLE = False
    def open_state_bag(event):
        """Fallback: Í∏∞Î≥∏ Ï∂îÏ∂ú Î°úÏßÅ"""
        if not isinstance(event, dict):
            return {}
        state_data = event.get('state_data', {})
        if isinstance(state_data, dict):
            bag = state_data.get('bag')
            if isinstance(bag, dict):
                return bag
            return state_data
        return event

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# ÌôòÍ≤Ω Î≥ÄÏàòÎ°ú ÌÖåÏù¥Î∏îÎ™Ö ÏßÄÏ†ï
TASK_TOKENS_TABLE = DynamoDBConfig.TASK_TOKENS_TABLE

table = dynamodb.Table(TASK_TOKENS_TABLE)

# Í∏∞Î≥∏ TTL: 1 day (ÏÉÅÏàò ÏÇ¨Ïö©)
try:
    from src.common.constants import TTLConfig, EnvironmentVariables
    DEFAULT_TTL_SECONDS = int(os.environ.get(EnvironmentVariables.TASK_TOKEN_TTL_SECONDS, TTLConfig.TASK_TOKEN_DEFAULT))
except ImportError:
    DEFAULT_TTL_SECONDS = int(os.environ.get('TASK_TOKEN_TTL_SECONDS', 86400))


def get_user_connection_ids(owner_id):
    """
    DynamoDB GSIÎ•º ÏøºÎ¶¨ÌïòÏó¨ ownerIdÏóê Ïó∞Í≤∞Îêú Î™®Îì† connectionIdÎ•º Î∞òÌôòÌï©ÎãàÎã§.
    
    Returns:
        list: connectionId Î™©Î°ù, ÎπÑÏñ¥ÏûàÏùÑ Ïàò ÏûàÏùå
    """
    table_name = DynamoDBConfig.WEBSOCKET_CONNECTIONS_TABLE
    gsi_name = DynamoDBConfig.WEBSOCKET_OWNER_ID_GSI
    
    if not table_name or not owner_id:
        return []
    
    try:
        conn_table = dynamodb.Table(table_name)
        from boto3.dynamodb.conditions import Key
        
        response = conn_table.query(
            IndexName=gsi_name,
            KeyConditionExpression=Key('ownerId').eq(owner_id)
        )
        
        return [item['connectionId'] for item in response.get('Items', []) if 'connectionId' in item]
    except Exception as e:
        import logging
        logger = logging.getLogger()
        logger.error(f'Failed to query connections for owner={owner_id}: {e}')
        return []


def _send_hitp_notification(owner_id, conversation_id, execution_id, context_info, payload):
    """
    HITP Î∞úÏÉù Ïãú ÏÇ¨Ïö©ÏûêÏóêÍ≤å ÏïåÎ¶ºÏùÑ Ï†ÑÏÜ°Ìï©ÎãàÎã§.
    
    3Í∞ÄÏßÄ ÏïåÎ¶º Ï±ÑÎÑê:
    1. Ïã§ÏãúÍ∞Ñ Push (WebSocket) - ÏÇ¨Ïö©ÏûêÍ∞Ä Ï†ëÏÜç Ï§ëÏùº Îïå
    2. ÏòÅÍµ¨Ï†Å ÏïåÎ¶º Ï†ÄÏû• (DynamoDB PendingNotifications) - ÏïåÎ¶º ÏÑºÌÑ∞Ïö©
    3. Ïù¥Î©îÏùº/SMS (ÏòµÏÖò) - ÏôÑÏ†Ñ Ïò§ÌîÑÎùºÏù∏ ÏÇ¨Ïö©ÏûêÏö©
    
    Args:
        owner_id: ÏÇ¨Ïö©Ïûê ID (ÌÖåÎÑåÌä∏ Í≤©Î¶¨)
        conversation_id: ÎåÄÌôî/Ïã§Ìñâ ID
        execution_id: Step Functions Ïã§Ìñâ ID
        context_info: ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ïª®ÌÖçÏä§Ìä∏ Ï†ïÎ≥¥
        payload: Ï†ÑÏ≤¥ payload (segment_to_run Îì± Ï∂îÍ∞Ä Ï†ïÎ≥¥ Ìè¨Ìï®)
    
    Returns:
        dict: Ï†ÑÏÜ° Í≤∞Í≥º {'websocket': bool, 'persistent': bool, 'email': bool}
    """
    import logging
    logger = logging.getLogger()
    
    result = {
        'websocket': False,
        'persistent': False,
        'email': False
    }
    
    # ÏïåÎ¶º ÌéòÏù¥Î°úÎìú Íµ¨ÏÑ±
    notification_payload = {
        'type': 'hitp_pause',
        'action': 'hitp_pause',
        'conversation_id': conversation_id,
        'execution_id': execution_id,
        'workflowId': context_info.get('workflowId') or payload.get('workflowId'),
        'workflow_name': context_info.get('workflow_name'),
        'segment_to_run': context_info.get('segment_to_run') or payload.get('segment_to_run'),
        'total_segments': context_info.get('total_segments') or payload.get('total_segments'),
        'status': 'PAUSED_FOR_HITP',
        'message': 'ÏõåÌÅ¨ÌîåÎ°úÏö∞Í∞Ä ÏùºÏãúÏ†ïÏßÄÎêòÏóàÏäµÎãàÎã§. ÏÇ¨Ïö©Ïûê ÏûÖÎ†•ÏùÑ Í∏∞Îã§Î¶ΩÎãàÎã§.',
        'timestamp': int(time.time()),
        'created_at': int(time.time())
    }
    
    # --- 1. Ïã§ÏãúÍ∞Ñ Push (WebSocket) ---
    try:
        connection_ids = get_user_connection_ids(owner_id)
        if connection_ids:
            websocket_endpoint = os.environ.get('WEBSOCKET_ENDPOINT_URL')
            if websocket_endpoint:
                apigw = boto3.client('apigatewaymanagementapi', endpoint_url=websocket_endpoint)
                payload_bytes = json.dumps({
                    'type': 'workflow_status',
                    'payload': notification_payload
                }, ensure_ascii=False).encode('utf-8')
                
                for conn_id in connection_ids:
                    try:
                        apigw.post_to_connection(ConnectionId=conn_id, Data=payload_bytes)
                        logger.info(f"üì± WebSocket HITP ÏïåÎ¶º Ï†ÑÏÜ°: owner={owner_id}, conn={conn_id}")
                        result['websocket'] = True
                    except apigw.exceptions.GoneException:
                        logger.info(f"Stale connection {conn_id}, will be cleaned up later")
                    except Exception as e:
                        logger.warning(f"Failed to send WebSocket to {conn_id}: {e}")
            else:
                logger.debug("WEBSOCKET_ENDPOINT_URL not configured, skipping WebSocket push")
    except Exception as e:
        logger.warning(f"WebSocket notification failed: {e}")
    
    # --- 2. ÏòÅÍµ¨Ï†Å ÏïåÎ¶º Ï†ÄÏû• (Persistent Notification for Inbox) ---
    try:
        pending_table_name = os.environ.get('PENDING_NOTIFICATIONS_TABLE')
        if pending_table_name:
            pending_table = dynamodb.Table(pending_table_name)
            # üö® [Fix] timestampÎ•º Î∞ÄÎ¶¨Ï¥à(ms)Î°ú Ï†ÄÏû•ÌïòÏó¨ list_notifications.pyÏôÄ ÏùºÏπòÏãúÌÇ¥
            current_time = int(time.time())
            notification_item = {
                'ownerId': owner_id,
                'notificationId': str(uuid.uuid4()),
                'timestamp': current_time * 1000,  # Î∞ÄÎ¶¨Ï¥à Îã®ÏúÑ (ÌîÑÎ°†Ìä∏ÏóîÎìú Ìò∏Ìôò)
                'timestamp_seconds': current_time,  # Ï¥à Îã®ÏúÑ (Î∞±ÏóîÎìú Ìò∏Ìôò)
                'notification': notification_payload,
                'status': 'pending',  # pending = ÎØ∏ÏùΩÏùå, sent = ÏùΩÏùå
                'type': 'hitp_pause',
                'conversation_id': conversation_id,
                'execution_id': execution_id,
                'ttl': current_time + TTLConfig.PENDING_NOTIFICATION  # 30Ïùº Î≥¥Í¥Ä
            }
            pending_table.put_item(Item=notification_item)
            logger.info(f"üíæ ÏòÅÍµ¨ ÏïåÎ¶º Ï†ÄÏû• ÏôÑÎ£å: owner={owner_id}, notification_id={notification_item['notificationId']}")
            result['persistent'] = True
        else:
            logger.debug("PENDING_NOTIFICATIONS_TABLE not configured, skipping persistent notification")
    except Exception as e:
        logger.warning(f"Persistent notification storage failed: {e}")
    
    # --- 3. Ïù¥Î©îÏùº/SMS ÏïåÎ¶º (ÏòµÏÖò, ÌôòÍ≤ΩÎ≥ÄÏàòÎ°ú Ï†úÏñ¥) ---
    # Íµ¨ÌòÑ ÏòàÏ†ï: backend/backend_issues.md Ï∞∏Ï°∞
    if os.environ.get('ENABLE_EMAIL_NOTIFICATIONS', 'false').lower() == 'true':
        try:
            # [ISSUE] SES/SNS Ïù¥Î©îÏùº ÏïåÎ¶º ÎØ∏Íµ¨ÌòÑ - backend_issues.md Ï∞∏Ï°∞
            logger.info(f"üìß Ïù¥Î©îÏùº ÏïåÎ¶ºÏùÄ Ìñ•ÌõÑ Íµ¨ÌòÑ ÏòàÏ†ï: owner={owner_id}")
            result['email'] = False
        except Exception as e:
            logger.warning(f"Email notification failed: {e}")
    
    return result


from decimal import Decimal

def _convert_floats_to_decimals(obj):
    """Recursively convert float values to Decimal for DynamoDB."""
    if isinstance(obj, float):
        return Decimal(str(obj))
    elif isinstance(obj, dict):
        return {k: _convert_floats_to_decimals(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [_convert_floats_to_decimals(v) for v in obj]
    return obj


def _mock_auto_resume(task_token: str, bag: dict, max_retries: int = 3) -> dict:
    """
    MOCK_MODE/AUTO_RESUME_HITPÏóêÏÑú HITP ÏûêÎèô ÏäπÏù∏ - ÌÖåÏä§Ìä∏ ÏãúÎÆ¨Î†àÏù¥ÌÑ∞Ïö©.
    
    Step FunctionsÏùò waitForTaskToken ÏÉÅÌÉúÍ∞Ä Ï§ÄÎπÑÎêòÍ∏∞ Ï†ÑÏóê
    send_task_successÎ•º Ìò∏Ï∂úÌïòÎ©¥ InvalidToken ÏóêÎü¨Í∞Ä Î∞úÏÉùÌï† Ïàò ÏûàÏúºÎØÄÎ°ú
    Ïû¨ÏãúÎèÑ Î°úÏßÅÏùÑ Ìè¨Ìï®Ìï©ÎãàÎã§.
    
    [v3.22] MergeCallbackResultÍ∞Ä Í∏∞ÎåÄÌïòÎäî Ïä§ÌÇ§ÎßàÎ°ú ÏàòÏ†ï:
    - user_response: HITL ÏùëÎãµ Îç∞Ïù¥ÌÑ∞
    - segment_to_run: NoneÏù¥Î©¥ Îã§Ïùå ÏÑ∏Í∑∏Î®ºÌä∏Î°ú ÏßÑÌñâ (_increment_segment=True)
    
    Args:
        task_token: Step Functions TaskToken
        bag: StateBag (open_state_bagÏúºÎ°ú Ï∂îÏ∂úÎêú ÏÉÅÌÉú)
        max_retries: ÏµúÎåÄ Ïû¨ÏãúÎèÑ ÌöüÏàò (Î†àÏù¥Ïä§ Ïª®ÎîîÏÖò ÎåÄÏùë)
    
    Returns:
        dict: ÏûêÎèô resume Í≤∞Í≥º
    """
    import logging
    logger = logging.getLogger()
    
    # [v3.22] MergeCallbackResultÍ∞Ä Í∏∞ÎåÄÌïòÎäî Ïä§ÌÇ§ÎßàÎ°ú resume_output Íµ¨ÏÑ±
    # universal_sync_core.merge_callback Ï∞∏Ï°∞:
    #   - user_response -> delta['last_hitp_response']
    #   - segment_to_run = None -> _increment_segment = True (Îã§Ïùå ÏÑ∏Í∑∏Î®ºÌä∏ ÏßÑÌñâ)
    resume_output = {
        "status": "APPROVED",
        "user_response": {
            "action": "APPROVED",
            "comment": "Auto-approved by Simulator (AUTO_RESUME_HITP)",
            "approved_at": int(time.time()),
            "auto_resume": True
        },
        # segment_to_runÏùÑ NoneÏúºÎ°ú ÏÑ§Ï†ïÌïòÎ©¥ _increment_segment=TrueÎ°ú Ï≤òÎ¶¨Îê®
        "segment_to_run": None
    }
    
    for attempt in range(max_retries):
        try:
            # Î†àÏù¥Ïä§ Ïª®ÎîîÏÖò Î∞©ÏßÄ: Step FunctionsÍ∞Ä ÎåÄÍ∏∞ ÏÉÅÌÉú ÏßÑÏûÖÌï† ÏãúÍ∞Ñ ÌôïÎ≥¥
            if attempt > 0:
                wait_time = 1.0 * attempt  # 1Ï¥à, 2Ï¥à Ï†êÏßÑÏ†Å ÎåÄÍ∏∞
                logger.info(f"‚è≥ Retry {attempt + 1}/{max_retries}: waiting {wait_time}s before send_task_success")
                time.sleep(wait_time)
            
            sfn_client.send_task_success(
                taskToken=task_token,
                output=json.dumps(resume_output, ensure_ascii=False, default=str)
            )
            
            logger.info(f"[v3.22] Auto-resumed HITP task successfully (attempt {attempt + 1})")
            return {
                "status": "AUTO_RESUMED",
                "attempt": attempt + 1,
                "user_response": resume_output.get("user_response")
            }
            
        except sfn_client.exceptions.InvalidToken as e:
            # ÌÜ†ÌÅ∞Ïù¥ ÏïÑÏßÅ Ïú†Ìö®ÌïòÏßÄ ÏïäÏùå - Step FunctionsÍ∞Ä ÎåÄÍ∏∞ ÏÉÅÌÉú ÏßÑÏûÖ Ï†Ñ
            logger.warning(f"InvalidToken on attempt {attempt + 1}: {e}")
            if attempt == max_retries - 1:
                logger.error(f"[v3.22] Failed to auto-resume after {max_retries} attempts")
                return {
                    "status": "AUTO_RESUME_FAILED",
                    "error": "InvalidToken after max retries",
                    "attempts": max_retries
                }
        except sfn_client.exceptions.TaskTimedOut as e:
            # Ïù¥ÎØ∏ ÌÉÄÏûÑÏïÑÏõÉÎê®
            logger.warning(f"Task already timed out: {e}")
            return {
                "status": "AUTO_RESUME_SKIPPED",
                "reason": "Task already timed out"
            }
        except Exception as e:
            logger.error(f"[v3.22] Unexpected error in auto-resume: {e}")
            return {
                "status": "AUTO_RESUME_FAILED",
                "error": str(e)
            }
    
    return {"status": "AUTO_RESUME_FAILED", "error": "Unknown error"}

def lambda_handler(event, context):
    """
    Step Functions WaitForCallbackÏóêÏÑú Ìò∏Ï∂úÎêòÎäî TaskToken Ï†ÄÏû• Lambda.
    
    Í∏∞Îä•:
    - TaskTokenÏùÑ DynamoDBÏóê Ï†ÄÏû•
    - Step Functions Ïû¨ÏãúÏûëÏùÑ ÏúÑÌïú Ïª®ÌÖçÏä§Ìä∏ Î≥¥Ï°¥
    - resume_handlerÏóêÏÑú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎèÑÎ°ù execution context Ï†ÄÏû•
    
    Ï§ëÏöî: Step Functions ÎÇ¥Î∂Ä Ìò∏Ï∂úÏù¥ÎØÄÎ°ú HTTP ÏùëÎãµÏù¥ ÏïÑÎãå ÏàúÏàò JSON Î∞òÌôò
    """
    import logging
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    try:
        # event may already be a dict or a JSON string inside 'Payload' when invoked by Step Functions
        payload = event
        if isinstance(event, dict) and 'Payload' in event:
            # When invoked through Lambda Invoke, Step Functions wraps the original payload under 'Payload'
            payload = event['Payload']
            if isinstance(payload, str):
                try:
                    payload = json.loads(payload)
                except (json.JSONDecodeError, ValueError):
                    payload = {}

        if isinstance(payload, str):
            try:
                payload = json.loads(payload)
            except (json.JSONDecodeError, ValueError):
                payload = {}

        try:
            # ========================================
            # [v3.17] Kernel ProtocolÎ°ú ÏïàÏ†ÑÌïú State Bag Ï∂îÏ∂ú
            # ========================================
            # open_state_bagÏùÄ Ïñ¥Îñ§ Íµ¨Ï°∞Î°ú Ïò§Îì† Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞Î•º Ï∂îÏ∂ú:
            # - state_data.bag (v3.13 ÌëúÏ§Ä)
            # - state_data (ÌèâÌÉÑÌôî)
            # - event ÏûêÏ≤¥ (legacy)
            bag = open_state_bag(payload)
            
            logger.info(f"[v3.17] Kernel Protocol: bag keys={list(bag.keys())[:10]}")
            
            # [v3.18] current_stateÏóêÏÑúÎèÑ ÌïÑÎìú Ï∂îÏ∂ú (ÍπäÏùÄ ÌÉêÏÉâ)
            current_state = bag.get('current_state', {})
            if not isinstance(current_state, dict):
                current_state = {}
            
            # TaskTokenÏùÄ payload ÏµúÏÉÅÏúÑÏóêÏÑúÎßå Ïò¨ Ïàò ÏûàÏùå (SFNÏù¥ ÏßÅÏ†ë Ï£ºÏûÖ)
            task_token = payload.get('TaskToken') or payload.get('taskToken')
            
            # [v3.19] ASL v3ÏóêÏÑúÎäî execution_idÎ•º ÏÇ¨Ïö© (conversation_idÎäî legacy)
            # execution_idÍ∞Ä ÏûàÏúºÎ©¥ Í∑∏Í≤ÉÏùÑ conversation_idÎ°úÎèÑ ÏÇ¨Ïö©
            execution_id = (
                payload.get('execution_id') or 
                payload.get('executionId') or 
                bag.get('execution_id') or
                bag.get('executionId') or
                current_state.get('execution_id')
            )
            
            # conversation_id ÌÉêÏÉâ (execution_id fallback Ìè¨Ìï®)
            conversation_id = (
                payload.get('conversation_id') or 
                payload.get('conversationId') or
                bag.get('conversation_id') or
                bag.get('conversationId') or
                current_state.get('conversation_id') or
                current_state.get('conversationId') or
                execution_id  # v3ÏóêÏÑúÎäî execution_idÎ•º conversation_idÎ°ú ÏÇ¨Ïö©
            )
            
            # execution_idÍ∞Ä ÏóÜÏúºÎ©¥ conversation_idÎ°ú ÏÑ§Ï†ï
            if not execution_id:
                execution_id = conversation_id
            
            owner_id = (
                payload.get('ownerId') or 
                payload.get('owner_id') or
                bag.get('ownerId') or
                bag.get('owner_id') or
                current_state.get('ownerId') or
                current_state.get('owner_id')
            )
            
            # ÌïÑÏàò Í∞í Í≤ÄÏ¶ù
            if not task_token or not conversation_id or not owner_id:
                logger.error(f"Missing required fields: task_token={bool(task_token)}, "
                           f"conversation_id={bool(conversation_id)}, owner_id={bool(owner_id)}, "
                           f"execution_id={bool(execution_id)}, bag_keys={list(bag.keys())[:15]}")
                raise ValueError('Missing TaskToken, conversation_id or ownerId')

            now = int(time.time())
            ttl = now + DEFAULT_TTL_SECONDS

            # Resume HandlerÍ∞Ä ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎèÑÎ°ù Ï∂îÍ∞Ä Ïª®ÌÖçÏä§Ìä∏ Ï†ïÎ≥¥ Ï†ÄÏû•
            # Store the full workflow_config and other state so resume can reconstruct
            # the execution context reliably. Previously only 'workflow_name' was
            # stored which caused resume to miss required fields like workflow_config.
            # [v3.17] bag ÏÇ¨Ïö© (Kernel Protocol)
            # Build context_info with None safety checks
            workflow_config_source = payload.get('workflow_config') or bag.get('workflow_config') or {}
            context_info = {
                'workflow_config': workflow_config_source if isinstance(workflow_config_source, dict) else None,
                'workflowId': payload.get('workflowId') or bag.get('workflowId'),
                'workflow_name': workflow_config_source.get('name') if isinstance(workflow_config_source, dict) else None,
                'segment_to_run': payload.get('segment_to_run') if payload.get('segment_to_run') is not None else bag.get('segment_to_run'),
                'total_segments': payload.get('total_segments') or bag.get('total_segments'),
                'partition_map': payload.get('partition_map') or bag.get('partition_map'),
                'current_state': payload.get('current_state') or bag.get('current_state'),
                # preserve state history if present so resume can reconstruct past snapshots
                'state_history': payload.get('state_history') or bag.get('state_history'),
                'state_s3_path': payload.get('state_s3_path') or bag.get('state_s3_path'),
                'idempotency_key': payload.get('idempotency_key') or bag.get('idempotency_key'),
                'execution_name': payload.get('execution_name'),
                # üö® [Critical Fix] Ï∂îÍ∞Ä ÌïÑÎìú Î≥¥Ï°¥
                'ownerId': owner_id,
                'max_concurrency': payload.get('max_concurrency') or bag.get('max_concurrency'),
                'distributed_mode': payload.get('distributed_mode') or bag.get('distributed_mode'),
            }
            if context and hasattr(context, 'aws_request_id'):
                context_info['request_id'] = context.aws_request_id

            item = {
                # Scope the token to ownerId (tenant) as partition key and conversation_id as sort key
                'ownerId': owner_id,
                'conversation_id': conversation_id,
                # execution_id is unique per execution/run and should be returned to callers so
                # they can reference the execution directly when resuming.
                'execution_id': execution_id,
                'taskToken': task_token,
                'createdAt': now,
                'ttl': ttl,
                # Resume HandlerÍ∞Ä ÏÇ¨Ïö©Ìï† Ï∂îÍ∞Ä Ïª®ÌÖçÏä§Ìä∏ Ï†ÄÏû•
                'context': context_info
            }

            # [Fix] Convert floats to Decimals for DynamoDB
            item = _convert_floats_to_decimals(item)

            # Idempotent write: only create the item if conversation_id doesn't already exist.
            # If this Lambda is retried by Step Functions the conditional put will fail
            # with ConditionalCheckFailedException and we treat that as success (already stored).
            try:
                table.put_item(Item=item, ConditionExpression='attribute_not_exists(conversation_id)')
                logger.info(f"TaskToken stored successfully: conversation_id={conversation_id}, owner_id={owner_id}")
            except ClientError as e:
                # [Fix] None defense: e.response['Error']Í∞Ä NoneÏùº Ïàò ÏûàÏùå
                code = (e.response.get('Error') or {}).get('Code')
                if code == 'ConditionalCheckFailedException':
                    # Already stored by a previous attempt - this is OK (idempotent)
                    logger.info(f"TaskToken already stored (retry detected): conversation_id={conversation_id}")
                else:
                    logger.exception(f"DynamoDB error storing TaskToken: {e}")
                    raise

            # --- üÜï HITP ÏïåÎ¶º Ï†ÑÏÜ° (Ïã§ÏãúÍ∞Ñ Push + ÏòÅÍµ¨ ÏïåÎ¶º) ---
            notification_sent = _send_hitp_notification(
                owner_id=owner_id,
                conversation_id=conversation_id,
                execution_id=execution_id,
                context_info=context_info,
                payload=payload
            )
            logger.info(f"HITP notification sent: {notification_sent}")
            
            # --- üÜï ÏûêÎèô Resume Î°úÏßÅ (MOCK_MODE ÎòêÎäî AUTO_RESUME_HITP) ---
            # 1. MOCK_MODE=true: Ï¶âÏãú ÏûêÎèô ÏäπÏù∏ (E2E ÌÖåÏä§Ìä∏Ïö©)
            # 2. AUTO_RESUME_HITP=true: ÌÉÄÏûÑÏïÑÏõÉ ÌõÑ ÏûêÎèô ÏäπÏù∏ (ÏãúÎÆ¨Î†àÏù¥ÌÑ∞Ïö©)
            # [v3.21] AUTO_RESUME_HITP ÌîåÎûòÍ∑∏ Ï∂îÍ∞Ä - LLM Simulator Î¨¥ÌïúÎåÄÍ∏∞ Ìï¥Í≤∞
            
            mock_mode_value = (
                payload.get('MOCK_MODE') or 
                bag.get('MOCK_MODE') or 
                os.environ.get('MOCK_MODE', 'false')
            )
            mock_mode = str(mock_mode_value).lower() == 'true'
            
            auto_resume_value = (
                payload.get('AUTO_RESUME_HITP') or 
                bag.get('AUTO_RESUME_HITP') or 
                os.environ.get('AUTO_RESUME_HITP', 'false')
            )
            auto_resume_hitp = str(auto_resume_value).lower() == 'true'
            
            # ÏûêÎèô resume ÌÉÄÏûÑÏïÑÏõÉ (Ï¥à) - Í∏∞Î≥∏ 5Ï¥à, ÌôòÍ≤ΩÎ≥ÄÏàòÎ°ú Ï°∞Ï†ï Í∞ÄÎä•
            auto_resume_delay = int(
                payload.get('AUTO_RESUME_DELAY_SECONDS') or
                bag.get('AUTO_RESUME_DELAY_SECONDS') or
                os.environ.get('AUTO_RESUME_DELAY_SECONDS', '5')
            )
            
            mock_resume_result = None
            
            logger.info(f"üîç Auto-resume check: MOCK_MODE={mock_mode}, AUTO_RESUME_HITP={auto_resume_hitp}, delay={auto_resume_delay}s")
            
            if mock_mode or auto_resume_hitp:
                if auto_resume_hitp and not mock_mode and auto_resume_delay > 0:
                    # ÌÉÄÏûÑÏïÑÏõÉ Í∏∞Î∞ò ÏûêÎèô resume: Step FunctionsÍ∞Ä ÎåÄÍ∏∞ ÏÉÅÌÉú ÏßÑÏûÖÌï† ÏãúÍ∞Ñ ÌôïÎ≥¥
                    logger.info(f"‚è≥ AUTO_RESUME_HITP: waiting {auto_resume_delay}s before auto-resume")
                    time.sleep(auto_resume_delay)
                
                resume_mode = "MOCK_MODE" if mock_mode else "AUTO_RESUME_HITP"
                logger.info(f"ü§ñ {resume_mode} detected - initiating auto-resume for HITP")
                mock_resume_result = _mock_auto_resume(
                    task_token=task_token,
                    bag=bag,  # [v3.22] payload -> bag (StateBag)
                    max_retries=3
                )
                logger.info(f"ü§ñ Auto-resume result: {mock_resume_result}")

            # Step Functions expects pure JSON output (not HTTP response format)
            # Return the payload with additional context for next states
            # Important: Include state_data object for PrepareStateAfterPause to reference
            return {
                'message': 'TaskToken stored',
                'conversation_id': conversation_id,
                'ownerId': owner_id,
                'execution_id': execution_id,
                'taskToken': task_token,
                'stored_at': now,
                # MOCK_MODE ÏûêÎèô resume Í≤∞Í≥º Ìè¨Ìï® (ÎîîÎ≤ÑÍπÖÏö©)
                'mock_resume_result': mock_resume_result,
                # Pass through important fields for next Step Functions states
                'workflowId': payload.get('workflowId'),
                'segment_to_run': payload.get('segment_to_run'),
                'workflow_config': payload.get('workflow_config'),
                'partition_map': payload.get('partition_map'),
                'total_segments': payload.get('total_segments'),
                'current_state': payload.get('current_state'),
                'state_s3_path': payload.get('state_s3_path'),
                # State bag pattern: wrap state data for consistent access pattern
                'state_data': {
                    'workflow_config': payload.get('workflow_config'),
                    'partition_map': payload.get('partition_map'),
                    'total_segments': payload.get('total_segments'),
                    'state_history': payload.get('state_history') or (payload.get('state_data') or {}).get('state_history'),
                    'ownerId': owner_id,
                    'workflowId': payload.get('workflowId'),
                    'segment_to_run': payload.get('segment_to_run'),
                    'current_state': payload.get('current_state'),
                    'state_s3_path': payload.get('state_s3_path'),
                    'idempotency_key': payload.get('idempotency_key')
                }
            }
        except Exception as e:
            logger.exception(f"Error in store_task_token: {e}")
            raise
    except Exception as e:
        logger.exception(f"Outer exception in store_task_token: {e}")
        raise
