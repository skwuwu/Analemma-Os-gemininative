import json
import os
import time
import logging
import boto3
from botocore.exceptions import ClientError
import uuid

# ê³µí†µ ëª¨ë“ˆì—ì„œ AWS í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
try:
    from src.common.aws_clients import get_dynamodb_resource, get_stepfunctions_client
    from src.common.constants import DynamoDBConfig
    dynamodb = get_dynamodb_resource()
    sfn_client = get_stepfunctions_client()
except ImportError:
    dynamodb = boto3.resource('dynamodb')
    sfn_client = boto3.client('stepfunctions')

# [v3.17] Kernel Protocol ì‚¬ìš© - ì•ˆì „í•œ state bag ì¶”ì¶œ
try:
    from src.common.kernel_protocol import open_state_bag
    KERNEL_PROTOCOL_AVAILABLE = True
except ImportError:
    KERNEL_PROTOCOL_AVAILABLE = False
    def open_state_bag(event):
        """Fallback: ê¸°ë³¸ ì¶”ì¶œ ë¡œì§"""
        if not isinstance(event, dict):
            return {}
        state_data = event.get('state_data', {})
        if isinstance(state_data, dict):
            bag = state_data.get('bag')
            if isinstance(bag, dict):
                return bag
            return state_data
        return event

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# í™˜ê²½ ë³€ìˆ˜ë¡œ í…Œì´ë¸”ëª… ì§€ì •
TASK_TOKENS_TABLE = DynamoDBConfig.TASK_TOKENS_TABLE

table = dynamodb.Table(TASK_TOKENS_TABLE)

# ê¸°ë³¸ TTL: 1 day (ìƒìˆ˜ ì‚¬ìš©)
try:
    from src.common.constants import TTLConfig, EnvironmentVariables
    DEFAULT_TTL_SECONDS = int(os.environ.get(EnvironmentVariables.TASK_TOKEN_TTL_SECONDS, TTLConfig.TASK_TOKEN_DEFAULT))
except ImportError:
    DEFAULT_TTL_SECONDS = int(os.environ.get('TASK_TOKEN_TTL_SECONDS', 86400))


def get_user_connection_ids(owner_id):
    """
    DynamoDB GSIë¥¼ ì¿¼ë¦¬í•˜ì—¬ ownerIdì— ì—°ê²°ëœ ëª¨ë“  connectionIdë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        list: connectionId ëª©ë¡, ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ
    """
    table_name = DynamoDBConfig.WEBSOCKET_CONNECTIONS_TABLE
    gsi_name = DynamoDBConfig.WEBSOCKET_OWNER_ID_GSI
    
    if not table_name or not owner_id:
        return []
    
    try:
        conn_table = dynamodb.Table(table_name)
        from boto3.dynamodb.conditions import Key
        
        response = conn_table.query(
            IndexName=gsi_name,
            KeyConditionExpression=Key('ownerId').eq(owner_id)
        )
        
        return [item['connectionId'] for item in response.get('Items', []) if 'connectionId' in item]
    except Exception as e:
        import logging
        logger = logging.getLogger()
        logger.error(f'Failed to query connections for owner={owner_id}: {e}')
        return []


def _send_hitp_notification(owner_id, conversation_id, execution_id, context_info, payload):
    """
    HITP ë°œìƒ ì‹œ ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼ì„ ì „ì†¡í•©ë‹ˆë‹¤.
    
    3ê°€ì§€ ì•Œë¦¼ ì±„ë„:
    1. ì‹¤ì‹œê°„ Push (WebSocket) - ì‚¬ìš©ìê°€ ì ‘ì† ì¤‘ì¼ ë•Œ
    2. ì˜êµ¬ì  ì•Œë¦¼ ì €ì¥ (DynamoDB PendingNotifications) - ì•Œë¦¼ ì„¼í„°ìš©
    3. ì´ë©”ì¼/SMS (ì˜µì…˜) - ì™„ì „ ì˜¤í”„ë¼ì¸ ì‚¬ìš©ììš©
    
    Args:
        owner_id: ì‚¬ìš©ì ID (í…Œë„ŒíŠ¸ ê²©ë¦¬)
        conversation_id: ëŒ€í™”/ì‹¤í–‰ ID
        execution_id: Step Functions ì‹¤í–‰ ID
        context_info: ì›Œí¬í”Œë¡œìš° ì»¨í…ìŠ¤íŠ¸ ì •ë³´
        payload: ì „ì²´ payload (segment_to_run ë“± ì¶”ê°€ ì •ë³´ í¬í•¨)
    
    Returns:
        dict: ì „ì†¡ ê²°ê³¼ {'websocket': bool, 'persistent': bool, 'email': bool}
    """
    import logging
    logger = logging.getLogger()
    
    result = {
        'websocket': False,
        'persistent': False,
        'email': False
    }
    
    # ì•Œë¦¼ í˜ì´ë¡œë“œ êµ¬ì„±
    notification_payload = {
        'type': 'hitp_pause',
        'action': 'hitp_pause',
        'conversation_id': conversation_id,
        'execution_id': execution_id,
        'workflowId': context_info.get('workflowId') or payload.get('workflowId'),
        'workflow_name': context_info.get('workflow_name'),
        'segment_to_run': context_info.get('segment_to_run') or payload.get('segment_to_run'),
        'total_segments': context_info.get('total_segments') or payload.get('total_segments'),
        'status': 'PAUSED_FOR_HITP',
        'message': 'ì›Œí¬í”Œë¡œìš°ê°€ ì¼ì‹œì •ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.',
        'timestamp': int(time.time()),
        'created_at': int(time.time())
    }
    
    # --- 1. ì‹¤ì‹œê°„ Push (WebSocket) ---
    try:
        connection_ids = get_user_connection_ids(owner_id)
        if connection_ids:
            websocket_endpoint = os.environ.get('WEBSOCKET_ENDPOINT_URL')
            if websocket_endpoint:
                apigw = boto3.client('apigatewaymanagementapi', endpoint_url=websocket_endpoint)
                payload_bytes = json.dumps({
                    'type': 'workflow_status',
                    'payload': notification_payload
                }, ensure_ascii=False).encode('utf-8')
                
                for conn_id in connection_ids:
                    try:
                        apigw.post_to_connection(ConnectionId=conn_id, Data=payload_bytes)
                        logger.info(f"ğŸ“± WebSocket HITP ì•Œë¦¼ ì „ì†¡: owner={owner_id}, conn={conn_id}")
                        result['websocket'] = True
                    except apigw.exceptions.GoneException:
                        logger.info(f"Stale connection {conn_id}, will be cleaned up later")
                    except Exception as e:
                        logger.warning(f"Failed to send WebSocket to {conn_id}: {e}")
            else:
                logger.debug("WEBSOCKET_ENDPOINT_URL not configured, skipping WebSocket push")
    except Exception as e:
        logger.warning(f"WebSocket notification failed: {e}")
    
    # --- 2. ì˜êµ¬ì  ì•Œë¦¼ ì €ì¥ (Persistent Notification for Inbox) ---
    try:
        pending_table_name = os.environ.get('PENDING_NOTIFICATIONS_TABLE')
        if pending_table_name:
            pending_table = dynamodb.Table(pending_table_name)
            # ğŸš¨ [Fix] timestampë¥¼ ë°€ë¦¬ì´ˆ(ms)ë¡œ ì €ì¥í•˜ì—¬ list_notifications.pyì™€ ì¼ì¹˜ì‹œí‚´
            current_time = int(time.time())
            notification_item = {
                'ownerId': owner_id,
                'notificationId': str(uuid.uuid4()),
                'timestamp': current_time * 1000,  # ë°€ë¦¬ì´ˆ ë‹¨ìœ„ (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜)
                'timestamp_seconds': current_time,  # ì´ˆ ë‹¨ìœ„ (ë°±ì—”ë“œ í˜¸í™˜)
                'notification': notification_payload,
                'status': 'pending',  # pending = ë¯¸ì½ìŒ, sent = ì½ìŒ
                'type': 'hitp_pause',
                'conversation_id': conversation_id,
                'execution_id': execution_id,
                'ttl': current_time + TTLConfig.PENDING_NOTIFICATION  # 30ì¼ ë³´ê´€
            }
            pending_table.put_item(Item=notification_item)
            logger.info(f"ğŸ’¾ ì˜êµ¬ ì•Œë¦¼ ì €ì¥ ì™„ë£Œ: owner={owner_id}, notification_id={notification_item['notificationId']}")
            result['persistent'] = True
        else:
            logger.debug("PENDING_NOTIFICATIONS_TABLE not configured, skipping persistent notification")
    except Exception as e:
        logger.warning(f"Persistent notification storage failed: {e}")
    
    # --- 3. ì´ë©”ì¼/SMS ì•Œë¦¼ (ì˜µì…˜, í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´) ---
    # êµ¬í˜„ ì˜ˆì •: backend/backend_issues.md ì°¸ì¡°
    if os.environ.get('ENABLE_EMAIL_NOTIFICATIONS', 'false').lower() == 'true':
        try:
            # [ISSUE] SES/SNS ì´ë©”ì¼ ì•Œë¦¼ ë¯¸êµ¬í˜„ - backend_issues.md ì°¸ì¡°
            logger.info(f"ğŸ“§ ì´ë©”ì¼ ì•Œë¦¼ì€ í–¥í›„ êµ¬í˜„ ì˜ˆì •: owner={owner_id}")
            result['email'] = False
        except Exception as e:
            logger.warning(f"Email notification failed: {e}")
    
    return result


from decimal import Decimal

def _convert_floats_to_decimals(obj):
    """Recursively convert float values to Decimal for DynamoDB."""
    if isinstance(obj, float):
        return Decimal(str(obj))
    elif isinstance(obj, dict):
        return {k: _convert_floats_to_decimals(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [_convert_floats_to_decimals(v) for v in obj]
    return obj


def _mock_auto_resume(task_token: str, payload: dict, max_retries: int = 3) -> dict:
    """
    MOCK_MODEì—ì„œ HITP ìë™ ìŠ¹ì¸ - í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´í„°ìš©.
    
    Step Functionsì˜ waitForTaskToken ìƒíƒœê°€ ì¤€ë¹„ë˜ê¸° ì „ì—
    send_task_successë¥¼ í˜¸ì¶œí•˜ë©´ InvalidToken ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ
    ì¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•©ë‹ˆë‹¤.
    
    Args:
        task_token: Step Functions TaskToken
        payload: ì›ë³¸ ì´ë²¤íŠ¸ í˜ì´ë¡œë“œ
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ë ˆì´ìŠ¤ ì»¨ë””ì…˜ ëŒ€ì‘)
    
    Returns:
        dict: ìë™ resume ê²°ê³¼
    """
    import logging
    logger = logging.getLogger()
    
    # ìë™ ìŠ¹ì¸ í˜ì´ë¡œë“œ êµ¬ì„±
    resume_output = {
        "status": "COMPLETED",
        "hitp_result": {
            "action": "APPROVED",
            "comment": "Auto-approved by Mission Simulator (MOCK_MODE)",
            "approved_at": int(time.time())
        },
        # PrepareStateAfterPauseê°€ í•„ìš”ë¡œ í•˜ëŠ” í•„ë“œë“¤ ì „ë‹¬
        "final_state": payload.get('current_state', {}),
        "segment_to_run": payload.get('segment_to_run'),
        "workflow_config": payload.get('workflow_config'),
        "partition_map": payload.get('partition_map'),
        "total_segments": payload.get('total_segments'),
        "ownerId": payload.get('ownerId') or payload.get('owner_id'),
        "workflowId": payload.get('workflowId')
    }
    
    for attempt in range(max_retries):
        try:
            # ë ˆì´ìŠ¤ ì»¨ë””ì…˜ ë°©ì§€: Step Functionsê°€ ëŒ€ê¸° ìƒíƒœ ì§„ì…í•  ì‹œê°„ í™•ë³´
            if attempt > 0:
                wait_time = 1.0 * attempt  # 1ì´ˆ, 2ì´ˆ ì ì§„ì  ëŒ€ê¸°
                logger.info(f"â³ Retry {attempt + 1}/{max_retries}: waiting {wait_time}s before send_task_success")
                time.sleep(wait_time)
            
            sfn_client.send_task_success(
                taskToken=task_token,
                output=json.dumps(resume_output, ensure_ascii=False, default=str)
            )
            
            logger.info(f"âœ… MOCK_MODE: Auto-resumed HITP task successfully (attempt {attempt + 1})")
            return {
                "status": "MOCK_RESUMED",
                "attempt": attempt + 1,
                "hitp_result": resume_output.get("hitp_result")
            }
            
        except sfn_client.exceptions.InvalidToken as e:
            # í† í°ì´ ì•„ì§ ìœ íš¨í•˜ì§€ ì•ŠìŒ - Step Functionsê°€ ëŒ€ê¸° ìƒíƒœ ì§„ì… ì „
            logger.warning(f"âš ï¸ InvalidToken on attempt {attempt + 1}: {e}")
            if attempt == max_retries - 1:
                logger.error(f"âŒ MOCK_MODE: Failed to auto-resume after {max_retries} attempts")
                return {
                    "status": "MOCK_RESUME_FAILED",
                    "error": "InvalidToken after max retries",
                    "attempts": max_retries
                }
        except sfn_client.exceptions.TaskTimedOut as e:
            # ì´ë¯¸ íƒ€ì„ì•„ì›ƒë¨
            logger.warning(f"âš ï¸ Task already timed out: {e}")
            return {
                "status": "MOCK_RESUME_SKIPPED",
                "reason": "Task already timed out"
            }
        except Exception as e:
            logger.error(f"âŒ MOCK_MODE: Unexpected error in auto-resume: {e}")
            return {
                "status": "MOCK_RESUME_FAILED",
                "error": str(e)
            }
    
    return {"status": "MOCK_RESUME_FAILED", "error": "Unknown error"}

def lambda_handler(event, context):
    """
    Step Functions WaitForCallbackì—ì„œ í˜¸ì¶œë˜ëŠ” TaskToken ì €ì¥ Lambda.
    
    ê¸°ëŠ¥:
    - TaskTokenì„ DynamoDBì— ì €ì¥
    - Step Functions ì¬ì‹œì‘ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´
    - resume_handlerì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ execution context ì €ì¥
    
    ì¤‘ìš”: Step Functions ë‚´ë¶€ í˜¸ì¶œì´ë¯€ë¡œ HTTP ì‘ë‹µì´ ì•„ë‹Œ ìˆœìˆ˜ JSON ë°˜í™˜
    """
    import logging
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    try:
        # event may already be a dict or a JSON string inside 'Payload' when invoked by Step Functions
        payload = event
        if isinstance(event, dict) and 'Payload' in event:
            # When invoked through Lambda Invoke, Step Functions wraps the original payload under 'Payload'
            payload = event['Payload']
            if isinstance(payload, str):
                try:
                    payload = json.loads(payload)
                except (json.JSONDecodeError, ValueError):
                    payload = {}

        if isinstance(payload, str):
            try:
                payload = json.loads(payload)
            except (json.JSONDecodeError, ValueError):
                payload = {}

        try:
            # ========================================
            # [v3.17] Kernel Protocolë¡œ ì•ˆì „í•œ State Bag ì¶”ì¶œ
            # ========================================
            # open_state_bagì€ ì–´ë–¤ êµ¬ì¡°ë¡œ ì˜¤ë“  ì‹¤ì œ ë°ì´í„°ë¥¼ ì¶”ì¶œ:
            # - state_data.bag (v3.13 í‘œì¤€)
            # - state_data (í‰íƒ„í™”)
            # - event ìì²´ (legacy)
            bag = open_state_bag(payload)
            
            logger.info(f"[v3.17] Kernel Protocol: bag keys={list(bag.keys())[:10]}")
            
            # [v3.18] current_stateì—ì„œë„ í•„ë“œ ì¶”ì¶œ (ê¹Šì€ íƒìƒ‰)
            current_state = bag.get('current_state', {})
            if not isinstance(current_state, dict):
                current_state = {}
            
            # TaskTokenì€ payload ìµœìƒìœ„ì—ì„œë§Œ ì˜¬ ìˆ˜ ìˆìŒ (SFNì´ ì§ì ‘ ì£¼ì…)
            task_token = payload.get('TaskToken') or payload.get('taskToken')
            
            # [v3.19] ASL v3ì—ì„œëŠ” execution_idë¥¼ ì‚¬ìš© (conversation_idëŠ” legacy)
            # execution_idê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ conversation_idë¡œë„ ì‚¬ìš©
            execution_id = (
                payload.get('execution_id') or 
                payload.get('executionId') or 
                bag.get('execution_id') or
                bag.get('executionId') or
                current_state.get('execution_id')
            )
            
            # conversation_id íƒìƒ‰ (execution_id fallback í¬í•¨)
            conversation_id = (
                payload.get('conversation_id') or 
                payload.get('conversationId') or
                bag.get('conversation_id') or
                bag.get('conversationId') or
                current_state.get('conversation_id') or
                current_state.get('conversationId') or
                execution_id  # v3ì—ì„œëŠ” execution_idë¥¼ conversation_idë¡œ ì‚¬ìš©
            )
            
            # execution_idê°€ ì—†ìœ¼ë©´ conversation_idë¡œ ì„¤ì •
            if not execution_id:
                execution_id = conversation_id
            
            owner_id = (
                payload.get('ownerId') or 
                payload.get('owner_id') or
                bag.get('ownerId') or
                bag.get('owner_id') or
                current_state.get('ownerId') or
                current_state.get('owner_id')
            )
            
            # í•„ìˆ˜ ê°’ ê²€ì¦
            if not task_token or not conversation_id or not owner_id:
                logger.error(f"Missing required fields: task_token={bool(task_token)}, "
                           f"conversation_id={bool(conversation_id)}, owner_id={bool(owner_id)}, "
                           f"execution_id={bool(execution_id)}, bag_keys={list(bag.keys())[:15]}")
                raise ValueError('Missing TaskToken, conversation_id or ownerId')

            now = int(time.time())
            ttl = now + DEFAULT_TTL_SECONDS

            # Resume Handlerê°€ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì €ì¥
            # Store the full workflow_config and other state so resume can reconstruct
            # the execution context reliably. Previously only 'workflow_name' was
            # stored which caused resume to miss required fields like workflow_config.
            # [v3.17] bag ì‚¬ìš© (Kernel Protocol)
            # Build context_info with None safety checks
            workflow_config_source = payload.get('workflow_config') or bag.get('workflow_config') or {}
            context_info = {
                'workflow_config': workflow_config_source if isinstance(workflow_config_source, dict) else None,
                'workflowId': payload.get('workflowId') or bag.get('workflowId'),
                'workflow_name': workflow_config_source.get('name') if isinstance(workflow_config_source, dict) else None,
                'segment_to_run': payload.get('segment_to_run') if payload.get('segment_to_run') is not None else bag.get('segment_to_run'),
                'total_segments': payload.get('total_segments') or bag.get('total_segments'),
                'partition_map': payload.get('partition_map') or bag.get('partition_map'),
                'current_state': payload.get('current_state') or bag.get('current_state'),
                # preserve state history if present so resume can reconstruct past snapshots
                'state_history': payload.get('state_history') or bag.get('state_history'),
                'state_s3_path': payload.get('state_s3_path') or bag.get('state_s3_path'),
                'idempotency_key': payload.get('idempotency_key') or bag.get('idempotency_key'),
                'execution_name': payload.get('execution_name'),
                # ğŸš¨ [Critical Fix] ì¶”ê°€ í•„ë“œ ë³´ì¡´
                'ownerId': owner_id,
                'max_concurrency': payload.get('max_concurrency') or bag.get('max_concurrency'),
                'distributed_mode': payload.get('distributed_mode') or bag.get('distributed_mode'),
            }
            if context and hasattr(context, 'aws_request_id'):
                context_info['request_id'] = context.aws_request_id

            item = {
                # Scope the token to ownerId (tenant) as partition key and conversation_id as sort key
                'ownerId': owner_id,
                'conversation_id': conversation_id,
                # execution_id is unique per execution/run and should be returned to callers so
                # they can reference the execution directly when resuming.
                'execution_id': execution_id,
                'taskToken': task_token,
                'createdAt': now,
                'ttl': ttl,
                # Resume Handlerê°€ ì‚¬ìš©í•  ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì €ì¥
                'context': context_info
            }

            # [Fix] Convert floats to Decimals for DynamoDB
            item = _convert_floats_to_decimals(item)

            # Idempotent write: only create the item if conversation_id doesn't already exist.
            # If this Lambda is retried by Step Functions the conditional put will fail
            # with ConditionalCheckFailedException and we treat that as success (already stored).
            try:
                table.put_item(Item=item, ConditionExpression='attribute_not_exists(conversation_id)')
                logger.info(f"TaskToken stored successfully: conversation_id={conversation_id}, owner_id={owner_id}")
            except ClientError as e:
                # [Fix] None defense: e.response['Error']ê°€ Noneì¼ ìˆ˜ ìˆìŒ
                code = (e.response.get('Error') or {}).get('Code')
                if code == 'ConditionalCheckFailedException':
                    # Already stored by a previous attempt - this is OK (idempotent)
                    logger.info(f"TaskToken already stored (retry detected): conversation_id={conversation_id}")
                else:
                    logger.exception(f"DynamoDB error storing TaskToken: {e}")
                    raise

            # --- ğŸ†• HITP ì•Œë¦¼ ì „ì†¡ (ì‹¤ì‹œê°„ Push + ì˜êµ¬ ì•Œë¦¼) ---
            notification_sent = _send_hitp_notification(
                owner_id=owner_id,
                conversation_id=conversation_id,
                execution_id=execution_id,
                context_info=context_info,
                payload=payload
            )
            logger.info(f"HITP notification sent: {notification_sent}")
            
            # --- ğŸ†• MOCK_MODE: ìë™ Resume (ì‹œë®¬ë ˆì´í„° E2E í…ŒìŠ¤íŠ¸ìš©) ---
            # MOCK_MODEê°€ í™œì„±í™”ëœ ê²½ìš° ì‚¬ëŒì˜ ìŠ¹ì¸ì„ ëª¨í‚¹í•˜ì—¬ ìë™ìœ¼ë¡œ ì›Œí¬í”Œë¡œìš° ì¬ê°œ
            # MOCK_MODEëŠ” payload ì§ì ‘, bag ë‚´ë¶€, ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŒ
            # [v3.20] state_data â†’ bag ë³€ê²½ (Kernel Protocol í‘œì¤€)
            mock_mode_value = (
                payload.get('MOCK_MODE') or 
                bag.get('MOCK_MODE') or 
                os.environ.get('MOCK_MODE', 'false')
            )
            mock_mode = str(mock_mode_value).lower() == 'true'
            mock_resume_result = None
            
            logger.info(f"ğŸ” MOCK_MODE check: payload={payload.get('MOCK_MODE')}, bag={bag.get('MOCK_MODE')}, env={os.environ.get('MOCK_MODE')}, resolved={mock_mode}")
            
            if mock_mode:
                logger.info(f"ğŸ¤– MOCK_MODE detected - initiating auto-resume for HITP")
                mock_resume_result = _mock_auto_resume(
                    task_token=task_token,
                    payload=payload,
                    max_retries=3
                )
                logger.info(f"ğŸ¤– MOCK auto-resume result: {mock_resume_result}")

            # Step Functions expects pure JSON output (not HTTP response format)
            # Return the payload with additional context for next states
            # Important: Include state_data object for PrepareStateAfterPause to reference
            return {
                'message': 'TaskToken stored',
                'conversation_id': conversation_id,
                'ownerId': owner_id,
                'execution_id': execution_id,
                'taskToken': task_token,
                'stored_at': now,
                # MOCK_MODE ìë™ resume ê²°ê³¼ í¬í•¨ (ë””ë²„ê¹…ìš©)
                'mock_resume_result': mock_resume_result,
                # Pass through important fields for next Step Functions states
                'workflowId': payload.get('workflowId'),
                'segment_to_run': payload.get('segment_to_run'),
                'workflow_config': payload.get('workflow_config'),
                'partition_map': payload.get('partition_map'),
                'total_segments': payload.get('total_segments'),
                'current_state': payload.get('current_state'),
                'state_s3_path': payload.get('state_s3_path'),
                # State bag pattern: wrap state data for consistent access pattern
                'state_data': {
                    'workflow_config': payload.get('workflow_config'),
                    'partition_map': payload.get('partition_map'),
                    'total_segments': payload.get('total_segments'),
                    'state_history': payload.get('state_history') or (payload.get('state_data') or {}).get('state_history'),
                    'ownerId': owner_id,
                    'workflowId': payload.get('workflowId'),
                    'segment_to_run': payload.get('segment_to_run'),
                    'current_state': payload.get('current_state'),
                    'state_s3_path': payload.get('state_s3_path'),
                    'idempotency_key': payload.get('idempotency_key')
                }
            }
        except Exception as e:
            logger.exception(f"Error in store_task_token: {e}")
            raise
    except Exception as e:
        logger.exception(f"Outer exception in store_task_token: {e}")
        raise
