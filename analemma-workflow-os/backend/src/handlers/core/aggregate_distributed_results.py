"""
Distributed Map ì‹¤í–‰ ê²°ê³¼ë“¤ì„ ì§‘ê³„í•˜ì—¬ ìµœì¢… ì›Œí¬í”Œë¡œìš° ìƒíƒœ ìƒì„±

ëª¨ë“  ì²­í¬ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ê³  ë³‘í•©í•˜ì—¬ 
ë‹¨ì¼ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ê²°ê³¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.
"""

import json
import logging
import os
import time
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import reduce
from src.common.constants import DynamoDBConfig

# ğŸš€ [Last-mile Optimization] ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
MAX_PARALLEL_S3_FETCHES = int(os.environ.get('MAX_PARALLEL_S3_FETCHES', '50'))
HIERARCHICAL_MERGE_THRESHOLD = int(os.environ.get('HIERARCHICAL_MERGE_THRESHOLD', '100'))
MERGE_BATCH_SIZE = int(os.environ.get('MERGE_BATCH_SIZE', '10'))

logger = logging.getLogger(__name__)

def lambda_handler(event: Dict[str, Any], context: Any = None) -> Dict[str, Any]:
    """
    Distributed Map ì‹¤í–‰ ê²°ê³¼ë“¤ì„ ì§‘ê³„í•˜ì—¬ ìµœì¢… ì›Œí¬í”Œë¡œìš° ìƒíƒœ ìƒì„±
    
    ğŸš¨ [Critical Fix] S3ì—ì„œ ê²°ê³¼ë¥¼ ì½ì–´ì„œ í˜ì´ë¡œë“œ ì œí•œ í•´ê²°
    
    ğŸš€ [Hybrid Mode] MAP_REDUCE / BATCHED ëª¨ë“œ ì§€ì›
    
    Args:
        event: {
            # ê¸°ì¡´ ë¶„ì‚° ë§µ ëª¨ë“œ
            "distributed_results_s3_path": "s3://bucket/key" (S3 ì‚¬ìš© ì‹œ),
            "distributed_results": [...] (ì¸ë¼ì¸ ì‚¬ìš© ì‹œ),
            "state_data": {...},
            "use_s3_results": boolean,
            
            # ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ (MAP_REDUCE / BATCHED)
            "execution_mode": "MAP_REDUCE" | "BATCHED",
            "map_results": [...] (MAP_REDUCE ëª¨ë“œ),
            "batch_results": [...] (BATCHED ëª¨ë“œ),
            "ownerId": str,
            "workflowId": str
        }
        
    Returns:
        ì§‘ê³„ëœ ìµœì¢… ê²°ê³¼
    """
    try:
        # ğŸš€ [Hybrid Mode] ì‹¤í–‰ ëª¨ë“œ ê°ì§€
        execution_mode = event.get('execution_mode')
        
        if execution_mode == 'MAP_REDUCE':
            return _aggregate_map_reduce_results(event)
        elif execution_mode == 'BATCHED':
            return _aggregate_batched_results(event)
        
        # ê¸°ì¡´ ë¶„ì‚° ë§µ ëª¨ë“œ ì²˜ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
        use_s3_results = event.get('use_s3_results', False)
        state_data = event.get('state_data', {})
        
        # ğŸš¨ [Critical Fix] S3ì—ì„œ ê²°ê³¼ ë¡œë“œ ë˜ëŠ” ì¸ë¼ì¸ ê²°ê³¼ ì‚¬ìš©
        if use_s3_results and event.get('distributed_results_s3_path'):
            distributed_results = _load_results_from_s3(event['distributed_results_s3_path'])
            logger.info(f"Loaded distributed results from src.S3: {event['distributed_results_s3_path']}")
        else:
            distributed_results = event.get('distributed_results', [])
            logger.info(f"Using inline distributed results: {len(distributed_results)} items")
        
        logger.info(f"Aggregating results from {len(distributed_results)} distributed chunks")
        
        if not distributed_results:
            logger.warning("No distributed results to aggregate")
            return _build_aggregation_response(
                status="FAILED",
                error="No distributed results provided",
                total_chunks=0
            )
        
        # ê²°ê³¼ ë¶„ë¥˜ ë° ê²€ì¦
        successful_chunks = []
        failed_chunks = []
        partial_chunks = []
        paused_chunks = []  # ğŸ¯ HITP ëŒ€ê¸° ì²­í¬ ì¶”ê°€
        
        for result in distributed_results:
            if not isinstance(result, dict):
                logger.warning(f"Invalid result format: {type(result)}")
                continue
                
            chunk_status = result.get('status', 'UNKNOWN')
            if chunk_status == 'COMPLETED':
                successful_chunks.append(result)
            elif chunk_status == 'FAILED':
                failed_chunks.append(result)
            elif chunk_status == 'PARTIAL_FAILURE':
                partial_chunks.append(result)
            elif chunk_status == 'PAUSED_FOR_HITP':
                paused_chunks.append(result)
            elif chunk_status == 'ASYNC_CHILD_WORKFLOW_STARTED':
                # ğŸ¯ [Critical] Fire and Forget: Treat async launch as success
                # But track it separately for logging
                successful_chunks.append(result)
                logger.info(f"Async child workflow launched: {result.get('executionName')}")
            else:
                logger.warning(f"Unknown chunk status: {chunk_status}")
                failed_chunks.append(result)
        
        total_chunks = len(distributed_results)
        successful_count = len(successful_chunks)
        failed_count = len(failed_chunks)
        partial_count = len(partial_chunks)
        paused_count = len(paused_chunks)
        
        logger.info(f"Chunk results: {successful_count} successful (inc. async), {failed_count} failed, {partial_count} partial, {paused_count} paused")
        
        # ğŸ¯ [Critical] HITP ëŒ€ê¸° ìƒíƒœ ì²˜ë¦¬
        if paused_count > 0:
            return _build_aggregation_response(
                status="PAUSED_FOR_HITP",
                successful_chunks=successful_count,
                failed_chunks=failed_count,
                paused_chunks=paused_count,
                total_chunks=total_chunks,
                paused_chunk_details=paused_chunks,
                message=f"Workflow paused: {paused_count} chunks waiting for human input"
            )
        
        # ì‹¤íŒ¨ ì²˜ë¦¬ ì •ì±… ê²°ì •
        failure_policy = os.environ.get('DISTRIBUTED_FAILURE_POLICY', 'fail_on_any_failure')
        
        if failure_policy == 'fail_on_any_failure' and (failed_count > 0 or partial_count > 0):
            return _build_aggregation_response(
                status="FAILED",
                successful_chunks=successful_count,
                failed_chunks=failed_count + partial_count,
                total_chunks=total_chunks,
                failed_chunk_details=failed_chunks + partial_chunks,
                error=f"Distributed execution failed: {failed_count} failed chunks, {partial_count} partial failures"
            )
        elif failure_policy == 'fail_on_majority_failure' and failed_count > successful_count:
            return _build_aggregation_response(
                status="FAILED",
                successful_chunks=successful_count,
                failed_chunks=failed_count,
                total_chunks=total_chunks,
                failed_chunk_details=failed_chunks,
                error=f"Majority of chunks failed: {failed_count}/{total_chunks}"
            )
        
        # ğŸ¯ [Critical] ìµœì‹  ìƒíƒœë¥¼ DynamoDB/S3ì—ì„œ ë¡œë“œ
        final_state = _load_latest_state(state_data)
        
        # ì„±ê³µí•œ ì²­í¬ë“¤ì˜ ê²°ê³¼ ë³‘í•©
        aggregated_logs = []
        execution_summary = {
            'distributed_mode': True,
            'total_chunks': total_chunks,
            'successful_chunks': successful_count,
            'failed_chunks': failed_count,
            'partial_chunks': partial_count,
            'paused_chunks': paused_count,
            'chunk_details': [],
            'total_segments_processed': 0,
            'total_execution_time': 0,
            'aggregation_timestamp': datetime.now(timezone.utc).isoformat(),
            'state_continuity_method': 'latest_pointer'
        }
        
        # ì²­í¬ë³„ ê²°ê³¼ ë³‘í•©
        for chunk_result in successful_chunks + partial_chunks:
            chunk_id = chunk_result.get('chunk_id', 'unknown')
            chunk_results = chunk_result.get('chunk_results', [])
            
            # ë¡œê·¸ ë³‘í•©
            chunk_logs = []
            for segment_result in chunk_results:
                if isinstance(segment_result, dict) and segment_result.get('result'):
                    segment_logs = segment_result['result'].get('new_history_logs', [])
                    if isinstance(segment_logs, list):
                        chunk_logs.extend(segment_logs)
            
            if chunk_logs:
                aggregated_logs.extend(chunk_logs)
            
            # ì‹¤í–‰ í†µê³„ ìˆ˜ì§‘
            processed_segments = chunk_result.get('processed_segments', 0)
            execution_time = chunk_result.get('execution_time', 0)
            
            execution_summary['total_segments_processed'] += processed_segments
            execution_summary['total_execution_time'] = max(
                execution_summary['total_execution_time'], 
                execution_time
            )  # ë³‘ë ¬ ì‹¤í–‰ì´ë¯€ë¡œ ìµœëŒ€ê°’ ì‚¬ìš©
            
            execution_summary['chunk_details'].append({
                'chunk_id': chunk_id,
                'status': chunk_result.get('status'),
                'processed_segments': processed_segments,
                'execution_time': execution_time,
                'start_segment': chunk_result.get('start_segment'),
                'end_segment': chunk_result.get('end_segment')
            })
        
        # ìµœì¢… ìƒíƒœ ê²°ì •
        if successful_count == total_chunks:
            final_status = "COMPLETED"
        elif successful_count > 0:
            final_status = "PARTIAL_SUCCESS"
        else:
            final_status = "FAILED"
        
        # ë¡œê·¸ ì •ë ¬ (ì‹œê°„ìˆœ) - ëŒ€ìš©ëŸ‰ ë¡œê·¸ ì²˜ë¦¬ ìµœì í™”
        if _should_defer_sorting_to_client(len(aggregated_logs)):
            # í´ë¼ì´ì–¸íŠ¸ ì¸¡ ì •ë ¬ ê¶Œì¥
            execution_summary['client_side_sorting_recommended'] = True
            execution_summary['unsorted_log_count'] = len(aggregated_logs)
            logger.info(f"Deferring log sorting to client: {len(aggregated_logs)} logs")
        else:
            aggregated_logs = _sort_logs_by_timestamp(aggregated_logs)
        
        # ğŸ¯ [Critical Fix] ì§‘ê³„ ì™„ë£Œ í›„ ìµœì¢… ìƒíƒœë¥¼ DynamoDBì— ì €ì¥
        final_execution_state = {
            **final_state,
            'execution_summary': execution_summary,
            'aggregated_logs': aggregated_logs if len(aggregated_logs) < 1000 else [],  # ëŒ€ìš©ëŸ‰ ë¡œê·¸ëŠ” S3ì— ì €ì¥
            'aggregation_completed': True,
            'final_status': final_status,
            'completion_timestamp': datetime.now(timezone.utc).isoformat()
        }
        
        # ìµœì¢… ìƒíƒœ ì €ì¥ - [Fix] S3 ê²½ë¡œ ë°˜í™˜ê°’ ì‚¬ìš©
        final_state_s3_path = _save_final_state(state_data, final_execution_state, execution_summary)
        
        # ğŸ¯ [Optimization] ì¤‘ê°„ ìƒíƒœ ì •ë¦¬ (í™˜ê²½ ë³€ìˆ˜ë¡œ ì œì–´)
        cleanup_enabled = os.environ.get('CLEANUP_INTERMEDIATE_STATES', 'true').lower() == 'true'
        if cleanup_enabled:
            try:
                execution_id = state_data.get('workflowId', 'unknown')
                _cleanup_intermediate_states(execution_id)
            except Exception as cleanup_error:
                logger.warning(f"Cleanup failed but continuing: {cleanup_error}")
        else:
            logger.info("Intermediate state cleanup disabled by configuration")
        
        logger.info(f"Aggregation completed: {final_status}, {execution_summary['total_segments_processed']} segments processed")
        
        return _build_aggregation_response(
            status=final_status,
            final_state=final_state,
            final_state_s3_path=final_state_s3_path,  # [Fix] S3 ê²½ë¡œ ì „ë‹¬
            total_segments_processed=execution_summary['total_segments_processed'],
            total_chunks=total_chunks,
            successful_chunks=successful_count,
            failed_chunks=failed_count,
            execution_summary=execution_summary,
            all_results=aggregated_logs,
            aggregation_metadata={
                'aggregation_time': time.time(),
                'distributed_execution': True,
                'failure_policy': failure_policy,
                'state_continuity_ensured': True
            }
        )
        
    except Exception as e:
        logger.exception("Failed to aggregate distributed results")
        return _build_aggregation_response(
            status="FAILED",
            error=f"Aggregation failed: {str(e)}",
            total_chunks=len(event.get('distributed_results', []))
        )


def _merge_states(base_state: Dict[str, Any], chunk_state: Dict[str, Any], chunk_id: str) -> Dict[str, Any]:
    """
    ë‘ ìƒíƒœë¥¼ ì•ˆì „í•˜ê²Œ ë³‘í•©
    
    Args:
        base_state: ê¸°ë³¸ ìƒíƒœ
        chunk_state: ì²­í¬ì—ì„œ ìƒì„±ëœ ìƒíƒœ
        chunk_id: ì²­í¬ ì‹ë³„ì
        
    Returns:
        ë³‘í•©ëœ ìƒíƒœ
    """
    if not isinstance(chunk_state, dict):
        return base_state
    
    merged = base_state.copy()
    
    # ì²­í¬ë³„ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê²©ë¦¬
    if 'chunks' not in merged:
        merged['chunks'] = {}
    
    # ì²­í¬ ê²°ê³¼ë¥¼ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì €ì¥
    merged['chunks'][chunk_id] = chunk_state
    
    # ê¸€ë¡œë²Œ ìƒíƒœ ë³‘í•© (í‚¤ ì¶©ëŒ ë°©ì§€)
    for key, value in chunk_state.items():
        if key.startswith('__'):
            # ì‹œìŠ¤í…œ í‚¤ëŠ” ë¬´ì‹œ
            continue
        elif key in merged and key != 'chunks':
            # ê¸°ì¡´ í‚¤ê°€ ìˆìœ¼ë©´ ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ë³´ì¡´
            if not isinstance(merged[key], list):
                merged[key] = [merged[key]]
            if isinstance(merged[key], list):
                merged[key].append(value)
        else:
            # ìƒˆë¡œìš´ í‚¤ëŠ” ì§ì ‘ ì¶”ê°€
            merged[key] = value
    
    return merged


def _sort_logs_by_timestamp(logs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ë¡œê·¸ë¥¼ íƒ€ì„ìŠ¤íƒ¬í”„ ìˆœìœ¼ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ì •ë ¬
    
    ğŸš¨ [Performance Fix] ëŒ€ìš©ëŸ‰ ë¡œê·¸ ì²˜ë¦¬ ìµœì í™”
    """
    if not logs:
        return []
    
    # ğŸš¨ [Critical] ëŒ€ìš©ëŸ‰ ë¡œê·¸ ê°ì§€ ë° ì²˜ë¦¬ ë°©ì‹ ê²°ì •
    log_count = len(logs)
    memory_threshold = 10000  # 10K ë¡œê·¸ ì´ìƒ ì‹œ ìµœì í™” ì ìš©
    
    if log_count > memory_threshold:
        logger.warning(f"Large log dataset detected: {log_count} entries. Using optimized sorting.")
        return _sort_logs_optimized(logs)
    
    # ì¼ë°˜ í¬ê¸°ëŠ” ê¸°ì¡´ ë°©ì‹ ìœ ì§€
    def get_timestamp(log_entry):
        if isinstance(log_entry, dict):
            # ë‹¤ì–‘í•œ íƒ€ì„ìŠ¤íƒ¬í”„ í•„ë“œ ì§€ì›
            for ts_field in ['timestamp', 'created_at', 'time', 'date']:
                if ts_field in log_entry:
                    try:
                        if isinstance(log_entry[ts_field], (int, float)):
                            return log_entry[ts_field]
                        elif isinstance(log_entry[ts_field], str):
                            # ISO í˜•ì‹ íŒŒì‹± ì‹œë„
                            from datetime import datetime
                            return datetime.fromisoformat(log_entry[ts_field].replace('Z', '+00:00')).timestamp()
                    except:
                        continue
        return 0  # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì²˜ë¦¬
    
    try:
        return sorted(logs, key=get_timestamp)
    except Exception as e:
        logger.warning(f"Failed to sort logs by timestamp: {e}")
        return logs


def _sort_logs_optimized(logs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ëŒ€ìš©ëŸ‰ ë¡œê·¸ë¥¼ ìœ„í•œ ìµœì í™”ëœ ì •ë ¬
    
    ğŸš¨ [Performance Fix] ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ëŒ€ìš©ëŸ‰ ë¡œê·¸ ì •ë ¬
    
    ì „ëµ:
    1. ì²­í¬ë³„ë¡œ ì´ë¯¸ ì •ë ¬ëœ ë¡œê·¸ë“¤ì„ Merge Sort ë°©ì‹ìœ¼ë¡œ ë³‘í•©
    2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì œí•œí•˜ë©´ì„œ ìŠ¤íŠ¸ë¦¬ë° ì •ë ¬
    3. í•„ìš”ì‹œ ì„ì‹œ íŒŒì¼ ì‚¬ìš©
    """
    try:
        import heapq
        from collections import defaultdict
        
        # ğŸ¯ [Optimization] ì²­í¬ë³„ ë¡œê·¸ ê·¸ë£¹í•‘ (ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
        chunk_logs = defaultdict(list)
        
        for log_entry in logs:
            if isinstance(log_entry, dict):
                # ì²­í¬ ID ì¶”ì¶œ (ë¡œê·¸ì— ì²­í¬ ì •ë³´ê°€ ìˆë‹¤ê³  ê°€ì •)
                chunk_id = log_entry.get('chunk_id', 'default')
                chunk_logs[chunk_id].append(log_entry)
        
        # ê° ì²­í¬ ë‚´ì—ì„œ ì •ë ¬ (ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆì„ ê°€ëŠ¥ì„± ë†’ìŒ)
        sorted_chunks = []
        for chunk_id, chunk_log_list in chunk_logs.items():
            sorted_chunk = sorted(chunk_log_list, key=_extract_timestamp)
            sorted_chunks.append((chunk_id, sorted_chunk))
        
        # ğŸ¯ [Critical] K-way merge using heap
        result = []
        heap = []
        chunk_iterators = {}
        
        # ê° ì²­í¬ì˜ ì²« ë²ˆì§¸ ë¡œê·¸ë¥¼ í™ì— ì¶”ê°€
        for chunk_id, sorted_chunk in sorted_chunks:
            if sorted_chunk:
                chunk_iter = iter(sorted_chunk)
                first_log = next(chunk_iter)
                timestamp = _extract_timestamp(first_log)
                heapq.heappush(heap, (timestamp, chunk_id, first_log))
                chunk_iterators[chunk_id] = chunk_iter
        
        # K-way merge ìˆ˜í–‰
        while heap:
            timestamp, chunk_id, log_entry = heapq.heappop(heap)
            result.append(log_entry)
            
            # í•´ë‹¹ ì²­í¬ì—ì„œ ë‹¤ìŒ ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°
            try:
                next_log = next(chunk_iterators[chunk_id])
                next_timestamp = _extract_timestamp(next_log)
                heapq.heappush(heap, (next_timestamp, chunk_id, next_log))
            except StopIteration:
                # í•´ë‹¹ ì²­í¬ì˜ ë¡œê·¸ê°€ ëª¨ë‘ ì†Œì§„ë¨
                pass
        
        logger.info(f"Optimized sorting completed: {len(result)} logs processed")
        return result
        
    except Exception as e:
        logger.error(f"Optimized sorting failed, falling back to standard sort: {e}")
        # ì‹¤íŒ¨ ì‹œ í‘œì¤€ ì •ë ¬ë¡œ í´ë°±
        return sorted(logs, key=_extract_timestamp)


def _extract_timestamp(log_entry: Dict[str, Any]) -> float:
    """
    ë¡œê·¸ ì—”íŠ¸ë¦¬ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ (ìµœì í™”ëœ ë²„ì „)
    """
    if not isinstance(log_entry, dict):
        return 0.0
    
    # ì„±ëŠ¥ì„ ìœ„í•´ ê°€ì¥ ì¼ë°˜ì ì¸ í•„ë“œë¶€í„° í™•ì¸
    timestamp_fields = ['timestamp', 'created_at', 'time', 'date']
    
    for ts_field in timestamp_fields:
        if ts_field in log_entry:
            ts_value = log_entry[ts_field]
            
            # ìˆ«ìí˜• íƒ€ì„ìŠ¤íƒ¬í”„ (ê°€ì¥ ë¹ ë¦„)
            if isinstance(ts_value, (int, float)):
                return float(ts_value)
            
            # ë¬¸ìì—´ íƒ€ì„ìŠ¤íƒ¬í”„
            elif isinstance(ts_value, str):
                try:
                    # ISO í˜•ì‹ ìµœì í™”ëœ íŒŒì‹±
                    if 'T' in ts_value:  # ISO 8601 í˜•ì‹
                        from datetime import datetime
                        # Zë¥¼ +00:00ìœ¼ë¡œ ë³€í™˜
                        normalized = ts_value.replace('Z', '+00:00')
                        return datetime.fromisoformat(normalized).timestamp()
                    else:
                        # ë‹¤ë¥¸ í˜•ì‹ ì‹œë„
                        return float(ts_value)
                except (ValueError, TypeError):
                    continue
    
    return 0.0


def _should_defer_sorting_to_client(log_count: int) -> bool:
    """
    í´ë¼ì´ì–¸íŠ¸ ì¸¡ ì •ë ¬ì„ ê¶Œì¥í• ì§€ ê²°ì •
    
    ğŸ¯ [Strategy] ëŒ€ìš©ëŸ‰ ë¡œê·¸ëŠ” í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì •ë ¬í•˜ë„ë¡ ê¶Œì¥
    """
    # í™˜ê²½ ë³€ìˆ˜ë¡œ ì„ê³„ê°’ ì„¤ì • ê°€ëŠ¥
    client_sort_threshold = int(os.environ.get('CLIENT_SORT_THRESHOLD', '50000'))
    
    if log_count > client_sort_threshold:
        logger.info(f"Recommending client-side sorting for {log_count} logs (threshold: {client_sort_threshold})")
        return True
    
    return False


def _load_results_from_s3(s3_path: str) -> List[Dict[str, Any]]:
    """
    S3ì—ì„œ ë¶„ì‚° ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë¡œë“œ
    
    ğŸš¨ [Performance Fix] ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²­í¬ ë‹¨ìœ„ ë¡œë”©
    
    Args:
        s3_path: S3 ê²½ë¡œ (s3://bucket/key)
        
    Returns:
        ë¶„ì‚° ì‹¤í–‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    try:
        import boto3
        s3_client = boto3.client('s3')
        
        # S3 ê²½ë¡œ íŒŒì‹±
        if not s3_path.startswith('s3://'):
            raise ValueError(f"Invalid S3 path format: {s3_path}")
        
        bucket, key = s3_path[5:].split('/', 1)
        
        # ğŸš¨ [Critical Fix] íŒŒì¼ í¬ê¸° ë¨¼ì € í™•ì¸
        head_response = s3_client.head_object(Bucket=bucket, Key=key)
        file_size = head_response['ContentLength']
        
        # ë©”ëª¨ë¦¬ ì œí•œ í™•ì¸ (ëŒë‹¤ ë©”ëª¨ë¦¬ì˜ 80% ì´í•˜ë¡œ ì œí•œ)
        lambda_memory_mb = int(os.environ.get('AWS_LAMBDA_FUNCTION_MEMORY_SIZE', '512'))
        max_file_size = (lambda_memory_mb * 1024 * 1024) * 0.8  # 80% ì œí•œ
        
        if file_size > max_file_size:
            logger.warning(f"Large S3 file detected: {file_size} bytes (limit: {max_file_size})")
            # ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
            return _load_large_results_streaming(s3_client, bucket, key, file_size)
        
        # ì¼ë°˜ í¬ê¸° íŒŒì¼ì€ ê¸°ì¡´ ë°©ì‹
        response = s3_client.get_object(Bucket=bucket, Key=key)
        results_json = response['Body'].read().decode('utf-8')
        
        # ğŸš¨ [Memory Optimization] JSON íŒŒì‹± ì „ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…
        import sys
        memory_before = sys.getsizeof(results_json)
        logger.info(f"JSON string size: {memory_before} bytes")
        
        results = json.loads(results_json)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del results_json
        
        if not isinstance(results, list):
            logger.warning(f"Expected list from src.S3, got {type(results)}")
            return []
        
        logger.info(f"Loaded {len(results)} results from src.S3: {s3_path}")
        return results
        
    except Exception as e:
        logger.error(f"Failed to load results from src.S3 {s3_path}: {e}")
        return []


def _load_large_results_streaming(s3_client, bucket: str, key: str, file_size: int) -> List[Dict[str, Any]]:
    """
    ëŒ€ìš©ëŸ‰ S3 íŒŒì¼ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
    
    ğŸš¨ [Critical Fix] ijson ì˜ì¡´ì„± ê°•í™” ë° í´ë°± ë¡œì§ ê°œì„ 
    """
    try:
        # ğŸš¨ [Dependency Check] ijson ê°€ìš©ì„± í™•ì¸
        try:
            import ijson
            logger.info(f"Using ijson streaming parser for large file: {file_size} bytes")
            return _load_with_ijson_streaming(s3_client, bucket, key)
        except ImportError as ijson_error:
            logger.warning(f"ijson not available ({ijson_error}), using enhanced fallback")
            return _load_results_enhanced_fallback(s3_client, bucket, key, file_size)
        
    except Exception as e:
        logger.error(f"Streaming load failed: {e}")
        return []


def _load_with_ijson_streaming(s3_client, bucket: str, key: str) -> List[Dict[str, Any]]:
    """
    ijsonì„ ì‚¬ìš©í•œ ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë° íŒŒì‹±
    """
    import ijson
    
    # S3 ìŠ¤íŠ¸ë¦¬ë° ê°ì²´ ìƒì„±
    response = s3_client.get_object(Bucket=bucket, Key=key)
    stream = response['Body']
    
    # ijsonì„ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° íŒŒì‹±
    results = []
    
    try:
        # JSON ë°°ì—´ì˜ ê° í•­ëª©ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ íŒŒì‹±
        parser = ijson.items(stream, 'item')
        
        chunk_count = 0
        for chunk_result in parser:
            results.append(chunk_result)
            chunk_count += 1
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ (1000ê°œë§ˆë‹¤)
            if chunk_count % 1000 == 0:
                logger.info(f"Processed {chunk_count} chunks via ijson streaming")
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
                import sys
                current_memory = sys.getsizeof(results)
                if current_memory > 500 * 1024 * 1024:  # 500MB ì œí•œ
                    logger.warning(f"High memory usage detected: {current_memory} bytes")
        
        logger.info(f"ijson streaming completed: {len(results)} results")
        return results
        
    except Exception as ijson_parse_error:
        logger.error(f"ijson parsing failed: {ijson_parse_error}")
        # ijson íŒŒì‹± ì‹¤íŒ¨ ì‹œ í–¥ìƒëœ í´ë°±ìœ¼ë¡œ ì „í™˜
        stream.close()
        return _load_results_enhanced_fallback(s3_client, bucket, key, None)


def _load_results_enhanced_fallback(s3_client, bucket: str, key: str, file_size: Optional[int]) -> List[Dict[str, Any]]:
    """
    ğŸš¨ [Enhanced Fallback] ijsonì´ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•  ë•Œì˜ í–¥ìƒëœ ëŒ€ì•ˆ
    
    ê¸°ì¡´ ì²­í¬ ë‹¨ìœ„ ì½ê¸°ë³´ë‹¤ ì •êµí•œ JSON íŒŒì‹± êµ¬í˜„
    """
    try:
        logger.info("Using enhanced fallback JSON parsing")
        
        # íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í¬ë©´ ë¶€ë¶„ ì²˜ë¦¬
        if file_size and file_size > 100 * 1024 * 1024:  # 100MB ì´ìƒ
            return _load_results_partial_processing(s3_client, bucket, key, file_size)
        
        # ì „ì²´ íŒŒì¼ ì½ê¸° (ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í¬í•¨)
        response = s3_client.get_object(Bucket=bucket, Key=key)
        
        # ğŸš¨ [Memory Management] ìŠ¤íŠ¸ë¦¬ë° ì½ê¸°ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        content_chunks = []
        chunk_size = 1024 * 1024  # 1MBì”© ì½ê¸°
        
        while True:
            chunk = response['Body'].read(chunk_size)
            if not chunk:
                break
            content_chunks.append(chunk)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
            total_size = sum(len(c) for c in content_chunks)
            if total_size > 200 * 1024 * 1024:  # 200MB ì œí•œ
                logger.warning(f"Large content detected: {total_size} bytes, switching to partial processing")
                return _load_results_partial_processing(s3_client, bucket, key, total_size)
        
        # ì „ì²´ ë‚´ìš© ì¡°í•©
        full_content = b''.join(content_chunks).decode('utf-8')
        del content_chunks  # ë©”ëª¨ë¦¬ ì •ë¦¬
        
        # JSON íŒŒì‹±
        results = json.loads(full_content)
        del full_content  # ë©”ëª¨ë¦¬ ì •ë¦¬
        
        if not isinstance(results, list):
            logger.warning(f"Expected list, got {type(results)}")
            return []
        
        logger.info(f"Enhanced fallback completed: {len(results)} results")
        return results
        
    except Exception as e:
        logger.error(f"Enhanced fallback failed: {e}")
        return []


def _load_results_partial_processing(s3_client, bucket: str, key: str, file_size: int) -> List[Dict[str, Any]]:
    """
    ğŸš¨ [Partial Processing] ì´ˆëŒ€ìš©ëŸ‰ íŒŒì¼ì„ ìœ„í•œ ë¶€ë¶„ ì²˜ë¦¬
    
    ì „ì²´ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ì„ ë•Œ ì¤‘ìš”í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ
    """
    try:
        logger.warning(f"Using partial processing for large file: {file_size} bytes")
        
        # íŒŒì¼ì˜ ì‹œì‘ê³¼ ë ë¶€ë¶„ë§Œ ì½ì–´ì„œ êµ¬ì¡° íŒŒì•…
        head_size = min(1024 * 1024, file_size // 10)  # 1MB ë˜ëŠ” íŒŒì¼ì˜ 10%
        tail_size = min(1024 * 1024, file_size // 10)
        
        # í—¤ë“œ ë¶€ë¶„ ì½ê¸°
        head_response = s3_client.get_object(
            Bucket=bucket, 
            Key=key,
            Range=f'bytes=0-{head_size-1}'
        )
        head_content = head_response['Body'].read().decode('utf-8')
        
        # í…Œì¼ ë¶€ë¶„ ì½ê¸°
        tail_start = max(0, file_size - tail_size)
        tail_response = s3_client.get_object(
            Bucket=bucket,
            Key=key,
            Range=f'bytes={tail_start}-{file_size-1}'
        )
        tail_content = tail_response['Body'].read().decode('utf-8')
        
        # ğŸ¯ [Strategy] ë¶€ë¶„ ë°ì´í„°ì—ì„œ ì™„ì „í•œ JSON ê°ì²´ ì¶”ì¶œ
        results = []
        
        # í—¤ë“œì—ì„œ ì™„ì „í•œ ê°ì²´ë“¤ ì¶”ì¶œ
        head_objects = _extract_complete_json_objects(head_content, from_start=True)
        results.extend(head_objects)
        
        # í…Œì¼ì—ì„œ ì™„ì „í•œ ê°ì²´ë“¤ ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
        tail_objects = _extract_complete_json_objects(tail_content, from_start=False)
        
        # ì¤‘ë³µ ì œê±° (chunk_id ê¸°ì¤€)
        seen_chunk_ids = {obj.get('chunk_id') for obj in results if isinstance(obj, dict)}
        for obj in tail_objects:
            if isinstance(obj, dict) and obj.get('chunk_id') not in seen_chunk_ids:
                results.append(obj)
        
        logger.warning(f"Partial processing extracted {len(results)} objects from {file_size} byte file")
        return results
        
    except Exception as e:
        logger.error(f"Partial processing failed: {e}")
        return []


def _extract_complete_json_objects(content: str, from_start: bool = True) -> List[Dict[str, Any]]:
    """
    ë¶€ë¶„ JSON ë‚´ìš©ì—ì„œ ì™„ì „í•œ ê°ì²´ë“¤ì„ ì¶”ì¶œ
    """
    try:
        objects = []
        
        # ê°„ë‹¨í•œ JSON ê°ì²´ ê²½ê³„ ì°¾ê¸°
        if from_start:
            # ì‹œì‘ë¶€í„° ì™„ì „í•œ ê°ì²´ë“¤ ì°¾ê¸°
            brace_count = 0
            current_obj = ""
            in_string = False
            escape_next = False
            
            for char in content:
                current_obj += char
                
                if escape_next:
                    escape_next = False
                    continue
                    
                if char == '\\':
                    escape_next = True
                    continue
                    
                if char == '"' and not escape_next:
                    in_string = not in_string
                    continue
                    
                if not in_string:
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        
                        if brace_count == 0 and current_obj.strip():
                            # ì™„ì „í•œ ê°ì²´ ë°œê²¬
                            try:
                                obj = json.loads(current_obj.strip().rstrip(','))
                                if isinstance(obj, dict):
                                    objects.append(obj)
                            except:
                                pass
                            current_obj = ""
        
        return objects
        
    except Exception as e:
        logger.warning(f"JSON object extraction failed: {e}")
        return []


def _load_results_chunked_fallback(s3_client, bucket: str, key: str) -> List[Dict[str, Any]]:
    """
    ijsonì´ ì—†ì„ ë•Œì˜ ëŒ€ì•ˆ: ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸°
    """
    try:
        # Range ìš”ì²­ìœ¼ë¡œ ì²­í¬ ë‹¨ìœ„ ì½ê¸° (1MBì”©)
        chunk_size = 1024 * 1024  # 1MB
        results = []
        offset = 0
        buffer = ""
        
        while True:
            try:
                response = s3_client.get_object(
                    Bucket=bucket, 
                    Key=key,
                    Range=f'bytes={offset}-{offset + chunk_size - 1}'
                )
                chunk_data = response['Body'].read().decode('utf-8')
                buffer += chunk_data
                
                # JSON ê°ì²´ ê²½ê³„ ì°¾ê¸° (ê°„ë‹¨í•œ êµ¬í˜„)
                # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹±ì´ í•„ìš”
                if chunk_data == "":
                    break
                    
                offset += chunk_size
                
            except Exception as range_error:
                # Range ìš”ì²­ ì‹¤íŒ¨ ì‹œ ì „ì²´ íŒŒì¼ ì½ê¸°ë¡œ í´ë°±
                logger.warning(f"Range request failed, using full read: {range_error}")
                response = s3_client.get_object(Bucket=bucket, Key=key)
                buffer = response['Body'].read().decode('utf-8')
                break
        
        # ìµœì¢… JSON íŒŒì‹±
        results = json.loads(buffer)
        return results if isinstance(results, list) else []
        
    except Exception as e:
        logger.error(f"Chunked fallback failed: {e}")
        return []


def _load_latest_state(state_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    ğŸ¯ [Critical] DynamoDB/S3ì—ì„œ ìµœì‹  ìƒíƒœ ë¡œë“œ
    
    Args:
        state_data: ì›Œí¬í”Œë¡œìš° ìƒíƒœ ë°ì´í„°
        
    Returns:
        ìµœì‹  ì›Œí¬í”Œë¡œìš° ìƒíƒœ
    """
    try:
        import boto3
        
        # DynamoDBì—ì„œ ìµœì‹  ìƒíƒœ í¬ì¸í„° ì¡°íšŒ
        dynamodb = boto3.resource('dynamodb')
        state_table_name = DynamoDBConfig.WORKFLOWS_TABLE
        state_table = dynamodb.Table(state_table_name)
        
        execution_id = state_data.get('workflowId', 'unknown')
        
        response = state_table.get_item(
            Key={
                'execution_id': execution_id,
                'state_type': 'LATEST'
            }
        )
        
        if 'Item' in response:
            item = response['Item']
            state_s3_path = item.get('state_s3_path')
            
            if state_s3_path:
                # S3ì—ì„œ ìµœì‹  ìƒíƒœ ë¡œë“œ
                s3_client = boto3.client('s3')
                bucket, key = state_s3_path.replace('s3://', '').split('/', 1)
                
                response = s3_client.get_object(Bucket=bucket, Key=key)
                latest_state = json.loads(response['Body'].read().decode('utf-8'))
                
                logger.info(f"Loaded latest state from src.S3: {state_s3_path}")
                return latest_state
            else:
                # ì¸ë¼ì¸ ìƒíƒœ ë°˜í™˜
                inline_state = item.get('state_data', {})
                logger.info("Loaded latest state from src.DynamoDB (inline)")
                return inline_state
        else:
            logger.warning(f"No latest state found for execution {execution_id}")
            return {}
            
    except Exception as e:
        logger.error(f"Failed to load latest state: {e}")
        return {}


def _build_aggregation_response(
    status: str,
    final_state: Optional[Dict] = None,
    final_state_s3_path: Optional[str] = None,  # [Fix] S3 ê²½ë¡œ íŒŒë¼ë¯¸í„° ì¶”ê°€
    total_segments_processed: int = 0,
    total_chunks: int = 0,
    successful_chunks: int = 0,
    failed_chunks: int = 0,
    paused_chunks: int = 0,  # ğŸ¯ HITP ì§€ì›
    execution_summary: Optional[Dict] = None,
    all_results: Optional[List] = None,
    failed_chunk_details: Optional[List] = None,
    paused_chunk_details: Optional[List] = None,  # ğŸ¯ HITP ì§€ì›
    error: Optional[str] = None,
    message: Optional[str] = None,  # ğŸ¯ ìƒíƒœ ë©”ì‹œì§€
    aggregation_metadata: Optional[Dict] = None
) -> Dict[str, Any]:
    """
    ì§‘ê³„ ê²°ê³¼ë¥¼ í‘œì¤€í™”ëœ í˜•íƒœë¡œ êµ¬ì„±
    """
    response = {
        "status": status,
        "final_state": final_state,
        "final_state_s3_path": final_state_s3_path,  # [Fix] S3 ê²½ë¡œ í¬í•¨
        "total_segments_processed": total_segments_processed,
        "total_chunks": total_chunks,
        "successful_chunks": successful_chunks,
        "failed_chunks": failed_chunks,
        "paused_chunks": paused_chunks,
        "execution_summary": execution_summary or {},
        "all_results": all_results or [],
        "aggregation_metadata": aggregation_metadata or {},
        "new_history_logs": all_results or []  # [Fix] ASL ResultSelector í˜¸í™˜ì„±
    }
    
    if failed_chunk_details:
        response["failed_chunk_details"] = failed_chunk_details
    
    if paused_chunk_details:
        response["paused_chunk_details"] = paused_chunk_details
    
    if error:
        response["error"] = error
        
    if message:
        response["message"] = message
    
    return response


def calculate_total_time(results: List[Dict]) -> float:
    """
    ë¶„ì‚° ì‹¤í–‰ì˜ ì´ ì‹œê°„ ê³„ì‚° (ë³‘ë ¬ ì‹¤í–‰ì´ë¯€ë¡œ ìµœëŒ€ê°’ ì‚¬ìš©)
    """
    execution_times = []
    
    for result in results:
        if isinstance(result, dict):
            exec_time = result.get('execution_time')
            if isinstance(exec_time, (int, float)) and exec_time > 0:
                execution_times.append(exec_time)
    
    return max(execution_times) if execution_times else 0


def _save_final_state(state_data: Dict[str, Any], final_state: Dict[str, Any], execution_summary: Dict[str, Any]) -> Optional[str]:
    """
    ğŸ¯ [Critical Fix] ì§‘ê³„ ì™„ë£Œ í›„ ìµœì¢… ìƒíƒœë¥¼ DynamoDBì— FINALë¡œ ì €ì¥
    
    Args:
        state_data: ì›Œí¬í”Œë¡œìš° ìƒíƒœ ë°ì´í„°
        final_state: ìµœì¢… ì§‘ê³„ëœ ìƒíƒœ
        execution_summary: ì‹¤í–‰ ìš”ì•½ ì •ë³´
        
    Returns:
        S3 ê²½ë¡œ (ëŒ€ìš©ëŸ‰ ìƒíƒœì˜ ê²½ìš°) ë˜ëŠ” None (ì¸ë¼ì¸ ì €ì¥ì˜ ê²½ìš°)
    """
    try:
        import boto3
        
        execution_id = state_data.get('workflowId', 'unknown')
        
        # DynamoDB í…Œì´ë¸” ì„¤ì •
        dynamodb = boto3.resource('dynamodb')
        state_table_name = DynamoDBConfig.WORKFLOWS_TABLE
        state_table = dynamodb.Table(state_table_name)
        
        # S3 ì„¤ì • (ëŒ€ìš©ëŸ‰ ìƒíƒœìš©)
        s3_client = boto3.client('s3')
        state_bucket = os.environ.get('WORKFLOW_STATE_BUCKET', 'workflow-states')
        
        # ìƒíƒœ í¬ê¸° í™•ì¸
        state_json = json.dumps(final_state, ensure_ascii=False)
        state_size = len(state_json.encode('utf-8'))
        
        # ğŸš¨ [Performance] ëŒ€ìš©ëŸ‰ ìƒíƒœëŠ” S3ì— ì €ì¥
        use_s3_storage = state_size > 100000  # 100KB ì´ìƒ
        
        final_record = {
            'execution_id': execution_id,
            'state_type': 'FINAL',
            'created_at': datetime.now(timezone.utc).isoformat(),
            'status': final_state.get('final_status', 'COMPLETED'),
            'total_segments_processed': execution_summary.get('total_segments_processed', 0),
            'total_chunks': execution_summary.get('total_chunks', 0),
            'successful_chunks': execution_summary.get('successful_chunks', 0),
            'failed_chunks': execution_summary.get('failed_chunks', 0),
            'execution_time': execution_summary.get('total_execution_time', 0),
            'aggregation_completed': True,
            'state_size_bytes': state_size,
            'uses_s3_storage': use_s3_storage
        }
        
        if use_s3_storage:
            # ëŒ€ìš©ëŸ‰ ìƒíƒœëŠ” S3ì— ì €ì¥
            s3_key = f"final-states/{execution_id}/final-state.json"
            s3_path = f"s3://{state_bucket}/{s3_key}"
            
            s3_client.put_object(
                Bucket=state_bucket,
                Key=s3_key,
                Body=state_json,
                ContentType='application/json',
                Metadata={
                    'execution_id': execution_id,
                    'state_type': 'FINAL',
                    'aggregation_timestamp': datetime.now(timezone.utc).isoformat()
                }
            )
            
            final_record['state_s3_path'] = s3_path
            final_record['execution_summary'] = execution_summary  # ìš”ì•½ë§Œ DynamoDBì—
            
            logger.info(f"Final state saved to S3: {s3_path} ({state_size} bytes)")
            final_state_s3_path = s3_path  # [Fix] S3 ê²½ë¡œ ì €ì¥
            
        else:
            # ì†Œìš©ëŸ‰ ìƒíƒœëŠ” DynamoDBì— ì¸ë¼ì¸ ì €ì¥
            final_record['state_data'] = final_state
            final_record['execution_summary'] = execution_summary
            final_state_s3_path = None  # [Fix] ì¸ë¼ì¸ ì €ì¥ì˜ ê²½ìš° None
            
            logger.info(f"Final state saved to DynamoDB inline ({state_size} bytes)")
        
        # DynamoDBì— ìµœì¢… ë ˆì½”ë“œ ì €ì¥
        state_table.put_item(Item=final_record)
        
        # ğŸ¯ [Performance] LATEST í¬ì¸í„°ë„ FINALë¡œ ì—…ë°ì´íŠ¸
        latest_record = {
            'execution_id': execution_id,
            'state_type': 'LATEST',
            'created_at': datetime.now(timezone.utc).isoformat(),
            'points_to_final': True,
            'final_status': final_state.get('final_status', 'COMPLETED'),
            'aggregation_completed': True
        }
        
        if use_s3_storage:
            latest_record['state_s3_path'] = final_record['state_s3_path']
        else:
            latest_record['state_data'] = final_state
        
        state_table.put_item(Item=latest_record)
        
        logger.info(f"Final state successfully saved for execution {execution_id}")
        return final_state_s3_path  # [Fix] S3 ê²½ë¡œ ë°˜í™˜
        
    except Exception as e:
        logger.error(f"Failed to save final state: {e}")
        # ì €ì¥ ì‹¤íŒ¨í•´ë„ ì§‘ê³„ ê²°ê³¼ëŠ” ë°˜í™˜ (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ìš°ì„ )
        return None


def _cleanup_intermediate_states(execution_id: str) -> None:
    """
    ğŸ¯ [Critical Fix] ì¤‘ê°„ ìƒíƒœë“¤ì„ ì •ë¦¬í•˜ì—¬ ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ì ˆì•½
    
    DynamoDB ë ˆì½”ë“œì™€ S3 ê°ì²´ë¥¼ ëª¨ë‘ ì •ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        execution_id: ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ID
    """
    try:
        import boto3
        
        # DynamoDB ì •ë¦¬
        _cleanup_dynamodb_states(execution_id)
        
        # ğŸš¨ [Critical Fix] S3 ì¤‘ê°„ ê°ì²´ ì •ë¦¬
        _cleanup_s3_intermediate_objects(execution_id)
        
    except Exception as e:
        logger.warning(f"Failed to cleanup intermediate states: {e}")
        # ì •ë¦¬ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ


def _cleanup_dynamodb_states(execution_id: str) -> None:
    """
    DynamoDBì—ì„œ ì¤‘ê°„ ìƒíƒœ ë ˆì½”ë“œ ì •ë¦¬
    """
    try:
        dynamodb = boto3.resource('dynamodb')
        state_table_name = DynamoDBConfig.WORKFLOWS_TABLE
        state_table = dynamodb.Table(state_table_name)
        
        # ì¤‘ê°„ ìƒíƒœë“¤ ì¡°íšŒ (INTERMEDIATE, CHUNK_* ë“±)
        response = state_table.query(
            KeyConditionExpression='execution_id = :exec_id',
            FilterExpression='begins_with(state_type, :intermediate) OR begins_with(state_type, :chunk)',
            ExpressionAttributeValues={
                ':exec_id': execution_id,
                ':intermediate': 'INTERMEDIATE',
                ':chunk': 'CHUNK_'
            }
        )
        
        # ë°°ì¹˜ ì‚­ì œ (ìµœëŒ€ 25ê°œì”©)
        items_to_delete = response.get('Items', [])
        
        if items_to_delete:
            with state_table.batch_writer() as batch:
                for item in items_to_delete:
                    batch.delete_item(
                        Key={
                            'execution_id': item['execution_id'],
                            'state_type': item['state_type']
                        }
                    )
            
            logger.info(f"Cleaned up {len(items_to_delete)} DynamoDB intermediate states for {execution_id}")
        
    except Exception as e:
        logger.warning(f"Failed to cleanup DynamoDB states: {e}")


def _cleanup_s3_intermediate_objects(execution_id: str) -> None:
    """
    ğŸš¨ [Critical Fix] S3ì—ì„œ ì¤‘ê°„ ìƒíƒœ ê°ì²´ë“¤ ì •ë¦¬
    
    ì •ë¦¬ ëŒ€ìƒ:
    - distributed-states/{owner_id}/{workflow_id}/{execution_id}/chunks/
    - distributed-states/{owner_id}/{workflow_id}/{execution_id}/segments/
    - distributed-chunks/{owner_id}/{workflow_id}/{execution_id}/
    """
    try:
        import boto3
        
        s3_client = boto3.client('s3')
        state_bucket = os.environ.get('WORKFLOW_STATE_BUCKET')
        
        if not state_bucket:
            logger.warning("No WORKFLOW_STATE_BUCKET configured, skipping S3 cleanup")
            return
        
        # ğŸ¯ [Strategy] execution_id ê¸°ë°˜ìœ¼ë¡œ ì¤‘ê°„ ê°ì²´ íŒ¨í„´ ë§¤ì¹­
        cleanup_prefixes = [
            f"distributed-states/",  # ëª¨ë“  ë¶„ì‚° ìƒíƒœ
            f"distributed-chunks/",  # ì²­í¬ ë°ì´í„°
            f"workflow-states/"      # ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° ìƒíƒœ
        ]
        
        total_deleted = 0
        
        for prefix in cleanup_prefixes:
            try:
                # execution_idê°€ í¬í•¨ëœ ê°ì²´ë“¤ ì¡°íšŒ
                paginator = s3_client.get_paginator('list_objects_v2')
                pages = paginator.paginate(
                    Bucket=state_bucket,
                    Prefix=prefix
                )
                
                objects_to_delete = []
                
                for page in pages:
                    if 'Contents' not in page:
                        continue
                    
                    for obj in page['Contents']:
                        key = obj['Key']
                        
                        # execution_idê°€ ê²½ë¡œì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                        if execution_id in key:
                            # ìµœì¢… ìƒíƒœëŠ” ë³´ì¡´ (final-states ì œì™¸)
                            if 'final-states' not in key and 'latest_state.json' not in key:
                                objects_to_delete.append({'Key': key})
                        
                        # ë°°ì¹˜ ì‚­ì œ (ìµœëŒ€ 1000ê°œì”©)
                        if len(objects_to_delete) >= 1000:
                            _batch_delete_s3_objects(s3_client, state_bucket, objects_to_delete)
                            total_deleted += len(objects_to_delete)
                            objects_to_delete = []
                
                # ë‚¨ì€ ê°ì²´ë“¤ ì‚­ì œ
                if objects_to_delete:
                    _batch_delete_s3_objects(s3_client, state_bucket, objects_to_delete)
                    total_deleted += len(objects_to_delete)
                    
            except Exception as prefix_error:
                logger.warning(f"Failed to cleanup prefix {prefix}: {prefix_error}")
        
        if total_deleted > 0:
            logger.info(f"Cleaned up {total_deleted} S3 intermediate objects for {execution_id}")
        else:
            logger.info(f"No S3 intermediate objects found for cleanup: {execution_id}")
            
    except Exception as e:
        logger.warning(f"Failed to cleanup S3 intermediate objects: {e}")


def _batch_delete_s3_objects(s3_client, bucket: str, objects: List[Dict[str, str]]) -> None:
    """
    S3 ê°ì²´ë“¤ì„ ë°°ì¹˜ë¡œ ì‚­ì œ
    """
    try:
        if not objects:
            return
            
        response = s3_client.delete_objects(
            Bucket=bucket,
            Delete={
                'Objects': objects,
                'Quiet': True  # ì„±ê³µí•œ ì‚­ì œëŠ” ì‘ë‹µì—ì„œ ì œì™¸
            }
        )
        
        # ì‚­ì œ ì‹¤íŒ¨í•œ ê°ì²´ë“¤ ë¡œê¹…
        errors = response.get('Errors', [])
        if errors:
            logger.warning(f"Failed to delete {len(errors)} S3 objects: {errors[:5]}")  # ì²˜ìŒ 5ê°œë§Œ ë¡œê¹…
            
    except Exception as e:
        logger.warning(f"Batch delete failed: {e}")


def _setup_s3_lifecycle_policy(bucket_name: str) -> None:
    """
    ğŸ¯ [Optimization] S3 Lifecycle Policy ì„¤ì •ìœ¼ë¡œ ìë™ ì •ë¦¬
    
    ì¤‘ê°„ ìƒíƒœ ê°ì²´ë“¤ì˜ ìë™ ë§Œë£Œ ì„¤ì •
    """
    try:
        import boto3
        
        s3_client = boto3.client('s3')
        
        lifecycle_config = {
            'Rules': [
                {
                    'ID': 'WorkflowIntermediateStatesCleanup',
                    'Status': 'Enabled',
                    'Filter': {
                        'Prefix': 'distributed-states/'
                    },
                    'Expiration': {
                        'Days': 7  # 7ì¼ í›„ ìë™ ì‚­ì œ
                    },
                    'AbortIncompleteMultipartUpload': {
                        'DaysAfterInitiation': 1
                    }
                },
                {
                    'ID': 'WorkflowChunksCleanup',
                    'Status': 'Enabled',
                    'Filter': {
                        'Prefix': 'distributed-chunks/'
                    },
                    'Expiration': {
                        'Days': 3  # ì²­í¬ ë°ì´í„°ëŠ” 3ì¼ í›„ ì‚­ì œ
                    }
                },
                {
                    'ID': 'WorkflowStatesCleanup',
                    'Status': 'Enabled',
                    'Filter': {
                        'Prefix': 'workflow-states/'
                    },
                    'Expiration': {
                        'Days': 14  # ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° ìƒíƒœëŠ” 14ì¼ ë³´ê´€
                    }
                },
                {
                    'ID': 'FinalStatesLongTermRetention',
                    'Status': 'Enabled',
                    'Filter': {
                        'Prefix': 'final-states/'
                    },
                    'Transitions': [
                        {
                            'Days': 30,
                            'StorageClass': 'STANDARD_IA'  # 30ì¼ í›„ IAë¡œ ì´ë™
                        },
                        {
                            'Days': 90,
                            'StorageClass': 'GLACIER'  # 90ì¼ í›„ Glacierë¡œ ì´ë™
                        }
                    ]
                }
            ]
        }
        
        s3_client.put_bucket_lifecycle_configuration(
            Bucket=bucket_name,
            LifecycleConfiguration=lifecycle_config
        )
        
        logger.info(f"S3 Lifecycle policy configured for bucket: {bucket_name}")
        
    except Exception as e:
        logger.warning(f"Failed to setup S3 lifecycle policy: {e}")
        # ì •ì±… ì„¤ì • ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ


def _validate_aggregated_state(aggregated_state: Dict[str, Any]) -> Dict[str, Any]:
    """
    ì§‘ê³„ ê²°ê³¼ì˜ ìœ íš¨ì„±ì„ ê²€ì¦
    """
    validation = {
        'is_valid': True,
        'warnings': [],
        'recommendations': []
    }
    
    # ìƒíƒœ í¬ê¸° ê²€ì¦
    try:
        state_size = len(json.dumps(aggregated_state, ensure_ascii=False).encode('utf-8'))
        if state_size > 200000:  # 200KB
            validation['warnings'].append(f"Large aggregated state: {state_size} bytes")
            validation['recommendations'].append("Consider using S3 storage for large states")
    except Exception as e:
        validation['warnings'].append(f"Failed to calculate state size: {e}")
    
    # ì²­í¬ ê²°ê³¼ ê²€ì¦
    chunks = aggregated_state.get('chunks', {})
    if len(chunks) == 0:
        validation['warnings'].append("No chunk results found in aggregated state")
    
    return validation


# ============================================================
# ğŸš€ HYBRID MODE: MAP_REDUCE / BATCHED ì§‘ê³„ í•¨ìˆ˜
# ============================================================

def _aggregate_map_reduce_results(event: Dict[str, Any]) -> Dict[str, Any]:
    """
    ğŸš€ MAP_REDUCE ëª¨ë“œ ê²°ê³¼ ì§‘ê³„
    
    ê³ ë„ë¡œ ë³‘ë ¬í™”ëœ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ê³  ë³‘í•©í•©ë‹ˆë‹¤.
    
    ğŸ”§ [Last-mile Optimization]
    1. ThreadPoolExecutorë¡œ S3 ê²°ê³¼ ë³‘ë ¬ fetch (N+1 I/O ë¬¸ì œ í•´ê²°)
    2. ê²°ê³¼ ìˆ˜ > HIERARCHICAL_MERGE_THRESHOLDì¼ ë•Œ ê³„ì¸µì  ë³‘í•© ì ìš©
    3. ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ S3ì— ìµœì¢… ê²°ê³¼ ì €ì¥
    """
    map_results = event.get('map_results', [])
    owner_id = event.get('ownerId')
    workflow_id = event.get('workflowId')
    
    start_time = time.time()
    logger.info(f"[MAP_REDUCE] Aggregating {len(map_results)} segment results")
    
    if not map_results:
        return {
            "status": "FAILED",
            "error": "No map results to aggregate",
            "final_state": {}
        }
    
    # ê²°ê³¼ ë¶„ë¥˜
    successful = []
    failed = []
    
    for result in map_results:
        if not isinstance(result, dict):
            continue
        status = result.get('status', 'UNKNOWN')
        if status in ('COMPLETED', 'SUCCESS'):
            successful.append(result)
        else:
            failed.append(result)
    
    logger.info(f"[MAP_REDUCE] {len(successful)} successful, {len(failed)} failed")
    
    # ğŸš€ [Optimization 1] S3 ê²°ê³¼ ë³‘ë ¬ Fetch
    fetched_states = _parallel_fetch_s3_states(successful)
    
    fetch_time = time.time()
    logger.info(f"[MAP_REDUCE] Parallel fetch completed in {fetch_time - start_time:.2f}s")
    
    # ğŸš€ [Optimization 2] ê³„ì¸µì  ë³‘í•© ë˜ëŠ” ì§ì ‘ ë³‘í•© ê²°ì •
    if len(fetched_states) > HIERARCHICAL_MERGE_THRESHOLD:
        logger.info(f"[MAP_REDUCE] Using hierarchical merge for {len(fetched_states)} states")
        merged_state = _hierarchical_merge(fetched_states)
    else:
        merged_state = _sequential_merge(fetched_states)
    
    merge_time = time.time()
    logger.info(f"[MAP_REDUCE] Merge completed in {merge_time - fetch_time:.2f}s")
    
    # ğŸš€ [Optimization 3] ëŒ€ìš©ëŸ‰ ê²°ê³¼ëŠ” ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ S3ì— ì €ì¥
    final_state_s3_path = None
    state_json = json.dumps(merged_state, ensure_ascii=False)
    state_size = len(state_json.encode('utf-8'))
    
    if state_size > 200 * 1024:  # 200KB ì´ìƒ
        final_state_s3_path = _stream_state_to_s3(
            merged_state, owner_id, workflow_id, "map_reduce_final"
        )
        logger.info(f"[MAP_REDUCE] Large state ({state_size} bytes) streamed to S3")
    
    total_time = time.time() - start_time
    final_status = "COMPLETED" if len(failed) == 0 else "PARTIAL_SUCCESS"
    
    return {
        "status": final_status,
        "final_state": merged_state if not final_state_s3_path else {},
        "final_state_s3_path": final_state_s3_path,
        "execution_summary": {
            "mode": "MAP_REDUCE",
            "total_segments": len(map_results),
            "successful": len(successful),
            "failed": len(failed),
            "aggregation_time_seconds": round(total_time, 2),
            "fetch_time_seconds": round(fetch_time - start_time, 2),
            "merge_time_seconds": round(merge_time - fetch_time, 2),
            "used_hierarchical_merge": len(fetched_states) > HIERARCHICAL_MERGE_THRESHOLD,
            "state_size_bytes": state_size,
            "aggregation_timestamp": datetime.now(timezone.utc).isoformat()
        }
    }


def _aggregate_batched_results(event: Dict[str, Any]) -> Dict[str, Any]:
    """
    ğŸš€ BATCHED ëª¨ë“œ ê²°ê³¼ ì§‘ê³„
    
    ë°°ì¹˜ ë‹¨ìœ„ ìˆœì°¨ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤.
    BATCHED ëª¨ë“œëŠ” ìˆœì„œê°€ ì¤‘ìš”í•˜ë¯€ë¡œ ìˆœì°¨ ë³‘í•©ì„ ìœ ì§€í•˜ë˜, 
    ëŒ€ìš©ëŸ‰ì¼ ê²½ìš° ê³„ì¸µì  ë³‘í•©ì„ ì ìš©í•©ë‹ˆë‹¤.
    """
    batch_results = event.get('batch_results', [])
    owner_id = event.get('ownerId')
    workflow_id = event.get('workflowId')
    
    start_time = time.time()
    logger.info(f"[BATCHED] Aggregating {len(batch_results)} batch results")
    
    if not batch_results:
        return {
            "status": "FAILED",
            "error": "No batch results to aggregate",
            "final_state": {}
        }
    
    # ê²°ê³¼ ë¶„ë¥˜
    successful = []
    failed = []
    
    for result in batch_results:
        if not isinstance(result, dict):
            continue
        status = result.get('status', 'UNKNOWN')
        if status in ('COMPLETED', 'SUCCESS'):
            successful.append(result)
        else:
            failed.append(result)
    
    logger.info(f"[BATCHED] {len(successful)} successful, {len(failed)} failed")
    
    # ìˆœì„œëŒ€ë¡œ ì •ë ¬
    sorted_results = sorted(successful, key=lambda x: x.get('segment_id', 0))
    
    # ì¸ë¼ì¸ ìƒíƒœ ì¶”ì¶œ (BATCHEDëŠ” ì£¼ë¡œ ì¸ë¼ì¸ ê²°ê³¼ ì‚¬ìš©)
    states_to_merge = []
    for result in sorted_results:
        segment_state = result.get('final_state', {})
        if segment_state:
            states_to_merge.append(segment_state)
    
    # ëŒ€ìš©ëŸ‰ì¼ ê²½ìš° ê³„ì¸µì  ë³‘í•©, ì•„ë‹ˆë©´ ìˆœì°¨ ë³‘í•©
    if len(states_to_merge) > HIERARCHICAL_MERGE_THRESHOLD:
        logger.info(f"[BATCHED] Using hierarchical merge for {len(states_to_merge)} states")
        merged_state = _hierarchical_merge_ordered(states_to_merge)
    else:
        merged_state = _sequential_merge(states_to_merge)
    
    # ëŒ€ìš©ëŸ‰ ê²°ê³¼ S3 ì €ì¥
    final_state_s3_path = None
    state_json = json.dumps(merged_state, ensure_ascii=False)
    state_size = len(state_json.encode('utf-8'))
    
    if state_size > 200 * 1024:
        final_state_s3_path = _stream_state_to_s3(
            merged_state, owner_id, workflow_id, "batched_final"
        )
    
    total_time = time.time() - start_time
    final_status = "COMPLETED" if len(failed) == 0 else "PARTIAL_SUCCESS"
    
    return {
        "status": final_status,
        "final_state": merged_state if not final_state_s3_path else {},
        "final_state_s3_path": final_state_s3_path,
        "execution_summary": {
            "mode": "BATCHED",
            "total_batches": len(batch_results),
            "successful": len(successful),
            "failed": len(failed),
            "aggregation_time_seconds": round(total_time, 2),
            "state_size_bytes": state_size,
            "aggregation_timestamp": datetime.now(timezone.utc).isoformat()
        }
    }


def _load_state_from_s3(s3_path: str) -> Dict[str, Any]:
    """S3 ê²½ë¡œì—ì„œ ìƒíƒœ ë¡œë“œ"""
    import boto3
    
    if not s3_path.startswith('s3://'):
        return {}
    
    parts = s3_path[5:].split('/', 1)
    bucket = parts[0]
    key = parts[1] if len(parts) > 1 else ''
    
    s3_client = boto3.client('s3')
    response = s3_client.get_object(Bucket=bucket, Key=key)
    content = response['Body'].read().decode('utf-8')
    
    return json.loads(content)


def _deep_merge(base: Dict[str, Any], overlay: Dict[str, Any]) -> Dict[str, Any]:
    """ë‘ ë”•ì…”ë„ˆë¦¬ë¥¼ ê¹Šê²Œ ë³‘í•© (ë¹„ì¬ê·€ì  êµ¬í˜„ìœ¼ë¡œ ìŠ¤íƒ ì˜¤ë²„í”Œë¡œìš° ë°©ì§€)"""
    result = base.copy()
    
    # ğŸš€ [Optimization] ì¬ê·€ ëŒ€ì‹  ìŠ¤íƒ ê¸°ë°˜ ë³‘í•©ìœ¼ë¡œ ê¹Šì€ êµ¬ì¡° ì²˜ë¦¬
    stack = [(result, overlay)]
    
    while stack:
        current_base, current_overlay = stack.pop()
        
        for key, value in current_overlay.items():
            if key in current_base:
                base_val = current_base[key]
                if isinstance(base_val, dict) and isinstance(value, dict):
                    # ë”•ì…”ë„ˆë¦¬: ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬í•  í•­ëª©ì„ ìŠ¤íƒì— ì¶”ê°€
                    stack.append((base_val, value))
                elif isinstance(base_val, list) and isinstance(value, list):
                    # ë¦¬ìŠ¤íŠ¸: ì§ì ‘ í•©ì¹¨
                    current_base[key] = base_val + value
                else:
                    # ê¸°íƒ€: ì˜¤ë²„ë¼ì´ë“œ
                    current_base[key] = value
            else:
                current_base[key] = value
    
    return result


# ============================================================
# ğŸš€ LAST-MILE OPTIMIZATION: ë³‘ë ¬ Fetch & ê³„ì¸µì  ë³‘í•©
# ============================================================

def _parallel_fetch_s3_states(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ğŸš€ [Optimization] ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ S3 ê²°ê³¼ë¥¼ ë³‘ë ¬ë¡œ fetch
    
    N+1 I/O ë¬¸ì œ í•´ê²°: ìˆœì°¨ì  500íšŒ ìš”ì²­ â†’ ë³‘ë ¬ 50ê°œì”© 10ë°°ì¹˜
    """
    fetched_states = []
    s3_paths_to_fetch = []
    inline_states = []
    
    # S3 ê²½ë¡œì™€ ì¸ë¼ì¸ ê²°ê³¼ ë¶„ë¦¬
    for result in results:
        output_s3_path = result.get('output_s3_path')
        if output_s3_path:
            s3_paths_to_fetch.append((result.get('segment_id', 0), output_s3_path))
        else:
            segment_state = result.get('final_state', {})
            if segment_state:
                inline_states.append((result.get('segment_id', 0), segment_state))
    
    logger.info(f"[Parallel Fetch] {len(s3_paths_to_fetch)} S3 paths, {len(inline_states)} inline states")
    
    # ì¸ë¼ì¸ ìƒíƒœ ë¨¼ì € ì¶”ê°€
    fetched_states.extend(inline_states)
    
    if not s3_paths_to_fetch:
        return [state for _, state in sorted(fetched_states, key=lambda x: x[0])]
    
    # ğŸš€ ë³‘ë ¬ S3 fetch
    def fetch_single(item: Tuple[int, str]) -> Tuple[int, Dict[str, Any]]:
        segment_id, s3_path = item
        try:
            state = _load_state_from_s3(s3_path)
            return (segment_id, state)
        except Exception as e:
            logger.warning(f"Failed to fetch {s3_path}: {e}")
            return (segment_id, {})
    
    # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=MAX_PARALLEL_S3_FETCHES) as executor:
        future_to_path = {
            executor.submit(fetch_single, item): item 
            for item in s3_paths_to_fetch
        }
        
        completed = 0
        for future in as_completed(future_to_path):
            try:
                segment_id, state = future.result(timeout=30)  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                if state:
                    fetched_states.append((segment_id, state))
                completed += 1
                
                # ì§„í–‰ ìƒí™© ë¡œê¹… (100ê°œë§ˆë‹¤)
                if completed % 100 == 0:
                    logger.info(f"[Parallel Fetch] Progress: {completed}/{len(s3_paths_to_fetch)}")
                    
            except Exception as e:
                logger.warning(f"Future failed: {e}")
    
    logger.info(f"[Parallel Fetch] Completed: {len(fetched_states)} states fetched")
    
    # segment_id ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒíƒœë§Œ ë°˜í™˜
    return [state for _, state in sorted(fetched_states, key=lambda x: x[0])]


def _sequential_merge(states: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ìˆœì°¨ì  ë³‘í•© (ì†Œê·œëª¨ ë°ì´í„°ìš©)
    """
    if not states:
        return {}
    
    result = {}
    for state in states:
        result = _deep_merge(result, state)
    
    return result


def _hierarchical_merge(states: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ğŸš€ [Optimization] ê³„ì¸µì  ë³‘í•© (Aggregation Tree)
    
    ëŒ€ìš©ëŸ‰ ê²°ê³¼ë¥¼ ìœ„í•œ ë¶„í•  ì •ë³µ ë°©ì‹:
    - 1000ê°œ ê²°ê³¼ â†’ 100ê°œì”© 10ê°œ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ  ë¨¼ì € ë³‘í•©
    - 10ê°œ ì¤‘ê°„ ê²°ê³¼ â†’ ìµœì¢… ë³‘í•©
    
    ì´ ë°©ì‹ì€ ë‹¨ì¼ _deep_merge í˜¸ì¶œì˜ ë©”ëª¨ë¦¬ í”¼í¬ë¥¼ ë¶„ì‚°ì‹œí‚µë‹ˆë‹¤.
    """
    if not states:
        return {}
    
    if len(states) <= MERGE_BATCH_SIZE:
        return _sequential_merge(states)
    
    logger.info(f"[Hierarchical Merge] Processing {len(states)} states in batches of {MERGE_BATCH_SIZE}")
    
    # ğŸ¯ Level 1: ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë³‘í•©
    intermediate_results = []
    
    for i in range(0, len(states), MERGE_BATCH_SIZE):
        batch = states[i:i + MERGE_BATCH_SIZE]
        batch_result = _sequential_merge(batch)
        intermediate_results.append(batch_result)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ íŒíŠ¸
        if i % (MERGE_BATCH_SIZE * 10) == 0 and i > 0:
            logger.info(f"[Hierarchical Merge] Level 1 progress: {i}/{len(states)}")
    
    logger.info(f"[Hierarchical Merge] Level 1 complete: {len(intermediate_results)} intermediate results")
    
    # ğŸ¯ Level 2: ì¤‘ê°„ ê²°ê³¼ê°€ ì—¬ì „íˆ í¬ë©´ ì¬ê·€ (í•˜ì§€ë§Œ ê¹Šì´ ì œí•œ)
    if len(intermediate_results) > MERGE_BATCH_SIZE:
        return _hierarchical_merge(intermediate_results)
    else:
        return _sequential_merge(intermediate_results)


def _hierarchical_merge_ordered(states: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ğŸš€ ìˆœì„œë¥¼ ë³´ì¡´í•˜ëŠ” ê³„ì¸µì  ë³‘í•© (BATCHED ëª¨ë“œìš©)
    
    BATCHED ëª¨ë“œëŠ” ì‹¤í–‰ ìˆœì„œê°€ ì¤‘ìš”í•˜ë¯€ë¡œ,
    ì¸ì ‘í•œ ë°°ì¹˜ë¼ë¦¬ë§Œ ë³‘í•©í•˜ì—¬ ìˆœì„œë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.
    """
    if not states:
        return {}
    
    if len(states) <= MERGE_BATCH_SIZE:
        return _sequential_merge(states)
    
    # ì¸ì ‘í•œ ë°°ì¹˜ë¼ë¦¬ ë³‘í•© (ìˆœì„œ ë³´ì¡´)
    intermediate_results = []
    
    for i in range(0, len(states), MERGE_BATCH_SIZE):
        batch = states[i:i + MERGE_BATCH_SIZE]
        # ìˆœì°¨ì ìœ¼ë¡œ ë³‘í•©í•˜ì—¬ ìˆœì„œ ë³´ì¡´
        batch_result = _sequential_merge(batch)
        intermediate_results.append(batch_result)
    
    # ì¬ê·€ì ìœ¼ë¡œ ì¤‘ê°„ ê²°ê³¼ ë³‘í•©
    if len(intermediate_results) > MERGE_BATCH_SIZE:
        return _hierarchical_merge_ordered(intermediate_results)
    else:
        return _sequential_merge(intermediate_results)


def _stream_state_to_s3(
    state: Dict[str, Any], 
    owner_id: str, 
    workflow_id: str,
    state_type: str
) -> Optional[str]:
    """
    ğŸš€ [Optimization] ëŒ€ìš©ëŸ‰ ìƒíƒœë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ S3ì— ì €ì¥
    
    ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì €ì¥ì„ ìœ„í•´ ì²­í¬ ë‹¨ìœ„ë¡œ ì—…ë¡œë“œ
    """
    try:
        import boto3
        
        bucket = os.environ.get('WORKFLOW_STATE_BUCKET')
        if not bucket:
            logger.warning("No WORKFLOW_STATE_BUCKET configured")
            return None
        
        s3_client = boto3.client('s3')
        timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')
        key = f"aggregated-states/{owner_id}/{workflow_id}/{state_type}_{timestamp}.json"
        
        # JSON ì§ë ¬í™”
        state_json = json.dumps(state, ensure_ascii=False)
        state_bytes = state_json.encode('utf-8')
        state_size = len(state_bytes)
        
        # 5MB ì´ìƒì´ë©´ ë©€í‹°íŒŒíŠ¸ ì—…ë¡œë“œ ì‚¬ìš©
        if state_size > 5 * 1024 * 1024:
            s3_path = _multipart_upload_state(s3_client, bucket, key, state_bytes)
        else:
            s3_client.put_object(
                Bucket=bucket,
                Key=key,
                Body=state_bytes,
                ContentType='application/json',
                Metadata={
                    'owner_id': owner_id or 'unknown',
                    'workflow_id': workflow_id or 'unknown',
                    'state_type': state_type,
                    'size_bytes': str(state_size)
                }
            )
            s3_path = f"s3://{bucket}/{key}"
        
        logger.info(f"[Stream to S3] Uploaded {state_size} bytes to {s3_path}")
        return s3_path
        
    except Exception as e:
        logger.error(f"Failed to stream state to S3: {e}")
        return None


def _multipart_upload_state(
    s3_client, 
    bucket: str, 
    key: str, 
    state_bytes: bytes
) -> str:
    """
    ğŸš€ ë©€í‹°íŒŒíŠ¸ ì—…ë¡œë“œë¡œ ëŒ€ìš©ëŸ‰ ìƒíƒœ ì €ì¥
    """
    from io import BytesIO
    
    # ë©€í‹°íŒŒíŠ¸ ì—…ë¡œë“œ ì‹œì‘
    response = s3_client.create_multipart_upload(
        Bucket=bucket,
        Key=key,
        ContentType='application/json'
    )
    upload_id = response['UploadId']
    
    try:
        parts = []
        part_size = 5 * 1024 * 1024  # 5MB per part
        
        stream = BytesIO(state_bytes)
        part_number = 1
        
        while True:
            chunk = stream.read(part_size)
            if not chunk:
                break
            
            part_response = s3_client.upload_part(
                Bucket=bucket,
                Key=key,
                UploadId=upload_id,
                PartNumber=part_number,
                Body=chunk
            )
            
            parts.append({
                'PartNumber': part_number,
                'ETag': part_response['ETag']
            })
            part_number += 1
        
        # ë©€í‹°íŒŒíŠ¸ ì—…ë¡œë“œ ì™„ë£Œ
        s3_client.complete_multipart_upload(
            Bucket=bucket,
            Key=key,
            UploadId=upload_id,
            MultipartUpload={'Parts': parts}
        )
        
        logger.info(f"[Multipart Upload] Completed with {len(parts)} parts")
        return f"s3://{bucket}/{key}"
        
    except Exception as e:
        # ì‹¤íŒ¨ ì‹œ ë©€í‹°íŒŒíŠ¸ ì—…ë¡œë“œ ì·¨ì†Œ
        s3_client.abort_multipart_upload(
            Bucket=bucket,
            Key=key,
            UploadId=upload_id
        )
        raise e