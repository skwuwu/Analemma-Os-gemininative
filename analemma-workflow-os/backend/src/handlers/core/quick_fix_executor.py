"""
Quick Fix Executor Lambda
==========================

ë™ì  ì—ëŸ¬ ë³µêµ¬ ì•¡ì…˜ì„ ì‹¤í–‰í•˜ëŠ” Lambda í•¸ë“¤ëŸ¬.

Quick Fix ìœ í˜•:
- RETRY: ë™ì¼ ë…¸ë“œ ì¬ì‹œë„ (ì¼ì‹œì  ì˜¤ë¥˜)
- REDIRECT: ëŒ€ì²´ ê²½ë¡œë¡œ ìš°íšŒ (ë¦¬ì†ŒìŠ¤ ê°€ìš©ì„± ë¬¸ì œ)
- SELF_HEALING: LLM ê¸°ë°˜ ìë™ ìˆ˜ì • (êµ¬ë¬¸/ë…¼ë¦¬ ì˜¤ë¥˜)
- INPUT: ì‚¬ìš©ì ì…ë ¥ ìš”ì²­ (í•„ìˆ˜ ì •ë³´ ëˆ„ë½)
- ESCALATE: ê´€ë¦¬ì ì—ìŠ¤ì»¬ë ˆì´ì…˜ (ê¶Œí•œ/ë³´ì•ˆ ë¬¸ì œ)

Author: Analemma Team
"""

import json
import os
import logging
from datetime import datetime, timezone
from typing import Any, Optional
from enum import Enum

import boto3
from botocore.exceptions import ClientError

logger = logging.getLogger()
logger.setLevel(logging.INFO)

# AWS Clients
dynamodb = boto3.resource("dynamodb")
sfn_client = boto3.client("stepfunctions")
sns_client = boto3.client("sns")

# Environment Variables - ğŸš¨ [Critical Fix] ê¸°ë³¸ê°’ì„ template.yamlê³¼ ì¼ì¹˜ì‹œí‚´
EXECUTIONS_TABLE = os.environ.get("EXECUTIONS_TABLE", "ExecutionsTableV3")
# ğŸš¨ [Critical Fix] í™˜ê²½ë³€ìˆ˜ í†µì¼: TASK_TOKENS_TABLE_NAME ìš°ì„  ì‚¬ìš©
TASK_TOKENS_TABLE = os.environ.get("TASK_TOKENS_TABLE_NAME", os.environ.get("TASK_TOKENS_TABLE", "TaskTokensTableV3"))
WORKFLOW_ORCHESTRATOR_ARN = os.environ.get("WORKFLOW_ORCHESTRATOR_ARN", "")
WORKFLOW_DISTRIBUTED_ORCHESTRATOR_ARN = os.environ.get("WORKFLOW_DISTRIBUTED_ORCHESTRATOR_ARN", "")
ALERT_TOPIC_ARN = os.environ.get("ALERT_TOPIC_ARN", "")

# Circuit Breaker ì„¤ì •
MAX_RETRY_ATTEMPTS = int(os.environ.get("MAX_RETRY_ATTEMPTS", "3"))
MAX_SELF_HEALING_ATTEMPTS = int(os.environ.get("MAX_SELF_HEALING_ATTEMPTS", "3"))
CIRCUIT_BREAKER_WINDOW_MINUTES = int(os.environ.get("CIRCUIT_BREAKER_WINDOW_MINUTES", "60"))


class QuickFixType(str, Enum):
    """Quick Fix ìœ í˜•"""
    RETRY = "retry"
    REDIRECT = "redirect"
    SELF_HEALING = "self_healing"
    INPUT = "input"
    ESCALATE = "escalate"


def lambda_handler(event: dict, context: Any) -> dict:
    """
    POST /tasks/quick-fix
    
    Request Body:
    {
        "task_id": "exec-123",
        "fix_type": "retry",
        "payload": { ... },  # fix_typeë³„ ì¶”ê°€ ë°ì´í„°
        "owner_id": "user-abc"
    }
    
    Response:
    {
        "success": true,
        "action_taken": "retry_initiated",
        "new_execution_arn": "arn:aws:states:...",
        "message": "ì¬ì‹œë„ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
    }
    """
    logger.info(f"Quick Fix request: {json.dumps(event)}")
    
    try:
        # Parse request body
        body = _parse_request_body(event)
        
        task_id = body.get("task_id")
        fix_type = body.get("fix_type")
        payload = body.get("payload", {})
        owner_id = _extract_owner_id(event, body)
        
        if not task_id or not fix_type:
            return _error_response(400, "task_idì™€ fix_typeì€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
        
        # Validate fix type
        try:
            fix_type_enum = QuickFixType(fix_type)
        except ValueError:
            return _error_response(400, f"ìœ íš¨í•˜ì§€ ì•Šì€ fix_type: {fix_type}")
        
        # Execute quick fix based on type
        result = _execute_quick_fix(
            task_id=task_id,
            fix_type=fix_type_enum,
            payload=payload,
            owner_id=owner_id
        )
        
        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json"},
            "body": json.dumps(result, ensure_ascii=False)
        }
        
    except PermissionError as e:
        logger.warning(f"Quick Fix permission denied: {str(e)}")
        return _error_response(403, str(e))
        
    except Exception as e:
        logger.exception("Quick Fix execution failed")
        return _error_response(500, f"Quick Fix ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")


def _execute_quick_fix(
    task_id: str,
    fix_type: QuickFixType,
    payload: dict,
    owner_id: str
) -> dict:
    """Quick Fix ìœ í˜•ë³„ ì‹¤í–‰ ë¡œì§"""
    
    # Get execution details
    execution = _get_execution(owner_id, task_id)
    if not execution:
        raise ValueError(f"Execution not found: {task_id}")
    
    # IDOR Prevention: ì†Œìœ ê¶Œ ê²€ì¦
    db_owner_id = execution.get("ownerId")
    if db_owner_id and db_owner_id != owner_id:
        logger.warning(f"IDOR attempt blocked: requested by {owner_id}, owned by {db_owner_id}")
        raise PermissionError("ì´ ì‘ì—…ì— ëŒ€í•œ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # Circuit Breaker ê²€ì¦ (RETRY, SELF_HEALINGì—ë§Œ ì ìš©)
    if fix_type in (QuickFixType.RETRY, QuickFixType.SELF_HEALING):
        _check_circuit_breaker(execution, fix_type)
    
    handlers = {
        QuickFixType.RETRY: _handle_retry,
        QuickFixType.REDIRECT: _handle_redirect,
        QuickFixType.SELF_HEALING: _handle_self_healing,
        QuickFixType.INPUT: _handle_input,
        QuickFixType.ESCALATE: _handle_escalate,
    }
    
    handler = handlers.get(fix_type)
    if not handler:
        raise ValueError(f"No handler for fix_type: {fix_type}")
    
    return handler(execution, payload, owner_id)


def _handle_retry(execution: dict, payload: dict, owner_id: str) -> dict:
    """
    RETRY: ë™ì¼í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ì²˜ìŒë¶€í„° ì¬ì‹œì‘
    
    ì‚¬ìš© ì‚¬ë¡€:
    - LLM API ì¼ì‹œì  ì˜¤ë¥˜
    - ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ
    - ë¦¬ì†ŒìŠ¤ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ì¼ì‹œì  ì‹¤íŒ¨
    """
    workflow_input = execution.get("input", {})
    if isinstance(workflow_input, str):
        try:
            workflow_input = json.loads(workflow_input)
        except json.JSONDecodeError:
            workflow_input = {}
    
    # Retry count ì¶”ì  (Atomic Updateë¡œ ë ˆì´ìŠ¤ ì»¨ë””ì…˜ ë°©ì§€)
    current_retry_count = _increment_retry_count(owner_id, original_arn, "retry_count")
    
    # Determine orchestrator - DBì— ì €ì¥ëœ ARN ìš°ì„  ì‚¬ìš©
    original_arn = execution.get("executionArn", "")
    state_machine_arn = _resolve_state_machine_arn(execution, original_arn)
    
    # Add retry metadata
    workflow_input["_retry_metadata"] = {
        "original_execution_arn": original_arn,
        "retry_timestamp": datetime.now(timezone.utc).isoformat(),
        "retry_reason": payload.get("reason", "user_initiated_retry")
    }
    
    # Start new execution
    new_execution = sfn_client.start_execution(
        stateMachineArn=state_machine_arn,
        input=json.dumps(workflow_input)
    )
    
    # Update original execution with retry info (retry_execution_arnë§Œ ì—…ë°ì´íŠ¸)
    _update_execution_status(
        owner_id=owner_id,
        execution_arn=original_arn,
        updates={
            "retry_execution_arn": new_execution["executionArn"],
            "retried_at": datetime.now(timezone.utc).isoformat()
        }
    )
    
    return {
        "success": True,
        "action_taken": "retry_initiated",
        "new_execution_arn": new_execution["executionArn"],
        "retry_count": current_retry_count,
        "max_retries": MAX_RETRY_ATTEMPTS,
        "message": f"ì›Œí¬í”Œë¡œìš°ê°€ ì¬ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ({current_retry_count}/{MAX_RETRY_ATTEMPTS})"
    }


def _handle_redirect(execution: dict, payload: dict, owner_id: str) -> dict:
    """
    REDIRECT: ëŒ€ì²´ ê²½ë¡œë‚˜ í´ë°± ë…¸ë“œë¡œ ìš°íšŒ
    
    ì‚¬ìš© ì‚¬ë¡€:
    - íŠ¹ì • LLM ëª¨ë¸ ì„œë¹„ìŠ¤ ë¶ˆê°€
    - ì™¸ë¶€ API ì¥ì• 
    - ë¦¬ì „ë³„ ê°€ìš©ì„± ë¬¸ì œ
    """
    redirect_target = payload.get("redirect_target")
    if not redirect_target:
        raise ValueError("redirect_targetì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    original_arn = execution.get("executionArn", "")
    workflow_input = execution.get("input", {})
    if isinstance(workflow_input, str):
        try:
            workflow_input = json.loads(workflow_input)
        except json.JSONDecodeError:
            workflow_input = {}
    
    # Add redirect metadata
    workflow_input["_redirect_metadata"] = {
        "original_execution_arn": original_arn,
        "redirect_target": redirect_target,
        "redirect_timestamp": datetime.now(timezone.utc).isoformat(),
        "redirect_reason": payload.get("reason", "resource_unavailable")
    }
    
    # Apply redirect config
    if redirect_target == "fallback_model":
        workflow_input["model_override"] = payload.get("fallback_model", "claude-3-haiku")
    elif redirect_target == "alternative_provider":
        workflow_input["provider_override"] = payload.get("alternative_provider", "bedrock")
    
    # Determine orchestrator - DBì— ì €ì¥ëœ ARN ìš°ì„  ì‚¬ìš©
    state_machine_arn = _resolve_state_machine_arn(execution, original_arn)
    
    # Start redirected execution
    new_execution = sfn_client.start_execution(
        stateMachineArn=state_machine_arn,
        input=json.dumps(workflow_input)
    )
    
    return {
        "success": True,
        "action_taken": "redirect_initiated",
        "new_execution_arn": new_execution["executionArn"],
        "redirect_target": redirect_target,
        "message": f"ëŒ€ì²´ ê²½ë¡œë¡œ ìš°íšŒë˜ì—ˆìŠµë‹ˆë‹¤: {redirect_target}"
    }


def _handle_self_healing(execution: dict, payload: dict, owner_id: str) -> dict:
    """
    SELF_HEALING: LLM ê¸°ë°˜ ìë™ ìˆ˜ì • í›„ ì¬ì‹œì‘
    
    ì‚¬ìš© ì‚¬ë¡€:
    - í”„ë¡¬í”„íŠ¸ êµ¬ë¬¸ ì˜¤ë¥˜
    - ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜
    - ë…¼ë¦¬ì  ëª¨ìˆœ
    """
    # Self-healingì€ í˜„ì¬ ì›ë³¸ ì…ë ¥ì— ìˆ˜ì • íŒíŠ¸ë¥¼ ì¶”ê°€í•˜ì—¬ ì¬ì‹œì‘
    original_arn = execution.get("executionArn", "")
    workflow_input = execution.get("input", {})
    if isinstance(workflow_input, str):
        try:
            workflow_input = json.loads(workflow_input)
        except json.JSONDecodeError:
            workflow_input = {}
    
    # Self-healing count ì¶”ì  (Atomic Updateë¡œ ë ˆì´ìŠ¤ ì»¨ë””ì…˜ ë°©ì§€)
    current_healing_count = _increment_retry_count(owner_id, original_arn, "self_healing_count")
    
    # ì •í™•í•œ ì—ëŸ¬ ë§¥ë½ í™•ë³´: Step Functions ì‹¤í–‰ íˆìŠ¤í† ë¦¬ì—ì„œ ë§ˆì§€ë§‰ ì‹¤íŒ¨ ì´ë²¤íŠ¸ ì¶”ì¶œ
    error_context = _get_last_failure_context(original_arn)
    suggested_fix = payload.get("suggested_fix", "")
    
    # Add self-healing metadata for the workflow to use
    workflow_input["_self_healing_metadata"] = {
        "original_execution_arn": original_arn,
        "error_context": error_context,
        "suggested_fix": suggested_fix,
        "healing_timestamp": datetime.now(timezone.utc).isoformat(),
        "enable_auto_correction": True,
        "healing_attempt": current_healing_count
    }
    
    # Determine orchestrator - DBì— ì €ì¥ëœ ARN ìš°ì„  ì‚¬ìš©
    state_machine_arn = _resolve_state_machine_arn(execution, original_arn)
    
    # Start self-healing execution
    new_execution = sfn_client.start_execution(
        stateMachineArn=state_machine_arn,
        input=json.dumps(workflow_input)
    )
    
    # Update healing count (healing_execution_arnë§Œ ì—…ë°ì´íŠ¸)
    _update_execution_status(
        owner_id=execution.get("ownerId", "unknown"),
        execution_arn=original_arn,
        updates={
            "healing_execution_arn": new_execution["executionArn"]
        }
    )
    
    return {
        "success": True,
        "action_taken": "self_healing_initiated",
        "new_execution_arn": new_execution["executionArn"],
        "healing_count": current_healing_count,
        "max_healing_attempts": MAX_SELF_HEALING_ATTEMPTS,
        "message": f"ìë™ ìˆ˜ì • í›„ ì¬ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ({current_healing_count}/{MAX_SELF_HEALING_ATTEMPTS})"
    }


def _handle_input(execution: dict, payload: dict, owner_id: str) -> dict:
    """
    INPUT: HITL Task Tokenìœ¼ë¡œ ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸° ìƒíƒœ ìƒì„±
    
    ì‚¬ìš© ì‚¬ë¡€:
    - í•„ìˆ˜ ì •ë³´ ëˆ„ë½
    - ëª¨í˜¸í•œ ì§€ì‹œì‚¬í•­
    - ì‚¬ìš©ì í™•ì¸ í•„ìš”
    """
    # Get the task token if available (for HITL resume)
    task_token = _get_task_token(owner_id, execution.get("executionArn", ""))
    
    required_fields = payload.get("required_fields", [])
    prompt_message = payload.get("prompt_message", "ì¶”ê°€ ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    if task_token:
        # There's an active HITL session - update pending notification
        return {
            "success": True,
            "action_taken": "input_requested",
            "task_token_exists": True,
            "required_fields": required_fields,
            "prompt_message": prompt_message,
            "message": "ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤. HITL íŒ¨ë„ì—ì„œ ì‘ë‹µí•´ì£¼ì„¸ìš”."
        }
    else:
        # No active HITL - this is informational
        return {
            "success": True,
            "action_taken": "input_info_provided",
            "task_token_exists": False,
            "required_fields": required_fields,
            "prompt_message": prompt_message,
            "message": "ì›Œí¬í”Œë¡œìš°ë¥¼ ìˆ˜ì •í•˜ì—¬ í•„ìˆ˜ ì…ë ¥ì„ ì¶”ê°€í•˜ì„¸ìš”."
        }


def _handle_escalate(execution: dict, payload: dict, owner_id: str) -> dict:
    """
    ESCALATE: ê´€ë¦¬ì ì—ìŠ¤ì»¬ë ˆì´ì…˜
    
    ì‚¬ìš© ì‚¬ë¡€:
    - ê¶Œí•œ ë¶€ì¡±
    - ë³´ì•ˆ ì •ì±… ìœ„ë°˜
    - ì‹œìŠ¤í…œ ë ˆë²¨ ì˜¤ë¥˜
    """
    escalation_reason = payload.get("reason", "manual_escalation")
    priority = payload.get("priority", "normal")
    
    # Record escalation in execution metadata
    original_arn = execution.get("executionArn", "")
    _update_execution_status(
        owner_id=owner_id,
        execution_arn=original_arn,
        updates={
            "escalation": {
                "status": "pending",
                "reason": escalation_reason,
                "priority": priority,
                "escalated_at": datetime.now(timezone.utc).isoformat(),
                "escalated_by": owner_id
            }
        }
    )
    
    # ì¦‰ê°ì ì¸ ê´€ë¦¬ì ì•Œë¦¼ (SNS ì—°ë™)
    if ALERT_TOPIC_ARN:
        try:
            sns_client.publish(
                TopicArn=ALERT_TOPIC_ARN,
                Subject=f"ğŸš¨ Analemma Escalation: {execution.get('task_id', 'Unknown')}",
                Message=f"User: {owner_id}\nReason: {escalation_reason}\nPriority: {priority}\nExecution ARN: {original_arn}\nError Context: {payload.get('error_context', 'N/A')}"
            )
            logger.info(f"Escalation alert sent to SNS topic: {ALERT_TOPIC_ARN}")
        except ClientError as e:
            logger.error(f"Failed to send escalation alert: {e}")
    
    # TODO: In production, integrate with ticketing system (Jira, PagerDuty, etc.)
    
    return {
        "success": True,
        "action_taken": "escalation_created",
        "priority": priority,
        "message": f"ê´€ë¦¬ìì—ê²Œ ì—ìŠ¤ì»¬ë ˆì´ì…˜ë˜ì—ˆìŠµë‹ˆë‹¤. (ìš°ì„ ìˆœìœ„: {priority})"
    }


# --- Helper Functions ---

def _parse_request_body(event: dict) -> dict:
    """Parse request body from API Gateway event"""
    body = event.get("body", "{}")
    if isinstance(body, str):
        try:
            return json.loads(body)
        except json.JSONDecodeError:
            return {}
    return body if isinstance(body, dict) else {}


def _extract_owner_id(event: dict, body: dict) -> str:
    """Extract owner_id from JWT claims or request body"""
    from src.common.auth_utils import extract_owner_id_from_event
    
    # Try JWT claims first
    owner_id = extract_owner_id_from_event(event)
    if owner_id:
        return owner_id
    
    # Fallback to request body
    return body.get("owner_id", "anonymous")


def _get_execution(owner_id: str, execution_arn: str) -> Optional[dict]:
    """Get execution details from DynamoDB"""
    table = dynamodb.Table(EXECUTIONS_TABLE)
    
    try:
        response = table.get_item(
            Key={
                "ownerId": owner_id,
                "executionArn": execution_arn
            }
        )
        return response.get("Item")
    except ClientError as e:
        logger.error(f"Failed to get execution: {e}")
        return None


def _get_task_token(owner_id: str, execution_arn: str) -> Optional[str]:
    """Get active task token for HITL resume"""
    table = dynamodb.Table(TASK_TOKENS_TABLE)
    
    # ğŸš¨ [Critical Fix] GSI ì´ë¦„ì„ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´ (template.yamlê³¼ ì¼ì¹˜)
    execution_id_index = os.environ.get('EXECUTION_ID_INDEX', 'ExecutionIdIndex')
    
    try:
        response = table.query(
            IndexName=execution_id_index,
            KeyConditionExpression="ownerId = :oid AND execution_id = :eid",
            ExpressionAttributeValues={
                ":oid": owner_id,
                ":eid": execution_arn
            },
            Limit=1
        )
        items = response.get("Items", [])
        if items:
            return items[0].get("task_token")
        return None
    except ClientError as e:
        logger.error(f"Failed to get task token: {e}")
        return None


def _update_execution_status(owner_id: str, execution_arn: str, updates: dict) -> None:
    """Update execution record in DynamoDB"""
    table = dynamodb.Table(EXECUTIONS_TABLE)
    
    update_expr_parts = []
    expr_attr_values = {}
    expr_attr_names = {}
    
    for idx, (key, value) in enumerate(updates.items()):
        placeholder = f":val{idx}"
        name_placeholder = f"#attr{idx}"
        update_expr_parts.append(f"{name_placeholder} = {placeholder}")
        expr_attr_values[placeholder] = value
        expr_attr_names[name_placeholder] = key
    
    if not update_expr_parts:
        return
    
    try:
        table.update_item(
            Key={
                "ownerId": owner_id,
                "executionArn": execution_arn
            },
            UpdateExpression="SET " + ", ".join(update_expr_parts),
            ExpressionAttributeValues=expr_attr_values,
            ExpressionAttributeNames=expr_attr_names
        )
    except ClientError as e:
        logger.error(f"Failed to update execution: {e}")


def _error_response(status_code: int, message: str) -> dict:
    """Generate error response"""
    return {
        "statusCode": status_code,
        "headers": {"Content-Type": "application/json"},
        "body": json.dumps({
            "success": False,
            "error": message
        }, ensure_ascii=False)
    }


def _check_circuit_breaker(execution: dict, fix_type: QuickFixType) -> None:
    """
    Circuit Breaker: íƒœìŠ¤í¬ë³„ ëˆ„ì  ì¬ì‹œë„/ìê°€ ì¹˜ìœ  íšŸìˆ˜ë¥¼ ì œí•œí•˜ì—¬ API ë¹„ìš© í­ì¦ ë°©ì§€.
    
    MAX_RETRY_ATTEMPTS ë˜ëŠ” MAX_SELF_HEALING_ATTEMPTSë¥¼ ì´ˆê³¼í•˜ë©´
    PermissionErrorë¥¼ ë°œìƒì‹œì¼œ ESCALATEë¡œ ê°•ì œ ì „í™˜ì„ ìœ ë„í•©ë‹ˆë‹¤.
    
    ì •ì±…: íƒœìŠ¤í¬ë³„ ëˆ„ì  í•œë„ (ì‹œê°„ ìœˆë„ìš° ì œê±°ë¡œ ì½”ë“œ ê°„ì†Œí™”)
    """
    if fix_type == QuickFixType.RETRY:
        retry_count = execution.get("retry_count", 0)
        
        if retry_count >= MAX_RETRY_ATTEMPTS:
            logger.warning(
                f"Circuit breaker triggered: retry_count={retry_count}, "
                f"max={MAX_RETRY_ATTEMPTS}"
            )
            raise PermissionError(
                f"ì¬ì‹œë„ íšŸìˆ˜ê°€ í•œë„({MAX_RETRY_ATTEMPTS}íšŒ)ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. "
                f"ê´€ë¦¬ìì—ê²Œ ì—ìŠ¤ì»¬ë ˆì´ì…˜í•˜ì„¸ìš”."
            )
    
    elif fix_type == QuickFixType.SELF_HEALING:
        healing_count = execution.get("self_healing_count", 0)
        
        if healing_count >= MAX_SELF_HEALING_ATTEMPTS:
            logger.warning(
                f"Circuit breaker triggered: healing_count={healing_count}, "
                f"max={MAX_SELF_HEALING_ATTEMPTS}"
            )
            raise PermissionError(
                f"ìê°€ ì¹˜ìœ  íšŸìˆ˜ê°€ í•œë„({MAX_SELF_HEALING_ATTEMPTS}íšŒ)ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. "
                f"ê´€ë¦¬ìì—ê²Œ ì—ìŠ¤ì»¬ë ˆì´ì…˜í•˜ì„¸ìš”."
            )


def _increment_retry_count(owner_id: str, execution_arn: str, field_name: str) -> int:
    """
    DynamoDB ë‚´ë¶€ì—ì„œ ì¹´ìš´íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ 1 ì¦ê°€ì‹œí‚¤ê³ , ìƒˆë¡œìš´ ì¹´ìš´íŠ¸ ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë ˆì´ìŠ¤ ì»¨ë””ì…˜ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ Atomic Updateë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    table = dynamodb.Table(EXECUTIONS_TABLE)
    
    try:
        response = table.update_item(
            Key={
                "ownerId": owner_id,
                "executionArn": execution_arn
            },
            UpdateExpression=f"SET {field_name} = if_not_exists({field_name}, :zero) + :one, last_{field_name}_at = :now",
            ExpressionAttributeValues={
                ":zero": 0,
                ":one": 1,
                ":now": datetime.now(timezone.utc).isoformat()
            },
            ReturnValues="UPDATED_NEW"
        )
        return int(response["Attributes"][field_name])
    except ClientError as e:
        logger.error(f"Failed to increment {field_name}: {e}")
        raise


def _get_last_failure_context(execution_arn: str) -> dict:
    """
    Step Functions ì‹¤í–‰ íˆìŠ¤í† ë¦¬ì—ì„œ ë§ˆì§€ë§‰ ì‹¤íŒ¨ ì´ë²¤íŠ¸ë¥¼ ì°¾ì•„ ì—ëŸ¬ ë§¥ë½ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    Self-Healingì˜ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.
    """
    try:
        response = sfn_client.get_execution_history(
            executionArn=execution_arn,
            maxResults=20,  # ìµœê·¼ 20ê°œ ì´ë²¤íŠ¸ë§Œ í™•ì¸
            reverseOrder=True  # ìµœì‹  ì´ë²¤íŠ¸ë¶€í„°
        )
        
        events = response.get("events", [])
        
        # ì‹¤íŒ¨í•œ ì´ë²¤íŠ¸ ì°¾ê¸° (ExecutionFailed, TaskFailed ë“±)
        for event in events:
            state_entered_event_id = event.get("stateEnteredEventId")
            if state_entered_event_id:
                # TaskFailed ì´ë²¤íŠ¸ ì°¾ê¸°
                task_failed_events = [e for e in events if e.get("taskFailedEventId") == state_entered_event_id]
                if task_failed_events:
                    failed_event = task_failed_events[0]
                    return {
                        "error_type": "TaskFailed",
                        "error_name": failed_event.get("error"),
                        "error_message": failed_event.get("errorCause", ""),
                        "state_name": event.get("stateEnteredEventDetails", {}).get("name", ""),
                        "timestamp": event.get("timestamp", "").isoformat() if event.get("timestamp") else ""
                    }
            
            # ExecutionFailed ì´ë²¤íŠ¸ ì§ì ‘ í™•ì¸
            if event.get("type") == "ExecutionFailed":
                return {
                    "error_type": "ExecutionFailed",
                    "error_name": event.get("executionFailedEventDetails", {}).get("error"),
                    "error_message": event.get("executionFailedEventDetails", {}).get("cause", ""),
                    "timestamp": event.get("timestamp", "").isoformat() if event.get("timestamp") else ""
                }

            # ğŸ¯ [Fix] TaskFailed ì´ë²¤íŠ¸ ì§ì ‘ í™•ì¸
            if event.get("type") == "TaskFailed":
                details = event.get("taskFailedEventDetails", {})
                return {
                    "error_type": "TaskFailed",
                    "error_name": details.get("error"),
                    "error_message": details.get("cause", ""),
                    "timestamp": event.get("timestamp", "").isoformat() if event.get("timestamp") else ""
                }
        
        # ì‹¤íŒ¨ ì´ë²¤íŠ¸ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë§¥ë½ ë°˜í™˜
        return {
            "error_type": "Unknown",
            "error_name": "NoFailureFound",
            "error_message": "ì‹¤í–‰ íˆìŠ¤í† ë¦¬ì—ì„œ ì‹¤íŒ¨ ì´ë²¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except ClientError as e:
        logger.error(f"Failed to get execution history: {e}")
        return {
            "error_type": "HistoryRetrievalFailed",
            "error_name": str(e),
            "error_message": "ì‹¤í–‰ íˆìŠ¤í† ë¦¬ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "timestamp": datetime.now(timezone.utc).isoformat()
        }


def _resolve_state_machine_arn(execution: dict, original_arn: str) -> str:
    """
    State Machine ARN í•´ê²° ë¡œì§.
    
    DBì— ëª…ì‹œì ìœ¼ë¡œ ì €ì¥ëœ state_machine_arnì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©í•˜ê³ ,
    ì—†ìœ¼ë©´ ê¸°ì¡´ ë¬¸ìì—´ ê¸°ë°˜ íŒë³„ ë¡œì§ì„ í´ë°±ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    # 1. DBì— ì €ì¥ëœ ARN ìš°ì„  ì‚¬ìš©
    stored_arn = execution.get("state_machine_arn")
    if stored_arn:
        logger.info(f"Using stored state_machine_arn: {stored_arn}")
        return stored_arn
    
    # 2. ì›Œí¬í”Œë¡œìš° ì •ì˜ì—ì„œ ì§€ì •ëœ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í™•ì¸
    workflow_config = execution.get("workflow_config", {})
    if isinstance(workflow_config, str):
        try:
            workflow_config = json.loads(workflow_config)
        except json.JSONDecodeError:
            workflow_config = {}
    
    orchestrator_type = workflow_config.get("orchestrator_type")
    if orchestrator_type == "distributed":
        return WORKFLOW_DISTRIBUTED_ORCHESTRATOR_ARN
    elif orchestrator_type == "standard":
        return WORKFLOW_ORCHESTRATOR_ARN
    
    # 3. í´ë°±: ê¸°ì¡´ ë¬¸ìì—´ ê¸°ë°˜ íŒë³„ (ë ˆê±°ì‹œ í˜¸í™˜ì„±)
    if "Distributed" in original_arn:
        logger.info("Fallback: detected Distributed orchestrator from ARN string")
        return WORKFLOW_DISTRIBUTED_ORCHESTRATOR_ARN
    
    return WORKFLOW_ORCHESTRATOR_ARN
