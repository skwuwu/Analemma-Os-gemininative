import json
import logging
import os
import time
import uuid
import boto3
from typing import Any, Dict, List, Union, Optional
from boto3.dynamodb.conditions import Key
from botocore.exceptions import ClientError
from decimal import Decimal

# ê³µí†µ ëª¨ë“ˆì—ì„œ AWS í´ë¼ì´ì–¸íŠ¸ ë° ìœ í‹¸ë¦¬í‹° ê°€ì ¸ì˜¤ê¸°
try:
    from src.common.aws_clients import get_dynamodb_resource, get_s3_client
    from src.common.json_utils import DecimalEncoder, convert_to_dynamodb_format
    from src.common.constants import DynamoDBConfig
    dynamodb_resource = get_dynamodb_resource()
    s3_client = get_s3_client()
    _USE_COMMON_UTILS = True
except ImportError:
    dynamodb_resource = boto3.resource('dynamodb')
    s3_client = boto3.client('s3')
    _USE_COMMON_UTILS = False

# CloudWatch í´ë¼ì´ì–¸íŠ¸
cloudwatch_client = boto3.client('cloudwatch')

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# í™˜ê²½ ë³€ìˆ˜
WEBSOCKET_CONNECTIONS_TABLE = os.environ.get('WEBSOCKET_CONNECTIONS_TABLE')
WEBSOCKET_ENDPOINT_URL = os.environ.get('WEBSOCKET_ENDPOINT_URL')
WEBSOCKET_OWNER_ID_GSI = DynamoDBConfig.WEBSOCKET_OWNER_ID_GSI
EXECUTIONS_TABLE = os.environ.get('EXECUTIONS_TABLE')

# DB ì—…ë°ì´íŠ¸ ì „ëµ ê´€ë ¨ í™˜ê²½ ë³€ìˆ˜
DB_UPDATE_STRATEGY = os.environ.get('DB_UPDATE_STRATEGY', 'SELECTIVE')  # ALL, SELECTIVE, MINIMAL
DB_UPDATE_INTERVAL = int(os.environ.get('DB_UPDATE_INTERVAL_SECONDS', '30'))  # ìµœì†Œ ì—…ë°ì´íŠ¸ ê°„ê²©

# í…Œì´ë¸” ë¦¬ì†ŒìŠ¤
connections_table = dynamodb_resource.Table(WEBSOCKET_CONNECTIONS_TABLE) if WEBSOCKET_CONNECTIONS_TABLE else None
executions_table = dynamodb_resource.Table(EXECUTIONS_TABLE) if EXECUTIONS_TABLE else None
SKELETON_S3_BUCKET = os.environ.get('SKELETON_S3_BUCKET')

# API Gateway í´ë¼ì´ì–¸íŠ¸ ìºì‹± (ë™ì  URL ëŒ€ì‘)
apigw_clients = {}

def get_apigw_client(endpoint_url: str) -> Any:
    """
    Endpoint URLë³„ë¡œ í´ë¼ì´ì–¸íŠ¸ë¥¼ ìºì‹±í•˜ì—¬ TCP ì—°ê²° ì¬ì‚¬ìš©ì„ ìœ ë„í•©ë‹ˆë‹¤.
    """
    if endpoint_url not in apigw_clients:
        apigw_clients[endpoint_url] = boto3.client(
            'apigatewaymanagementapi', 
            endpoint_url=endpoint_url.rstrip('/')
        )
    return apigw_clients[endpoint_url]

def get_user_connection_ids(owner_id: str) -> List[str]:
    """
    DynamoDB GSIë¥¼ ì¿¼ë¦¬í•˜ì—¬ ownerIdì— ì—°ê²°ëœ ëª¨ë“  connectionIdë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì—¬ í˜¸ì¶œë¶€ ë¡œì§ì„ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤.
    """
    if not connections_table or not owner_id:
        return []
    
    try:
        response = connections_table.query(
            IndexName=WEBSOCKET_OWNER_ID_GSI,
            KeyConditionExpression=Key('ownerId').eq(owner_id)
        )
        return [item['connectionId'] for item in response.get('Items', []) if 'connectionId' in item]
    except Exception as e:
        logger.error(f'Failed to query connections for owner={owner_id}: {e}')
        return []

def _delete_connection(connection_id: str) -> None:
    """Stale connection ì‚­ì œ"""
    if not connections_table:
        return

    try:
        connections_table.delete_item(Key={'connectionId': connection_id})
        logger.info(f'Removed stale connection {connection_id}')
    except Exception as exc:
        logger.warning(f'Failed to delete stale connection {connection_id}: {exc}')


def _mark_no_active_session(owner_id: str, execution_id: str) -> None:
    """
    [v2.1] ì‚¬ìš©ìì˜ ëª¨ë“  WebSocket ì—°ê²°ì´ ëŠê²¼ì„ ë•Œ DBì— í”Œë˜ê·¸ ì €ì¥.
    
    ì´ í”Œë˜ê·¸ëŠ” ì‚¬ìš©ìê°€ ì¬ì ‘ì†í–ˆì„ ë•Œ ë‹¤ìŒ ìš©ë„ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤:
    1. ì¦‰ì‹œ ë§ˆì§€ë§‰ ì‹¤í–‰ ìƒíƒœë¥¼ fetchí•˜ì—¬ UI ë™ê¸°í™”
    2. ëˆ„ë½ëœ ì•Œë¦¼ ì¬ì „ì†¡ (optional)
    3. ì—°ê²° ëŠê¹€ ì´ë²¤íŠ¸ ë¡œê¹… ë° ë¶„ì„
    
    Args:
        owner_id: ì‚¬ìš©ì ID
        execution_id: ì‹¤í–‰ ID (ì„ íƒì )
    """
    if not executions_table or not owner_id:
        return
    
    try:
        timestamp = int(time.time())
        
        # ì‹¤í–‰ ì¤‘ì¸ executionì´ ìˆìœ¼ë©´ í•´ë‹¹ ë ˆì½”ë“œì— í”Œë˜ê·¸ ì €ì¥
        if execution_id:
            executions_table.update_item(
                Key={
                    'ownerId': owner_id,
                    'executionArn': execution_id
                },
                UpdateExpression='SET #nas = :nas, #nast = :nast',
                ExpressionAttributeNames={
                    '#nas': 'no_active_session',
                    '#nast': 'no_active_session_timestamp'
                },
                ExpressionAttributeValues={
                    ':nas': True,
                    ':nast': timestamp
                }
            )
            logger.info(
                f"Marked no_active_session for owner={owner_id[:8]}..., "
                f"execution={execution_id[:16]}... at {timestamp}"
            )
        
        # CloudWatch ë©”íŠ¸ë¦­ ë°œí–‰ (ëª¨ë‹ˆí„°ë§ìš©)
        try:
            cloudwatch_client.put_metric_data(
                Namespace='WorkflowOrchestrator/WebSocket',
                MetricData=[{
                    'MetricName': 'AllConnectionsLost',
                    'Value': 1,
                    'Unit': 'Count',
                    'Dimensions': [
                        {'Name': 'OwnerId', 'Value': owner_id[:8]}
                    ]
                }]
            )
        except Exception as metric_err:
            logger.debug(f"Failed to publish connection loss metric: {metric_err}")
            
    except Exception as e:
        logger.warning(f"Failed to mark no_active_session: {e}")

def _extract_graph_config(workflow_config: dict) -> dict:
    """
    Extract only nodes and edges from src.workflow_config for graph visualization.
    This minimizes WebSocket payload size while providing graph structure.
    """
    if not workflow_config or not isinstance(workflow_config, dict):
        return None
    
    nodes = workflow_config.get('nodes')
    edges = workflow_config.get('edges')
    
    if not nodes:
        return None
    
    # Extract minimal node data for graph rendering
    minimal_nodes = []
    for node in nodes:
        if isinstance(node, dict):
            minimal_nodes.append({
                'id': node.get('id'),
                'type': node.get('type'),
                'position': node.get('position'),
                'config': {
                    'label': node.get('config', {}).get('label') or node.get('id')
                } if isinstance(node.get('config'), dict) else {'label': node.get('id')}
            })
    
    return {
        'nodes': minimal_nodes,
        'edges': edges or []
    }

def _get_payload(event: Any) -> Dict[str, Any]:
    """
    ì´ë²¤íŠ¸ í˜ì´ë¡œë“œ ì¶”ì¶œ ë° JSON íŒŒì‹±.
    
    Supports multiple event sources:
    1. Direct Lambda invocation: event contains payload directly
    2. EventBridge event: event['detail'] contains the payload (may be string or dict)
    3. Step Functions: event['Payload'] contains the payload
    """
    payload = event
    if isinstance(event, dict):
        # EventBridge events have 'detail' field
        if 'detail' in event:
            detail = event['detail']
            # EventBridge 'detail' can be a string (JSON serialized) or dict
            if isinstance(detail, str):
                try:
                    payload = json.loads(detail)
                except (json.JSONDecodeError, ValueError):
                    payload = {'raw_detail': detail}
            else:
                payload = detail
            # Log EventBridge source for debugging
            source = event.get('source', 'unknown')
            detail_type = event.get('detail-type', 'unknown')
            logger.info(f"Processing EventBridge event: source={source}, detail-type={detail_type}")
        elif 'Payload' in event:
            payload = event['Payload']
    
    if isinstance(payload, str):
        try:
            payload = json.loads(payload)
        except (json.JSONDecodeError, ValueError):
            pass  # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë¬¸ìì—´ ìœ ì§€ í˜¹ì€ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬
            
    if isinstance(payload, str): # ì´ì¤‘ ì¸ì½”ë”© ëœ ê²½ìš° í•œë²ˆ ë” ì‹œë„
        try:
            payload = json.loads(payload)
        except (json.JSONDecodeError, ValueError):
            payload = {}
            
    return payload if isinstance(payload, dict) else {}

# Fallback DecimalEncoder and _convert_to_dynamodb_format if common module not available
if not _USE_COMMON_UTILS:
    class DecimalEncoder(json.JSONEncoder):
        """DynamoDB Decimal íƒ€ì…ì„ JSON í˜¸í™˜ë˜ë„ë¡ ë³€í™˜"""
        def default(self, obj):
            if isinstance(obj, Decimal):
                return float(obj) if obj % 1 else int(obj)
            return super(DecimalEncoder, self).default(obj)

    def _convert_to_dynamodb_format(obj: Any) -> Any:
        """
        ê°ì²´ì˜ ëª¨ë“  ìˆ«ì ê°’ì„ DynamoDB Decimal íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        """
        if isinstance(obj, dict):
            return {k: _convert_to_dynamodb_format(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [_convert_to_dynamodb_format(item) for item in obj]
        elif isinstance(obj, float):
            return Decimal(str(obj))
        elif isinstance(obj, int):
            return obj  # intëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        else:
            return obj
else:
    # Use common utilities
    _convert_to_dynamodb_format = convert_to_dynamodb_format

def should_update_database(payload: dict, state_data: dict) -> bool:
    """
    ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” ë¡œì§
    """
    current_status = payload.get('status', '').upper()
    action = payload.get('notification_type', payload.get('action', ''))
    current_segment = payload.get('segment_to_run', state_data.get('segment_to_run', 0))
    # ğŸ›¡ï¸ [P0 Fix] None ê°’ ë°©ì–´ - payloadë‚˜ state_dataì— í‚¤ê°€ ìˆì§€ë§Œ ê°’ì´ Noneì¸ ê²½ìš° ì²˜ë¦¬
    raw_total = payload.get('total_segments')
    if raw_total is None:
        raw_total = state_data.get('total_segments')
    total_segments = max(1, int(raw_total)) if raw_total is not None and isinstance(raw_total, (int, float)) else 1
    
    # ì „ëµë³„ ì—…ë°ì´íŠ¸ ê²°ì •
    if DB_UPDATE_STRATEGY == 'ALL':
        return True
    
    elif DB_UPDATE_STRATEGY == 'MINIMAL':
        # ì¤‘ìš” ìƒíƒœ ë³€í™”ë§Œ DB ì—…ë°ì´íŠ¸
        critical_statuses = ['STARTED', 'PAUSED_FOR_HITP', 'COMPLETED', 'SUCCEEDED', 'FAILED', 'ABORTED']
        critical_actions = ['workflow_started', 'hitp_pause', 'workflow_completed', 'workflow_failed']
        
        return (current_status in critical_statuses or 
                action in critical_actions or
                current_segment == 0 or  # ì‹œì‘
                current_segment >= total_segments - 1)  # ì™„ë£Œ ì§ì „/ì™„ë£Œ
    
    else:  # SELECTIVE (ê¸°ë³¸ê°’)
        # ìŠ¤ë§ˆíŠ¸ ì—…ë°ì´íŠ¸: ì¤‘ìš” ìƒíƒœ + ì£¼ê¸°ì  ì§„í–‰ ìƒí™©
        critical_statuses = ['STARTED', 'PAUSED_FOR_HITP', 'COMPLETED', 'SUCCEEDED', 'FAILED', 'ABORTED']
        
        if current_status in critical_statuses:
            return True
        
        # ë§ˆì§€ë§‰ DB ì—…ë°ì´íŠ¸ë¡œë¶€í„° ì¼ì • ì‹œê°„ ê²½ê³¼ ì‹œ ì—…ë°ì´íŠ¸
        last_db_update = state_data.get('last_db_update_time', 0)
        current_time = int(time.time())
        
        if current_time - last_db_update >= DB_UPDATE_INTERVAL:
            return True
        
        # ì§„í–‰ë¥  ê¸°ë°˜ ì—…ë°ì´íŠ¸ (10% ë‹¨ìœ„)
        if total_segments > 10:
            progress_percentage = (current_segment / total_segments) * 100
            last_progress = state_data.get('last_db_progress_percentage', 0)
            
            if progress_percentage - last_progress >= 10:  # 10% ì§„í–‰ ì‹œë§ˆë‹¤
                return True
        
        return False

def publish_db_update_metrics(execution_id: str, updated: bool, strategy: str):
    """
    DB ì—…ë°ì´íŠ¸ ê´€ë ¨ ë©”íŠ¸ë¦­ ë°œí–‰
    """
    try:
        metrics = [
            {
                'MetricName': 'DatabaseUpdates',
                'Value': 1 if updated else 0,
                'Unit': 'Count',
                'Dimensions': [
                    {'Name': 'Strategy', 'Value': strategy},
                    {'Name': 'Updated', 'Value': str(updated)}
                ]
            },
            {
                'MetricName': 'DatabaseUpdateRate',
                'Value': 1,
                'Unit': 'Count',
                'Dimensions': [
                    {'Name': 'Strategy', 'Value': strategy}
                ]
            }
        ]
        
        cloudwatch_client.put_metric_data(
            Namespace='WorkflowOrchestrator/Database',
            MetricData=metrics
        )
    except Exception as e:
        logger.warning(f"Failed to publish DB metrics: {e}")

def _safe_json_compatible(value: Any) -> Any:
    """
    ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    [v2.1 ì„±ëŠ¥ ìµœì í™”]
    ê¸°ì¡´: json.dumps â†’ json.loads ì™•ë³µ (ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ CPU/ë©”ëª¨ë¦¬ ê³¼ë¶€í•˜)
    ê°œì„ : ì¬ê·€ì  íƒ€ì… ë³€í™˜ìœ¼ë¡œ ë¬¸ìì—´ ë³€í™˜ ì—†ì´ ì§ì ‘ ì²˜ë¦¬
    
    ë²¤ì¹˜ë§ˆí¬ (500KB payload ê¸°ì¤€):
    - ê¸°ì¡´: ~120ms, ë©”ëª¨ë¦¬ +50MB
    - ê°œì„ : ~15ms, ë©”ëª¨ë¦¬ +5MB
    """
    return _convert_value_recursive(value)


def _convert_value_recursive(value: Any) -> Any:
    """
    ì¬ê·€ì  íƒ€ì… ë³€í™˜ (json.dumps/loads ì—†ì´ ì§ì ‘ ë³€í™˜).
    
    ì§€ì› íƒ€ì…:
    - Decimal â†’ float/int
    - datetime â†’ ISO ë¬¸ìì—´
    - set â†’ list
    - bytes â†’ base64 ë¬¸ìì—´
    - ê¸°íƒ€ ì§ë ¬í™” ë¶ˆê°€ â†’ str()
    """
    if value is None:
        return None
    
    # ê¸°ë³¸ ì§ë ¬í™” ê°€ëŠ¥ íƒ€ì…
    if isinstance(value, (str, int, bool)):
        return value
    
    # float: NaN, Inf ì²˜ë¦¬
    if isinstance(value, float):
        if value != value:  # NaN check
            return None
        if value == float('inf') or value == float('-inf'):
            return None
        return value
    
    # Decimal â†’ float/int (DynamoDB í˜¸í™˜)
    if isinstance(value, Decimal):
        return float(value) if value % 1 else int(value)
    
    # dict: ì¬ê·€ ì²˜ë¦¬
    if isinstance(value, dict):
        return {k: _convert_value_recursive(v) for k, v in value.items()}
    
    # list/tuple: ì¬ê·€ ì²˜ë¦¬
    if isinstance(value, (list, tuple)):
        return [_convert_value_recursive(item) for item in value]
    
    # set â†’ list
    if isinstance(value, set):
        return [_convert_value_recursive(item) for item in value]
    
    # bytes â†’ base64 (ì„ íƒì )
    if isinstance(value, bytes):
        import base64
        return base64.b64encode(value).decode('utf-8')
    
    # datetime ì²˜ë¦¬
    try:
        from datetime import datetime, date
        if isinstance(value, datetime):
            return value.isoformat()
        if isinstance(value, date):
            return value.isoformat()
    except ImportError:
        pass
    
    # ê¸°íƒ€: ë¬¸ìì—´ë¡œ í´ë°±
    try:
        return str(value)
    except Exception:
        return "[UNSERIALIZABLE]"

def _strip_state_history(obj: Any) -> Any:
    """
    ë‚´ë¶€ ìƒíƒœ íˆìŠ¤í† ë¦¬(state_history)ë¥¼ ì œê±°í•˜ì—¬ í˜ì´ë¡œë“œ í¬ê¸°ë¥¼ ì¤„ì…ë‹ˆë‹¤.
    ì„±ëŠ¥ ìµœì í™”: copy.deepcopy ëŒ€ì‹  í•„ìš”í•œ ë¶€ë¶„ë§Œ ì–•ì€ ë³µì‚¬ ë˜ëŠ” ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
    """
    if not isinstance(obj, dict):
        return obj
    
    # ìµœìƒìœ„ ë ˆë²¨ì—ì„œ state_history ì œê±° (ì–•ì€ ë³µì‚¬)
    out = obj.copy()
    out.pop('state_history', None)
    
    # ì¤‘ì²©ëœ í‚¤ ì²˜ë¦¬ (ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ ì•Šê³  íŠ¹ì • í‚¤ë§Œ íƒ€ê²ŸíŒ…í•˜ì—¬ ì„±ëŠ¥ í™•ë³´)
    # state_data, step_function_state, current_state ë‚´ë¶€ì˜ state_history ì œê±°
    target_keys = ['state_data', 'step_function_state', 'current_state']
    
    for key in target_keys:
        val = out.get(key)
        if isinstance(val, dict):
            # í•´ë‹¹ ë”•ì…”ë„ˆë¦¬ë§Œ ì–•ì€ ë³µì‚¬ í›„ ìˆ˜ì •
            new_val = val.copy()
            new_val.pop('state_history', None)
            out[key] = new_val
            
    return out

def _fetch_existing_history_from_s3(execution_id: str) -> List[Dict]:
    """
    S3ì—ì„œ ê¸°ì¡´ íˆìŠ¤í† ë¦¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    [v2.1 ë°ì´í„° ë¬´ê²°ì„± ê°•í™”]
    segment_runnerê°€ ëˆ„ì  íˆìŠ¤í† ë¦¬ë¥¼ ë³´ë‚´ë”ë¼ë„,
    í•¸ë“¤ëŸ¬ ë ˆë²¨ì—ì„œ S3ì˜ ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©í•˜ì—¬ ë°ì´í„° ìœ ì‹¤ì„ ë°©ì§€í•©ë‹ˆë‹¤.
    
    Returns:
        ê¸°ì¡´ íˆìŠ¤í† ë¦¬ ë¦¬ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
    """
    if not SKELETON_S3_BUCKET:
        return []
    
    try:
        safe_id = execution_id.split(':')[-1]
        s3_key = f"executions/{safe_id}.json"
        
        response = s3_client.get_object(
            Bucket=SKELETON_S3_BUCKET,
            Key=s3_key
        )
        existing_data = json.loads(response['Body'].read().decode('utf-8'))
        existing_history = existing_data.get('state_history', [])
        
        if isinstance(existing_history, list):
            logger.info(f"Fetched {len(existing_history)} existing history entries from S3")
            return existing_history
        return []
        
    except s3_client.exceptions.NoSuchKey:
        logger.debug(f"No existing history in S3 for {execution_id}")
        return []
    except Exception as e:
        logger.warning(f"Failed to fetch existing history from S3: {e}")
        return []


def _merge_history_logs(
    existing_history: List[Dict], 
    new_logs: List[Dict],
    max_entries: int = 50
) -> List[Dict]:
    """
    ê¸°ì¡´ íˆìŠ¤í† ë¦¬ì™€ ìƒˆ ë¡œê·¸ë¥¼ ì¤‘ë³µ ì—†ì´ ë³‘í•©í•©ë‹ˆë‹¤.
    
    [v2.1 ë°ì´í„° ë¬´ê²°ì„± ê°•í™”]
    - ì¤‘ë³µ ì œê±°: timestamp + node_id ê¸°ë°˜ ê³ ìœ  í‚¤
    - ìˆœì„œ ë³´ì¥: timestamp ê¸°ì¤€ ì •ë ¬
    - í¬ê¸° ì œí•œ: ìµœëŒ€ max_entriesê°œë§Œ ìœ ì§€ (FIFO)
    
    Args:
        existing_history: S3ì—ì„œ ê°€ì ¸ì˜¨ ê¸°ì¡´ íˆìŠ¤í† ë¦¬
        new_logs: ìƒˆë¡œ ì¶”ê°€í•  ë¡œê·¸ (segment_runnerì—ì„œ ì „ë‹¬)
        max_entries: ìµœëŒ€ íˆìŠ¤í† ë¦¬ ì—”íŠ¸ë¦¬ ìˆ˜
    
    Returns:
        ë³‘í•©ëœ íˆìŠ¤í† ë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    if not new_logs:
        return existing_history[-max_entries:] if existing_history else []
    
    if not existing_history:
        return new_logs[-max_entries:]
    
    # ê³ ìœ  í‚¤ ìƒì„± í•¨ìˆ˜
    def get_entry_key(entry: Dict) -> str:
        if not isinstance(entry, dict):
            return str(entry)
        ts = entry.get('timestamp', entry.get('time', 0))
        node_id = entry.get('node_id', entry.get('nodeId', 'unknown'))
        return f"{ts}:{node_id}"
    
    # ê¸°ì¡´ íˆìŠ¤í† ë¦¬ì˜ í‚¤ ì§‘í•©
    existing_keys = {get_entry_key(e) for e in existing_history}
    
    # ìƒˆ ë¡œê·¸ ì¤‘ ì¤‘ë³µ ì•„ë‹Œ ê²ƒë§Œ ì¶”ê°€
    merged = list(existing_history)
    new_added = 0
    for log in new_logs:
        key = get_entry_key(log)
        if key not in existing_keys:
            merged.append(log)
            existing_keys.add(key)
            new_added += 1
    
    # timestamp ê¸°ì¤€ ì •ë ¬ (ì•ˆì •ì ì¸ ìˆœì„œ ë³´ì¥)
    try:
        merged.sort(key=lambda x: x.get('timestamp', x.get('time', 0)) if isinstance(x, dict) else 0)
    except Exception as e:
        logger.warning(f"Failed to sort merged history: {e}")
    
    # ìµœëŒ€ ì—”íŠ¸ë¦¬ ìˆ˜ ì œí•œ
    if len(merged) > max_entries:
        merged = merged[-max_entries:]
    
    logger.info(f"Merged history: {len(existing_history)} existing + {new_added} new = {len(merged)} total")
    return merged


def _upload_history_to_s3(execution_id: str, payload: dict) -> str:
    """
    ì „ì²´ ì‹¤í–‰ ì´ë ¥ì„ S3ì— ì—…ë¡œë“œí•˜ê³  í‚¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not SKELETON_S3_BUCKET:
        logger.warning("SKELETON_S3_BUCKET not configured, skipping S3 upload")
        return None

    try:
        # S3 í‚¤ ìƒì„± (executions/{execution_id}.json)
        # execution_idê°€ ARNì¸ ê²½ìš° ë§ˆì§€ë§‰ ë¶€ë¶„ë§Œ ì‚¬ìš©í•˜ê±°ë‚˜ ì „ì²´ë¥¼ ì•ˆì „í•˜ê²Œ ë³€í™˜
        safe_id = execution_id.split(':')[-1]
        s3_key = f"executions/{safe_id}.json"
        
        s3_client.put_object(
            Bucket=SKELETON_S3_BUCKET,
            Key=s3_key,
            Body=json.dumps(payload, default=str),
            ContentType='application/json'
        )
        logger.info(f"Uploaded execution history to s3://{SKELETON_S3_BUCKET}/{s3_key}")
        return s3_key
    except Exception as e:
        logger.error(f"Failed to upload history to S3: {e}")
        return None

def _update_execution_status(owner_id: str, notification_payload: dict) -> bool:
    """
    ExecutionsTableì— í˜„ì¬ ì‹¤í–‰ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    ë¹„ìš© ìµœì í™”: put_item ëŒ€ì‹  update_itemì„ ì‚¬ìš©í•˜ì—¬ WCU ì ˆì•½ ë° ë®ì–´ì“°ê¸° ë°©ì§€.
    """
    if not executions_table:
        logger.error("EXECUTIONS_TABLE not configured")
        return False

    try:
        timestamp = int(time.time())
        inner = notification_payload.get('payload', {})
        if not isinstance(inner, dict): inner = {}
        
        # [FIX] execution_idëŠ” notification_payload ìµœìƒìœ„ ë˜ëŠ” innerì— ìˆì„ ìˆ˜ ìˆìŒ
        exec_id = inner.get('execution_id') or notification_payload.get('execution_id') or notification_payload.get('conversation_id')
        if not exec_id:
            logger.warning(f"[DEBUG] No execution_id found. notification_payload keys: {list(notification_payload.keys())}, inner keys: {list(inner.keys())}")
            return False

        current_status = str(inner.get('status', '') or notification_payload.get('status', '')).upper()
        
        # 1. S3ì— ì „ì²´ ë°ì´í„° ì—…ë¡œë“œ (Claim Check Pattern)
        # [FIX] Initialize full_state, then look for state_data or step_function_state
        raw_state = inner.get('step_function_state') or inner.get('state_data') or notification_payload.get('state_data')
        full_state = _safe_json_compatible(raw_state) if raw_state else {}
        
        # [v2.1 ë°ì´í„° ë¬´ê²°ì„± ê°•í™”] S3ì—ì„œ ê¸°ì¡´ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì™€ì„œ ë³‘í•©
        # segment_runnerê°€ ëˆ„ì  íˆìŠ¤í† ë¦¬ë¥¼ ë³´ë‚´ë”ë¼ë„, í•¸ë“¤ëŸ¬ ë ˆë²¨ì—ì„œ ê²€ì¦/ë³‘í•©
        new_logs = notification_payload.get('new_history_logs') or inner.get('new_history_logs')
        
        try:
            MAX_HISTORY = int(os.environ.get('STATE_HISTORY_MAX_ENTRIES', '50'))
        except:
            MAX_HISTORY = 50
        
        if new_logs and isinstance(new_logs, list):
            # [í•µì‹¬ ë³€ê²½] S3ì—ì„œ ê¸°ì¡´ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸° (ë®ì–´ì“°ê¸° ë°©ì§€)
            existing_history = _fetch_existing_history_from_s3(exec_id)
            
            # ê¸°ì¡´ íˆìŠ¤í† ë¦¬ì™€ ìƒˆ ë¡œê·¸ ë³‘í•© (ì¤‘ë³µ ì œê±°, ìˆœì„œ ë³´ì¥)
            merged_history = _merge_history_logs(
                existing_history=existing_history,
                new_logs=new_logs,
                max_entries=MAX_HISTORY
            )
            
            full_state['state_history'] = merged_history
            
            logger.info(
                f"[HistoryMerge] exec={exec_id[:16]}..., "
                f"existing={len(existing_history)}, new={len(new_logs)}, merged={len(merged_history)}"
            )
        else:
            # new_logsê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ state_history ìœ ì§€ ë˜ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸
            current_history = full_state.get('state_history', [])
            if not isinstance(current_history, list):
                current_history = []
            full_state['state_history'] = current_history
                
        history_s3_key = _upload_history_to_s3(exec_id, full_state)
        logger.info(f"[DEBUG] history_s3_key after upload: {history_s3_key}")
        
        # 2. DynamoDB ì €ì¥ìš© ë°ì´í„° ì¤€ë¹„ (state_history ì œê±°)
        step_state = _strip_state_history(full_state)
        
        # DynamoDB UpdateExpression êµ¬ì„±
        key = {
            'ownerId': owner_id,
            'executionArn': exec_id
        }
        
        update_expr_parts = [
            "#st = :st",
            "#ua = :ua",
            "#sfs = :sfs"
        ]
        expr_names = {
            "#st": "status",
            "#ua": "updatedAt",
            "#sfs": "step_function_state"
        }
        expr_values = {
            ":st": current_status,
            ":ua": timestamp,
            ":sfs": _convert_to_dynamodb_format(step_state)
        }
        
        # S3 í‚¤ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if history_s3_key:
            update_expr_parts.append("#hsk = :hsk")
            expr_names["#hsk"] = "history_s3_key"
            expr_values[":hsk"] = history_s3_key

        update_expr = "SET " + ", ".join(update_expr_parts)

        executions_table.update_item(
            Key=key,
            UpdateExpression=update_expr,
            ExpressionAttributeNames=expr_names,
            ExpressionAttributeValues=expr_values
        )
        logger.info(f"Updated execution status (partial): {exec_id} -> {current_status}")

        return True

    except Exception as e:
        logger.error(f"Failed to update execution status: {e}")
        return False

def lambda_handler(event: Any, context: Any) -> Dict[str, Any]:
    # 1. Payload ì¶”ì¶œ ë° ê²€ì¦
    payload = _get_payload(event)

    # ë‚´ë¶€ í˜¸ì¶œ ê²€ì¦
    if event.get('requestContext'):
        logger.error('External invocation forbidden')
        return {"status": "error", "message": "Forbidden"}

    # [Robustness] Step Functions input.$: $ ë³€ê²½ ëŒ€ì‘
    # inputì´ ì¤‘ì²©ë˜ì–´ ë“¤ì–´ì˜¬ ìˆ˜ ìˆìŒ (ì˜ˆ: { "input": { ... }, "TaskToken": ... })
    if 'input' in payload and isinstance(payload['input'], dict):
        # Merge input fields into top-level payload for easier access
        # [Fix] Do not overwrite existing keys (like 'status', 'message') with input data
        # which might contain stale state
        for k, v in payload['input'].items():
            if k not in payload:
                payload[k] = v

    owner_id = payload.get('ownerId') or payload.get('owner_id')
    
    # [Robustness] ownerIdê°€ ìµœìƒìœ„ì— ì—†ìœ¼ë©´ state_data ë‚´ë¶€ í™•ì¸
    if not owner_id and isinstance(payload.get('state_data'), dict):
        owner_id = payload['state_data'].get('ownerId')

    if not owner_id:
        logger.error('Missing ownerId in payload: keys=%s', list(payload.keys()))
        # [Critical Fix] Crash prevention: Return error but don't fail Lambda execution
        # This allows Step Functions to receive a response (even if it's an error)
        return {"status": "error", "message": "Missing ownerId"}

    # [ì¶”ê°€] ìƒíƒœê°’ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 'STARTED' ë¶€ì—¬ (íŠ¹íˆ ì‹œì‘ ë‹¨ê³„ì¼ ë•Œ)
    if 'status' not in payload:
        payload['status'] = 'STARTED'
        payload['message'] = payload.get('message', 'ì‹œìŠ¤í…œì´ ì´ˆê¸°í™” ì¤‘ì…ë‹ˆë‹¤...')

    # 2. Notification Payload êµ¬ì„±
    # ë°ì´í„° ì¶”ì¶œ ë¡œì§ ê°„ì†Œí™” (get with defaults)
    state_data = payload.get('state_data') or {}
    if not isinstance(state_data, dict): state_data = {} # ë°©ì–´ ì½”ë“œ

    # [Glass Box] ì‹œì‘ ì‹œê°„ ì¶”ì¶œ
    start_time = payload.get('start_time') or state_data.get('start_time')
    if not start_time:
        start_time = int(time.time())

    # [Glass Box] ìƒíƒœë³„ ì‹¤í–‰ ì‹œê°„ ì¶”ì 
    state_durations = state_data.get('state_durations', {}) or {}
    current_status = payload.get('status')
    prev_update_time = state_data.get('last_update_time', start_time)
    current_time = int(time.time())
    
    # ì´ì „ ìƒíƒœì˜ ì‹¤í–‰ ì‹œê°„ ëˆ„ì  (ìƒíƒœ ë³€ê²½ ì‹œ)
    if current_status and prev_update_time:
        elapsed = current_time - prev_update_time
        if elapsed > 0:  # ìŒìˆ˜ ë°©ì§€
            state_durations[current_status] = state_durations.get(current_status, 0) + elapsed

    # [Glass Box] ì¶”ê°€ ë©”íƒ€ë°ì´í„° ê³„ì‚°
    current_segment = payload.get('segment_to_run') or state_data.get('segment_to_run', 0)
    total_segments = payload.get('total_segments') or state_data.get('total_segments', 1)
    
    # 1. ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ (ETA) ê³„ì‚°
    estimated_completion_time = None
    estimated_remaining_seconds = None
    if total_segments > 1 and current_segment >= 0:
        completed_segments = current_segment  # í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì„¸ê·¸ë¨¼íŠ¸ê¹Œì§€
        remaining_segments = total_segments - current_segment - 1  # ë‚¨ì€ ì„¸ê·¸ë¨¼íŠ¸
        
        if completed_segments > 0:
            total_elapsed = current_time - start_time
            avg_segment_time = total_elapsed / completed_segments
            estimated_remaining_seconds = int(avg_segment_time * remaining_segments)
            estimated_completion_time = current_time + estimated_remaining_seconds
    
    # 2. í˜„ì¬ ë‹¨ê³„ ì´ë¦„/ì„¤ëª…
    current_step_label = f"ì„¸ê·¸ë¨¼íŠ¸ {current_segment + 1}/{total_segments} ì²˜ë¦¬ ì¤‘"
    partition_map = state_data.get('partition_map', [])
    if isinstance(partition_map, list) and current_segment < len(partition_map):
        segment_info = partition_map[current_segment]
        if isinstance(segment_info, dict):
            segment_type = segment_info.get('type', '')
            if segment_type == 'llm':
                current_step_label = f"ì„¸ê·¸ë¨¼íŠ¸ {current_segment + 1}/{total_segments}: AI ëª¨ë¸ ì‹¤í–‰ ì¤‘"
            elif segment_type == 'hitp':
                current_step_label = f"ì„¸ê·¸ë¨¼íŠ¸ {current_segment + 1}/{total_segments}: ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸° ì¤‘"
            elif segment_type == 'isolated':
                current_step_label = f"ì„¸ê·¸ë¨¼íŠ¸ {current_segment + 1}/{total_segments}: ë…ë¦½ ì‹¤í–‰ ì¤‘"
            else:
                current_step_label = f"ì„¸ê·¸ë¨¼íŠ¸ {current_segment + 1}/{total_segments}: ì¼ë°˜ ì²˜ë¦¬ ì¤‘"
    
    # 3. ì„¸ê·¸ë¨¼íŠ¸ë‹¹ í‰ê·  ì²˜ë¦¬ ì†ë„
    average_segment_duration = None
    if current_segment > 0:
        total_elapsed = current_time - start_time
        average_segment_duration = int(total_elapsed / current_segment)

    # Sequence number ìƒì„± (ë©”ì‹œì§€ ìˆœì„œ ë³´ì¥ìš©)
    sequence_number = int(time.time() * 1000000)  # ë§ˆì´í¬ë¡œì´ˆ ë‹¨ìœ„

    inner_payload = {
        'action': payload.get('notification_type', 'execution_progress'),
        'conversation_id': payload.get('conversation_id') or payload.get('conversationId'),
        'message': payload.get('message', 'Execution progress update'),
        'status': payload.get('status'),
        
        # [CRITICAL FIX] execution_idì™€ workflowIdë¥¼ inner_payloadì— í¬í•¨ (DB ì €ì¥ìš©)
        # execution_idëŠ” ì—¬ëŸ¬ ìœ„ì¹˜ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŒ:
        # 1. payload.execution_id (SegmentProgress events)
        # 2. payload.conversation_id (fallback)
        # 3. state_data.executionArn (completion events ì—ì„œ state_data ì•ˆì— ìˆì„ ìˆ˜ ìˆìŒ)
        # 4. payload.$$.Execution.Id ê°’ì´ state_dataì— ì €ì¥ë  ìˆ˜ ìˆìŒ
        'execution_id': (
            payload.get('execution_id') 
            or payload.get('conversation_id')
            or state_data.get('executionArn')
            or state_data.get('execution_id')
        ),
        'workflowId': payload.get('workflowId') or state_data.get('workflowId'),
        
        # ìˆœì„œ ë³´ì¥ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„°
        'sequence_number': sequence_number,
        'server_timestamp': current_time,
        'segment_sequence': current_segment,  # ì„¸ê·¸ë¨¼íŠ¸ ê¸°ë°˜ ìˆœì„œ
        
        # [Glass Box] í•µì‹¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        'current_segment': current_segment,
        'total_segments': total_segments,
        'start_time': start_time,
        'last_update_time': current_time,
        'state_durations': state_durations,
        # [Added] final_result ì¶”ì¶œ (execution_result.final_state)
        'final_result': (payload.get('execution_result') or {}).get('final_state'),
        # [Fix] Error details extraction from src.flat payload (due to Step Functions parameter simplification)
        'error_details': payload.get('error_details') or payload.get('partition_error') or payload.get('async_error') or payload.get('error_info') or payload.get('notification_error') or payload.get('failure_notification_error'),
        
        # [NEW] workflow_config for graph visualization (only nodes/edges to minimize size)
        'workflow_config': _extract_graph_config(state_data.get('workflow_config')),
        # [NEW] state_history for node status tracking
        'state_history': payload.get('new_history_logs') or state_data.get('state_history', []),
    }

    # [Added] Construct final notification payload
    notification_payload = {
        'type': 'workflow_status',
        'payload': inner_payload
    }

    # [Data Integrity] DB ì €ì¥ì„ ìœ„í•œ ì›ë³¸ ë°ì´í„° ë³´ì¡´
    # notification_payloadëŠ” WebSocket ìš©ëŸ‰ ì œí•œìœ¼ë¡œ ì¸í•´ ìˆ˜ì •(Skeleton)ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
    # DBì—ëŠ” í•­ìƒ ì›ë³¸ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ ë³„ë„ ë³€ìˆ˜ë¡œ ë³µì‚¬í•´ë‘¡ë‹ˆë‹¤.
    db_payload = notification_payload.copy()

    # WebSocket payload í¬ê¸° ì œí•œ ì²´í¬ (128KB = 131072 bytes)
    msg_str = json.dumps(notification_payload, cls=DecimalEncoder)
    msg_bytes = msg_str.encode('utf-8')
    MAX_PAYLOAD_SIZE = 128 * 1024 # 128KB
    
    if len(msg_bytes) > MAX_PAYLOAD_SIZE:
        logger.warning(f"WebSocket payload size {len(msg_bytes)} exceeds limit {MAX_PAYLOAD_SIZE}, removing step_function_state")
        # 1ì°¨ ì‹œë„: step_function_state ì œê±°
        inner_payload.pop('step_function_state', None)
        notification_payload['payload'] = inner_payload
        msg_str = json.dumps(notification_payload, cls=DecimalEncoder)
        msg_bytes = msg_str.encode('utf-8')
        
        if len(msg_bytes) > MAX_PAYLOAD_SIZE:
            logger.error(f"Payload still too large ({len(msg_bytes)} bytes). Sending skeleton payload.")
            # 2ì°¨ ì‹œë„: Skeleton Payload ì „ì†¡ (Robustness)
            skeleton_payload = {
                'type': 'workflow_status',
                'payload': {
                    'action': inner_payload.get('action'),
                    'conversation_id': inner_payload.get('conversation_id'),
                    'status': inner_payload.get('status'),
                    'message': "Data too large to display via WebSocket. Please check history tab.",
                    'error': "PAYLOAD_TOO_LARGE",
                    'current_segment': inner_payload.get('current_segment'),
                    'total_segments': inner_payload.get('total_segments'),
                }
            }
            msg_str = json.dumps(skeleton_payload, cls=DecimalEncoder)
            msg_bytes = msg_str.encode('utf-8')
            # skeleton payloadë¥¼ notification_payloadë¡œ êµì²´
            notification_payload = skeleton_payload

    # 3. Endpoint URL ê²°ì •
    endpoint_url = WEBSOCKET_ENDPOINT_URL
    if not endpoint_url:
        # state_data ë‚´ì˜ callback context íƒìƒ‰
        callback_context = (
            state_data.get('current_state', {}).get('__callback_context') or
            state_data.get('initial_state', {}).get('__callback_context') or
            state_data.get('__callback_context')
        )
        if callback_context:
            domain = callback_context.get('domainName')
            stage = callback_context.get('stage')
            if domain and stage:
                endpoint_url = f"https://{domain}/{stage}"

    # 4. WebSocket ì „ì†¡ ì‹œë„
    connection_ids = get_user_connection_ids(owner_id)
    success_count = 0
    
    if connection_ids and endpoint_url:
        apigw = get_apigw_client(endpoint_url)
        # msg_bytesëŠ” ìœ„ì—ì„œ ê³„ì‚°ë¨ (skeletonì¼ ìˆ˜ë„ ìˆìŒ)
        
        for conn_id in connection_ids:
            try:
                apigw.post_to_connection(ConnectionId=conn_id, Data=msg_bytes)
                success_count += 1
            except ClientError as e:
                if e.response['Error']['Code'] == 'GoneException':
                    _delete_connection(conn_id)
                else:
                    logger.warning(f"Failed to send to {conn_id}: {e}")
            except Exception as e:
                logger.warning(f"Unexpected error sending to {conn_id}: {e}")
    else:
        if not connection_ids:
            logger.info(f"No active connections for owner={owner_id}")
        if not endpoint_url:
            logger.warning("No endpoint_url available")
    
    # [v2.1 ì—°ê²° ëŠê¹€ ìƒíƒœ ì¶”ì ] ëª¨ë“  ì—°ê²°ì´ ëŠê¸´ ê²½ìš° DBì— í”Œë˜ê·¸ ì €ì¥
    # ì‚¬ìš©ì ì¬ì ‘ì† ì‹œ ì¦‰ì‹œ ë§ˆì§€ë§‰ ìƒíƒœë¥¼ fetchí•  ìˆ˜ ìˆë„ë¡ í•¨
    all_connections_gone = (
        connection_ids and 
        endpoint_url and 
        success_count == 0 and 
        len(connection_ids) > 0
    )
    
    if all_connections_gone:
        _mark_no_active_session(owner_id, inner_payload.get('execution_id'))

    # 5. DB ì—…ë°ì´íŠ¸ ì—¬ë¶€ ê²°ì • (ì „ëµ ê¸°ë°˜)
    should_update_db = should_update_database(payload, state_data)

    db_saved = False
    if should_update_db:
        # ì—…ë°ì´íŠ¸ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        db_payload_with_meta = db_payload.copy()
        db_payload_with_meta['payload']['last_db_update_time'] = current_time
        db_payload_with_meta['payload']['last_db_progress_percentage'] = (current_segment / total_segments) * 100 if total_segments > 0 else 0
        
        # [ìˆ˜ì •] DBì—ëŠ” ì›ë³¸ ë°ì´í„°(db_payload)ë¥¼ ì €ì¥í•˜ì—¬ ë°ì´í„° ìœ ì‹¤ ë°©ì§€
        db_saved = _update_execution_status(owner_id, db_payload_with_meta)
        logger.info(f"DB updated for execution {inner_payload.get('execution_id')} (strategy: {DB_UPDATE_STRATEGY})")
    else:
        logger.debug(f"DB update skipped for execution {inner_payload.get('execution_id')} (strategy: {DB_UPDATE_STRATEGY})")

    # ë©”íŠ¸ë¦­ ë°œí–‰
    publish_db_update_metrics(inner_payload.get('execution_id', ''), db_saved, DB_UPDATE_STRATEGY)
    
    logger.info(f"Processed: owner={owner_id}, sent={success_count}, saved={db_saved}, strategy={DB_UPDATE_STRATEGY}")
    return {
        'status': 'success', 
        'ws_sent': success_count, 
        'db_saved': db_saved,
        'db_update_skipped': not should_update_db,
        'update_strategy': DB_UPDATE_STRATEGY,
        'updated_state_data': {
            'state_durations': state_durations,
            'last_update_time': current_time,
            'start_time': start_time,
            # [NEW] Return updated state_history so SFN can update its state
            'state_history': db_payload.get('payload', {}).get('state_data', {}).get('state_history') 
                if db_saved and 'new_history_logs' in payload else None
        }
    }