# -*- coding: utf-8 -*-
"""
Instruction Distiller Lambda

HITL ë‹¨ê³„ì—ì„œ ì‚¬ìš©ìê°€ ìˆ˜ì •í•œ ê²°ê³¼ë¬¼ì„ ë¶„ì„í•˜ì—¬ 
ì•”ë¬µì  ì§€ì¹¨ì„ ì¶”ì¶œí•˜ëŠ” ë¹„ë™ê¸° ì¦ë¥˜ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.

íŠ¸ë¦¬ê±°: HITL ìŠ¹ì¸ ì™„ë£Œ ì´ë²¤íŠ¸ (EventBridge ë˜ëŠ” SNS)
í”„ë¡œì„¸ìŠ¤:
  1. S3ì—ì„œ original_outputê³¼ user_corrected_output ë¡œë“œ
  2. LLM(Haiku)ì´ ë‘ í…ìŠ¤íŠ¸ì˜ ì°¨ì´ì (diff) ë¶„ì„
  3. ì¶”ì¶œëœ ì§€ì¹¨ì„ DistilledInstructions í…Œì´ë¸”ì— ì €ì¥
  4. ë‹¤ìŒ ì‹¤í–‰ë¶€í„° í•´ë‹¹ ì§€ì¹¨ ìë™ ë°˜ì˜
  5. [ì‹ ê·œ] ì¶©ëŒ ê°ì§€ ë° ìë™ í•´ê²° (InstructionConflictService)
"""

import os
import json
import logging
import hashlib
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional, Tuple
from decimal import Decimal
from dataclasses import dataclass
from enum import Enum

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError

# ê³µí†µ ìœ í‹¸ë¦¬í‹° import
try:
    from src.common.constants import is_mock_mode, LLMModels
    from src.common.secrets_utils import get_gemini_api_key
    from src.common.aws_clients import get_dynamodb_resource, get_s3_client, get_bedrock_client
    from src.common.logging_utils import (
        get_logger as get_powertools_logger,
        get_metrics,
        get_tracer,
        log_external_service_call,
        log_business_event,
        log_execution_context
    )
    _USE_COMMON = True
    _USE_POWERTOOLS = True
except ImportError:
    _USE_COMMON = False
    _USE_POWERTOOLS = False
    is_mock_mode = lambda: os.environ.get("MOCK_MODE", "false").lower() in {"true", "1", "yes", "on"}
    get_gemini_api_key = None
    
    # Fallback: ë°ì½”ë ˆì´í„° no-op
    def log_external_service_call(service_name, operation):
        def decorator(func):
            return func
        return decorator
    
    def log_execution_context(func):
        return func
    
    def log_business_event(*args, **kwargs):
        pass

# ì¶©ëŒ ê°ì§€ ì„œë¹„ìŠ¤ import
try:
    from src.services.instruction_conflict_service import InstructionConflictService
    _USE_CONFLICT_SERVICE = True
except ImportError:
    _USE_CONFLICT_SERVICE = False
    InstructionConflictService = None

# Model Router - Thinking Level Control
try:
    from src.common.model_router import (
        calculate_thinking_budget,
        get_thinking_config_for_workflow,
        ThinkingLevel,
        THINKING_BUDGET_MAP,
    )
    _USE_THINKING_CONTROL = True
except ImportError:
    _USE_THINKING_CONTROL = False
    calculate_thinking_budget = None
    ThinkingLevel = None
    THINKING_BUDGET_MAP = {}

# ë¡œê±° ì„¤ì • (Powertools ì‚¬ìš© ê°€ëŠ¥ ì‹œ êµ¬ì¡°í™”ëœ ë¡œê¹…)
if _USE_COMMON and _USE_POWERTOOLS:
    logger = get_powertools_logger(__name__)
    tracer = get_tracer()
    metrics = get_metrics()
else:
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    tracer = None
    metrics = None

# í™˜ê²½ ë³€ìˆ˜ - ğŸš¨ [Critical Fix] ê¸°ë³¸ê°’ì„ template.yamlê³¼ ì¼ì¹˜ì‹œí‚´
DISTILLED_INSTRUCTIONS_TABLE = os.environ.get("DISTILLED_INSTRUCTIONS_TABLE", "DistilledInstructionsTable")
S3_BUCKET = os.environ.get("WORKFLOW_STATE_BUCKET", "")
BEDROCK_REGION = os.environ.get("AWS_REGION", "us-east-1")

# Vertex AI í™˜ê²½ë³€ìˆ˜ (GitHub Secretsì—ì„œ ì£¼ì…)
GCP_PROJECT_ID = os.environ.get("GCP_PROJECT_ID", "")
GCP_LOCATION = os.environ.get("GCP_LOCATION", "us-central1")
GCP_SERVICE_ACCOUNT_KEY = os.environ.get("GCP_SERVICE_ACCOUNT_KEY", "")  # JSON string

# Fallback Model (Bedrock Haiku - ë ˆê±°ì‹œ í˜¸í™˜)
HAIKU_MODEL_ID = "anthropic.claude-3-haiku-20240307-v1:0"

# Gemini Native ì„¤ì •
GEMINI_FLASH_MODEL = "gemini-1.5-flash"  # ë¹„ìš© íš¨ìœ¨ì 
GEMINI_2_FLASH_MODEL = "gemini-2.0-flash"  # ê³ ì„±ëŠ¥
USE_GEMINI_NATIVE = os.environ.get("USE_GEMINI_NATIVE", "true").lower() == "true"

# ì§€ì¹¨ ê°€ì¤‘ì¹˜ ì„¤ì •
DEFAULT_INSTRUCTION_WEIGHT = Decimal("1.0")
MIN_INSTRUCTION_WEIGHT = Decimal("0.1")
BASE_WEIGHT_DECAY = Decimal("0.3")  # ê¸°ë³¸ ì¬ìˆ˜ì • ì‹œ ê°€ì¤‘ì¹˜ ê°ì†ŒëŸ‰ (ë™ì  ì¡°ì ˆë¨)
MAX_INSTRUCTIONS_PER_NODE = 10  # ë…¸ë“œë‹¹ ìµœëŒ€ ì§€ì¹¨ ìˆ˜
COMPRESSED_INSTRUCTION_COUNT = 3  # ì••ì¶• í›„ ìµœëŒ€ ì§€ì¹¨ ìˆ˜
FEW_SHOT_EXAMPLE_MAX_LENGTH = 500  # Few-shot ì˜ˆì‹œ ìµœëŒ€ ê¸¸ì´

# Context Caching ì„¤ì •
CONTEXT_CACHE_TTL_SECONDS = 3600  # 1ì‹œê°„
CONTEXT_CACHE_MIN_TOKENS = 32000  # ìµœì†Œ í† í° ìˆ˜ (ìºì‹± ì¡°ê±´)
ENABLE_CONTEXT_CACHING = os.environ.get("ENABLE_CONTEXT_CACHING", "true").lower() == "true"

# ì§€ì¹¨ êµ¬ì¡° (ë ˆê±°ì‹œ Python dict)
INSTRUCTION_SCHEMA_LEGACY = {
    "text": str,  # ì§€ì¹¨ í…ìŠ¤íŠ¸
    "weight": Decimal,  # ê°€ì¤‘ì¹˜ (1.0 ~ 0.1)
    "created_at": str,  # ìƒì„± ì‹œê°„
    "last_used": str,  # ë§ˆì§€ë§‰ ì‚¬ìš© ì‹œê°„
    "usage_count": int,  # ì‚¬ìš© íšŸìˆ˜
    "is_active": bool  # í™œì„±í™” ì—¬ë¶€
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Gemini Structured Output JSON Schema (Response Schema)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GEMINI_DISTILLATION_SCHEMA = {
    "type": "object",
    "properties": {
        "instructions": {
            "type": "array",
            "description": "ì¶”ì¶œëœ ì§€ì¹¨ ëª©ë¡ (êµ¬ì²´ì ì´ê³  ì¶”ìƒí™”ëœ ê·œì¹™)",
            "items": {
                "type": "object",
                "properties": {
                    "text": {"type": "string", "description": "ì§€ì¹¨ í…ìŠ¤íŠ¸"},
                    "category": {
                        "type": "string",
                        "enum": ["style", "content", "format", "tone", "prohibition"],
                        "description": "ì§€ì¹¨ ìœ í˜•"
                    },
                    "confidence": {
                        "type": "number",
                        "description": "ì¶”ì¶œ ì‹ ë¢°ë„ (0.0~1.0)"
                    }
                },
                "required": ["text", "category"]
            }
        },
        "semantic_diff_score": {
            "type": "number",
            "description": "ì›ë³¸ê³¼ ìˆ˜ì •ë³¸ ê°„ì˜ ì˜ë¯¸ì  ì°¨ì´ ì ìˆ˜ (0.0=ë™ì¼, 1.0=ì™„ì „íˆ ë‹¤ë¦„)"
        },
        "reasoning": {
            "type": "string",
            "description": "ìˆ˜ì • íŒ¨í„´ì„ ë¶„ì„í•œ ì´ìœ  ë° ê·¼ê±°"
        },
        "is_typo_only": {
            "type": "boolean",
            "description": "ë‹¨ìˆœ ì˜¤íƒ€ ìˆ˜ì •ë§Œ ìˆëŠ” ê²½ìš° true"
        }
    },
    "required": ["instructions", "semantic_diff_score", "reasoning", "is_typo_only"]
}

# ì§€ì¹¨ ì¶©ëŒ ê²€ì‚¬ìš© ìŠ¤í‚¤ë§ˆ
GEMINI_CONFLICT_CHECK_SCHEMA = {
    "type": "object",
    "properties": {
        "has_conflicts": {"type": "boolean"},
        "conflicts": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "instruction_a": {"type": "string"},
                    "instruction_b": {"type": "string"},
                    "conflict_type": {
                        "type": "string",
                        "enum": ["contradiction", "redundancy", "ambiguity"]
                    },
                    "resolution": {"type": "string"}
                },
                "required": ["instruction_a", "instruction_b", "conflict_type", "resolution"]
            }
        },
        "resolved_instructions": {
            "type": "array",
            "items": {"type": "string"}
        }
    },
    "required": ["has_conflicts", "resolved_instructions"]
}

# ì§€ëŠ¥í˜• ì§€ì¹¨ ì••ì¶•(Compression)ìš© ìŠ¤í‚¤ë§ˆ
GEMINI_COMPRESSION_SCHEMA = {
    "type": "object",
    "properties": {
        "compressed_instructions": {
            "type": "array",
            "description": "ì••ì¶•ëœ í•µì‹¬ ì§€ì¹¨ (3ê°œ ì´ë‚´)",
            "items": {"type": "string"},
            "maxItems": 3
        },
        "preserved_meaning": {
            "type": "boolean",
            "description": "ì›ë³¸ ì§€ì¹¨ë“¤ì˜ í•µì‹¬ ì˜ë¯¸ê°€ ë³´ì¡´ë˜ì—ˆëŠ”ì§€"
        },
        "compression_ratio": {
            "type": "number",
            "description": "ì••ì¶• ë¹„ìœ¨ (0.0~1.0, ë‚®ì„ìˆ˜ë¡ ë” ë§ì´ ì••ì¶•)"
        }
    },
    "required": ["compressed_instructions", "preserved_meaning"]
}

# Few-shot ì˜ˆì‹œ ì¶”ì¶œìš© ìŠ¤í‚¤ë§ˆ
GEMINI_FEWSHOT_EXTRACTION_SCHEMA = {
    "type": "object",
    "properties": {
        "is_good_example": {
            "type": "boolean",
            "description": "ì´ ìˆ˜ì • ì‚¬ë¡€ê°€ Few-shot ì˜ˆì‹œë¡œ ì í•©í•œì§€"
        },
        "example_quality_score": {
            "type": "number",
            "description": "ì˜ˆì‹œ í’ˆì§ˆ ì ìˆ˜ (0.0~1.0)"
        },
        "condensed_original": {
            "type": "string",
            "description": "ì••ì¶•ëœ ì›ë³¸ ì¶œë ¥ (ìµœëŒ€ 200ì)"
        },
        "condensed_corrected": {
            "type": "string",
            "description": "ì••ì¶•ëœ ìˆ˜ì •ë³¸ (ìµœëŒ€ 200ì)"
        },
        "key_difference": {
            "type": "string",
            "description": "í•µì‹¬ ì°¨ì´ì  ìš”ì•½"
        }
    },
    "required": ["is_good_example", "example_quality_score"]
}

# AWS í´ë¼ì´ì–¸íŠ¸ (ê³µí†µ ëª¨ë“ˆ ì‚¬ìš© ì‹œ ìºì‹œëœ í´ë¼ì´ì–¸íŠ¸ í™œìš©)
if _USE_COMMON:
    dynamodb = get_dynamodb_resource()
    s3_client = get_s3_client()
    bedrock_client = get_bedrock_client()
else:
    dynamodb = boto3.resource("dynamodb")
    s3_client = boto3.client("s3")
    bedrock_config = Config(
        retries={"max_attempts": 2, "mode": "standard"},
        read_timeout=30,
        connect_timeout=5,
    )
    bedrock_client = boto3.client(
        "bedrock-runtime",
        region_name=BEDROCK_REGION,
        config=bedrock_config
    )

instructions_table = dynamodb.Table(DISTILLED_INSTRUCTIONS_TABLE)

# Gemini í´ë¼ì´ì–¸íŠ¸ (Lazy Initialization)
_gemini_client = None

# Context Cache ì €ì¥ì†Œ (workflow_id#node_id -> cache_name)
_context_cache_registry: Dict[str, Dict[str, Any]] = {}

# ì¶©ëŒ ê°ì§€ ì„œë¹„ìŠ¤ (Lazy Initialization)
_conflict_service: Optional[InstructionConflictService] = None


def _get_conflict_service() -> Optional[InstructionConflictService]:
    """InstructionConflictService ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _conflict_service
    if _conflict_service is None and _USE_CONFLICT_SERVICE:
        try:
            _conflict_service = InstructionConflictService(
                dynamodb_resource=dynamodb,
                instructions_table_name=DISTILLED_INSTRUCTIONS_TABLE,
                enable_semantic_validation=USE_GEMINI_NATIVE  # Gemini ì‚¬ìš© ì‹œ ì˜ë¯¸ì  ê²€ì¦ í™œì„±í™”
            )
            logger.info("InstructionConflictService initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize InstructionConflictService: {e}")
    return _conflict_service


def _get_gemini_client():
    """Vertex AI GenerativeModel í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤"""
    global _gemini_client
    if _gemini_client is None and USE_GEMINI_NATIVE:
        try:
            import vertexai
            from vertexai import generative_models
            
            # Service Account JSONì´ í™˜ê²½ë³€ìˆ˜ë¡œ ì œê³µëœ ê²½ìš° ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            if GCP_SERVICE_ACCOUNT_KEY:
                import tempfile
                with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                    f.write(GCP_SERVICE_ACCOUNT_KEY)
                    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = f.name
            
            project_id = GCP_PROJECT_ID
            if not project_id:
                # Fallback: AWS Secrets Managerì—ì„œ ì¡°íšŒ (Lambda í™˜ê²½)
                project_id = _get_vertex_config_from_secrets()
            
            if project_id:
                vertexai.init(project=project_id, location=GCP_LOCATION)
                _gemini_client = generative_models
                logger.info(f"Vertex AI initialized for Instruction Distiller: {project_id}")
            else:
                logger.warning("GCP_PROJECT_ID not configured, falling back to Bedrock")
        except ImportError:
            logger.warning("google-cloud-aiplatform not installed, falling back to Bedrock")
        except Exception as e:
            logger.error(f"Failed to initialize Vertex AI: {e}")
    return _gemini_client


def _get_vertex_config_from_secrets() -> Optional[str]:
    """
    AWS Secrets Managerì—ì„œ Vertex AI ì„¤ì • ì¡°íšŒ
    
    Returns:
        GCP Project ID (Secretsì—ì„œ ì¡°íšŒ) ë˜ëŠ” None
    """
    try:
        secret_name = os.environ.get("VERTEX_SECRET_NAME", "backend-workflow-dev-vertex_ai_config")
        secrets_client = boto3.client("secretsmanager", region_name=BEDROCK_REGION)
        response = secrets_client.get_secret_value(SecretId=secret_name)
        secret = json.loads(response["SecretString"])
        
        # Service Account JSONë„ Secretsì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
        if secret.get("service_account_key"):
            import tempfile
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                f.write(json.dumps(secret["service_account_key"]))
                os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = f.name
        
        return secret.get("project_id", "")
    except Exception as e:
        logger.warning(f"Failed to get Vertex AI config from Secrets Manager: {e}")
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â‘  Context Caching ê¸°ë°˜ 'ì§€ì¹¨ ì¸ë±ìŠ¤' ìµœì í™”
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _get_or_create_instruction_cache(
    owner_id: str,
    workflow_id: str,
    node_id: str,
    instructions: List[str],
    few_shot_example: Optional[Dict[str, str]] = None
) -> Optional[str]:
    """
    ì§€ì¹¨ì„ Gemini Context Cacheì— ë“±ë¡í•˜ê³  cache_name ë°˜í™˜
    
    ë™ì¼ ì›Œí¬í”Œë¡œìš° ë°˜ë³µ ì‹¤í–‰ ì‹œ:
    - ë§¤ë²ˆ ì§€ì¹¨ì„ í…ìŠ¤íŠ¸ë¡œ ì£¼ì…í•˜ëŠ” ëŒ€ì‹  ìºì‹œëœ System Instruction ì‚¬ìš©
    - í† í° ë¹„ìš© ì ˆê° + 'ì»¤ë„ ë ˆë²¨ ê·œì¹™'ìœ¼ë¡œ ê°•ë ¥í•œ ì¸ì§€
    
    Returns:
        cache_name: ìºì‹œ ì°¸ì¡° ì´ë¦„ (ì‹¤íŒ¨ ì‹œ None)
    """
    if not ENABLE_CONTEXT_CACHING:
        return None
    
    gemini = _get_gemini_client()
    if not gemini:
        return None
    
    cache_key = f"{owner_id}#{workflow_id}#{node_id}"
    
    # ì´ë¯¸ ìºì‹œê°€ ìˆìœ¼ë©´ ë°˜í™˜
    if cache_key in _context_cache_registry:
        cached = _context_cache_registry[cache_key]
        # TTL ì²´í¬ (ë§Œë£Œë˜ì—ˆìœ¼ë©´ ì¬ìƒì„±)
        if datetime.now(timezone.utc).timestamp() < cached.get("expires_at", 0):
            return cached.get("cache_name")
    
    try:
        # System Instruction êµ¬ì„±
        system_instruction = _build_cached_system_instruction(instructions, few_shot_example)
        
        # ìºì‹œ ìƒì„± (Gemini 1.5+ Context Caching API)
        # Note: Vertex AI caching ëª¨ë“ˆ ì‚¬ìš©
        try:
            from vertexai import caching
            from vertexai.generative_models import Content, Part

            cache = caching.CachedContent.create(
                model_name=GEMINI_2_FLASH_MODEL,
                system_instruction=system_instruction,
                ttl=f"{CONTEXT_CACHE_TTL_SECONDS}s",
            )
            
            cache_name = cache.name
            
            # ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
            _context_cache_registry[cache_key] = {
                "cache_name": cache_name,
                "expires_at": datetime.now(timezone.utc).timestamp() + CONTEXT_CACHE_TTL_SECONDS,
                "instruction_hash": hashlib.md5(system_instruction.encode()).hexdigest(),
            }
            
            logger.info(f"Context cache created: {cache_name} for {cache_key}")
            return cache_name
            
        except ImportError:
            logger.debug("Gemini caching module not available")
            return None
        except Exception as e:
            logger.warning(f"Context cache creation failed: {e}")
            return None
            
    except Exception as e:
        logger.error(f"Error in context caching: {e}")
        return None


def _build_cached_system_instruction(
    instructions: List[str],
    few_shot_example: Optional[Dict[str, str]] = None
) -> str:
    """
    ìºì‹±ìš© System Instruction êµ¬ì„±
    
    ì§€ì¹¨ + Few-shot ì˜ˆì‹œë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ê²°í•©
    """
    parts = []
    
    # í•µì‹¬ ê·œì¹™ í—¤ë”
    parts.append("# ì‚¬ìš©ì ë§ì¶¤ ê·œì¹™ (ìµœìš°ì„  ì¤€ìˆ˜)")
    parts.append("")
    
    # ì§€ì¹¨ ëª©ë¡
    parts.append("<user_rules>")
    for i, inst in enumerate(instructions, 1):
        parts.append(f"  <rule priority=\"{i}\">{inst}</rule>")
    parts.append("</user_rules>")
    
    # Few-shot ì˜ˆì‹œ (ìˆëŠ” ê²½ìš°)
    if few_shot_example:
        parts.append("")
        parts.append("# ì°¸ì¡° ì˜ˆì‹œ (ì´ íŒ¨í„´ì„ ë”°ë¥´ì„¸ìš”)")
        parts.append("<example>")
        parts.append(f"  <bad>{few_shot_example.get('original', '')}</bad>")
        parts.append(f"  <good>{few_shot_example.get('corrected', '')}</good>")
        if few_shot_example.get('key_difference'):
            parts.append(f"  <explanation>{few_shot_example['key_difference']}</explanation>")
        parts.append("</example>")
    
    parts.append("")
    parts.append("ìœ„ ê·œì¹™ë“¤ì„ ëª¨ë“  ì‘ë‹µì—ì„œ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì‹­ì‹œì˜¤.")
    
    return "\n".join(parts)


def invalidate_instruction_cache(owner_id: str, workflow_id: str, node_id: str) -> None:
    """
    ì§€ì¹¨ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆì„ ë•Œ ìºì‹œ ë¬´íš¨í™”
    
    ìƒˆë¡œìš´ ì§€ì¹¨ì´ ì¶”ì¶œë˜ë©´ ê¸°ì¡´ ìºì‹œë¥¼ ì‚­ì œí•˜ì—¬
    ë‹¤ìŒ í˜¸ì¶œì—ì„œ ìƒˆ ìºì‹œê°€ ìƒì„±ë˜ë„ë¡ í•©ë‹ˆë‹¤.
    """
    cache_key = f"{owner_id}#{workflow_id}#{node_id}"
    
    if cache_key in _context_cache_registry:
        cached = _context_cache_registry.pop(cache_key)
        
        # Vertex AI ìºì‹œ ì‚­ì œ ì‹œë„
        try:
            from vertexai import caching
            cache = caching.CachedContent(cached_content_name=cached.get("cache_name", ""))
            cache.delete()
            logger.info(f"Context cache invalidated: {cache_key}")
        except Exception as e:
            logger.debug(f"Cache deletion skipped: {e}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â‘¡ Few-shot ì˜ˆì‹œ ìë™ ì¶”ì¶œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _extract_few_shot_example(
    original_output: str,
    corrected_output: str,
    node_id: str
) -> Optional[Dict[str, Any]]:
    """
    ìˆ˜ì • ì‚¬ë¡€ì—ì„œ Few-shot ì˜ˆì‹œ ì¶”ì¶œ
    
    'ë°± ë§ˆë”” ì§€ì¹¨ë³´ë‹¤ í•˜ë‚˜ì˜ ì‹¤ì œ ì˜ˆì‹œ'ê°€ ëª¨ë¸ ì´í–‰ë¥ ì„ ë¹„ì•½ì ìœ¼ë¡œ ë†’ì…ë‹ˆë‹¤.
    Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì‹œì˜ í’ˆì§ˆì„ í‰ê°€í•˜ê³ , í•µì‹¬ë§Œ ì••ì¶•í•©ë‹ˆë‹¤.
    
    Returns:
        {
            "original": "ì••ì¶•ëœ ì›ë³¸",
            "corrected": "ì••ì¶•ëœ ìˆ˜ì •ë³¸",
            "key_difference": "í•µì‹¬ ì°¨ì´ì ",
            "quality_score": 0.0~1.0
        }
    """
    gemini = _get_gemini_client()
    if not gemini:
        # Fallback: ë‹¨ìˆœ ì˜ë¼ë‚´ê¸°
        return {
            "original": original_output[:FEW_SHOT_EXAMPLE_MAX_LENGTH],
            "corrected": corrected_output[:FEW_SHOT_EXAMPLE_MAX_LENGTH],
            "key_difference": "",
            "quality_score": 0.5
        }
    
    try:
        prompt = f"""ë‹¤ìŒ AI ì¶œë ¥ê³¼ ì‚¬ìš©ì ìˆ˜ì •ë³¸ì„ ë¶„ì„í•˜ì—¬ Few-shot í•™ìŠµ ì˜ˆì‹œë¡œ ì í•©í•œì§€ í‰ê°€í•˜ì„¸ìš”.

## ì›ë³¸ AI ì¶œë ¥:
```
{original_output[:1500]}
```

## ì‚¬ìš©ì ìˆ˜ì •ë³¸:
```
{corrected_output[:1500]}
```

## í‰ê°€ ê¸°ì¤€:
1. ìˆ˜ì • ì‚¬í•­ì´ ëª…í™•í•˜ê³  ì¼ê´€ëœê°€?
2. ì˜ˆì‹œë¡œ ì‚¬ìš©í–ˆì„ ë•Œ ë‹¤ë¥¸ ìƒí™©ì—ë„ ì ìš© ê°€ëŠ¥í•œê°€?
3. í•µì‹¬ ì°¨ì´ì ì´ ë¬´ì—‡ì¸ê°€?

ì í•©í•˜ë‹¤ë©´ is_good_example=true, ì›ë³¸ê³¼ ìˆ˜ì •ë³¸ì„ ê°ê° 200ì ì´ë‚´ë¡œ ì••ì¶•í•˜ì„¸ìš”."""
        
        model = gemini.GenerativeModel(
            model_name=GEMINI_FLASH_MODEL,
            generation_config={
                "response_mime_type": "application/json",
                "response_schema": GEMINI_FEWSHOT_EXTRACTION_SCHEMA,
                "max_output_tokens": 500,
                "temperature": 0.2,
            }
        )
        
        response = model.generate_content(prompt)
        result = json.loads(response.text)
        
        if result.get("is_good_example", False):
            return {
                "original": result.get("condensed_original", original_output[:200]),
                "corrected": result.get("condensed_corrected", corrected_output[:200]),
                "key_difference": result.get("key_difference", ""),
                "quality_score": result.get("example_quality_score", 0.5)
            }
        
        logger.info(f"Example not suitable for few-shot: quality={result.get('example_quality_score', 0)}")
        return None
        
    except Exception as e:
        logger.warning(f"Few-shot extraction failed: {e}")
        return None


def _get_best_few_shot_example(pk: str, node_id: str) -> Optional[Dict[str, str]]:
    """
    ì €ì¥ëœ Few-shot ì˜ˆì‹œ ì¤‘ ê°€ì¥ í’ˆì§ˆì´ ë†’ì€ ê²ƒ ì¡°íšŒ
    """
    try:
        response = instructions_table.get_item(
            Key={"pk": pk, "sk": f"LATEST#{node_id}"}
        )
        
        if "Item" not in response:
            return None
        
        latest_sk = response["Item"].get("latest_instruction_sk")
        if not latest_sk:
            return None
        
        response = instructions_table.get_item(
            Key={"pk": pk, "sk": latest_sk}
        )
        
        if "Item" in response:
            return response["Item"].get("few_shot_example")
            
    except Exception as e:
        logger.warning(f"Failed to get few-shot example: {e}")
    
    return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â‘¢ ì§€ëŠ¥í˜• ì§€ì¹¨ ì••ì¶• (Instruction Compression)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _compress_instructions(
    instructions: List[str],
    target_count: int = COMPRESSED_INSTRUCTION_COUNT
) -> List[str]:
    """
    ì§€ì¹¨ë“¤ì„ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì••ì¶•
    
    10ê°œ ì´ìƒì˜ ì§€ì¹¨ì€ í”„ë¡¬í”„íŠ¸ë¥¼ ì§€ì €ë¶„í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
    Geminiì—ê²Œ 'ê°€ì¥ ì§§ê³  ê°•ë ¥í•œ 3ê°œì˜ ë¬¸ì¥'ìœ¼ë¡œ ì••ì¶•í•˜ë„ë¡ ì§€ì‹œí•©ë‹ˆë‹¤.
    
    Args:
        instructions: ì›ë³¸ ì§€ì¹¨ ëª©ë¡
        target_count: ëª©í‘œ ì§€ì¹¨ ìˆ˜ (ê¸°ë³¸ 3ê°œ)
    
    Returns:
        ì••ì¶•ëœ ì§€ì¹¨ ëª©ë¡
    """
    if len(instructions) <= target_count:
        return instructions
    
    gemini = _get_gemini_client()
    if not gemini:
        # Fallback: ì•ì—ì„œë¶€í„° target_countê°œë§Œ ì‚¬ìš©
        return instructions[:target_count]
    
    try:
        prompt = f"""ë‹¤ìŒ {len(instructions)}ê°œì˜ ì§€ì¹¨ì„ {target_count}ê°œì˜ í•µì‹¬ ë¬¸ì¥ìœ¼ë¡œ ì••ì¶•í•˜ì„¸ìš”.

## ì›ë³¸ ì§€ì¹¨:
{json.dumps(instructions, ensure_ascii=False, indent=2)}

## ì••ì¶• ê·œì¹™:
1. ëª¨ë“  ì§€ì¹¨ì˜ í•µì‹¬ ì˜ë¯¸ë¥¼ ë³´ì¡´í•  ê²ƒ
2. ìƒì¶©ë˜ëŠ” ì§€ì¹¨ì€ ë” ì¤‘ìš”í•œ ê²ƒì„ ìš°ì„ í•  ê²ƒ
3. ìœ ì‚¬í•œ ì§€ì¹¨ì€ í•˜ë‚˜ë¡œ í†µí•©í•  ê²ƒ
4. ê° ë¬¸ì¥ì€ ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•´ì•¼ í•¨
5. ê²°ê³¼ëŠ” ì •í™•íˆ {target_count}ê°œ ì´í•˜ì˜ ë¬¸ì¥

ê°€ì¥ ì§§ê³  ê°•ë ¥í•œ {target_count}ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ ì••ì¶•í•˜ì„¸ìš”."""
        
        model = gemini.GenerativeModel(
            model_name=GEMINI_FLASH_MODEL,
            generation_config={
                "response_mime_type": "application/json",
                "response_schema": GEMINI_COMPRESSION_SCHEMA,
                "max_output_tokens": 400,
                "temperature": 0.2,
            }
        )
        
        response = model.generate_content(prompt)
        result = json.loads(response.text)
        
        compressed = result.get("compressed_instructions", [])
        preserved = result.get("preserved_meaning", True)
        ratio = result.get("compression_ratio", 0.3)
        
        if compressed and preserved:
            logger.info(f"Instructions compressed: {len(instructions)} â†’ {len(compressed)} (ratio={ratio:.2f})")
            return compressed[:target_count]
        
        logger.warning("Compression did not preserve meaning, using original")
        return instructions[:target_count]
        
    except Exception as e:
        logger.warning(f"Instruction compression failed: {e}")
        return instructions[:target_count]


@log_execution_context
def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    HITL ìŠ¹ì¸ ì´ë²¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ì—¬ ì§€ì¹¨ì„ ì¦ë¥˜í•©ë‹ˆë‹¤.
    
    ì´ë²¤íŠ¸ êµ¬ì¡°:
    {
        "detail-type": "HITL Approval Completed",
        "detail": {
            "execution_id": "exec-123",
            "node_id": "generate_email",
            "workflow_id": "wf-456",
            "owner_id": "user-789",
            "original_output_ref": "s3://bucket/original.json",
            "corrected_output_ref": "s3://bucket/corrected.json",
            "approval_timestamp": "2026-01-04T12:00:00Z"
        }
    }
    """
    try:
        logger.info(f"Received event: {json.dumps(event, default=str)[:500]}")
        
        detail = event.get("detail", {})
        
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        required_fields = ["execution_id", "node_id", "original_output_ref", "corrected_output_ref"]
        for field in required_fields:
            if field not in detail:
                logger.warning(f"Missing required field: {field}")
                return {"statusCode": 400, "body": f"Missing: {field}"}
        
        execution_id = detail["execution_id"]
        node_id = detail["node_id"]
        workflow_id = detail.get("workflow_id", "unknown")
        owner_id = detail.get("owner_id", "unknown")
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ì´ë²¤íŠ¸: ì¦ë¥˜ ì‹œì‘
        log_business_event(
            "instruction_distillation_started",
            workflow_id=workflow_id,
            node_id=node_id,
            owner_id=owner_id,
            execution_id=execution_id
        )
        
        # S3ì—ì„œ ì›ë³¸ ë° ìˆ˜ì •ë³¸ ë¡œë“œ
        original_output = _load_from_s3_ref(detail["original_output_ref"])
        corrected_output = _load_from_s3_ref(detail["corrected_output_ref"])
        
        if not original_output or not corrected_output:
            logger.warning("Failed to load outputs from S3")
            return {"statusCode": 400, "body": "Failed to load outputs"}
        
        # ê¸°ì¡´ ì§€ì¹¨ ì¡°íšŒ (í†µí•©ì„ ìœ„í•´)
        existing_instructions = get_active_instructions(owner_id, workflow_id, node_id)
        
        # ì°¨ì´ì  ë¶„ì„ ë° ì§€ì¹¨ ì¶”ì¶œ (Gemini Native ë˜ëŠ” Fallback)
        distillation_result = _distill_instructions(
            original_output=original_output,
            corrected_output=corrected_output,
            node_id=node_id,
            workflow_id=workflow_id,
            owner_id=owner_id,
            existing_instructions=existing_instructions
        )
        
        distilled_instructions = distillation_result.get("instructions", [])
        semantic_diff_score = distillation_result.get("semantic_diff_score", 0.5)
        is_typo_only = distillation_result.get("is_typo_only", False)
        
        # ë‹¨ìˆœ ì˜¤íƒ€ ìˆ˜ì •ì´ë©´ ì§€ì¹¨ ì¶”ì¶œ ìŠ¤í‚µ
        if is_typo_only:
            logger.info(f"Typo-only correction detected for {node_id}, skipping distillation")
            return {"statusCode": 200, "body": "Typo-only correction, no instructions distilled"}
        
        # â‘¡ Few-shot ì˜ˆì‹œ ì¶”ì¶œ (ë†’ì€ í’ˆì§ˆì˜ ìˆ˜ì • ì‚¬ë¡€ë§Œ)
        few_shot_example = None
        if distilled_instructions and semantic_diff_score >= 0.3:
            few_shot_example = _extract_few_shot_example(
                original_output=original_output,
                corrected_output=corrected_output,
                node_id=node_id
            )
            if few_shot_example:
                logger.info(f"Few-shot example extracted (quality={few_shot_example.get('quality_score', 0):.2f})")
        
        if distilled_instructions:
            # DynamoDBì— ì €ì¥ (ë™ì  ê°€ì¤‘ì¹˜ + Few-shot ì˜ˆì‹œ)
            _save_distilled_instructions(
                workflow_id=workflow_id,
                node_id=node_id,
                owner_id=owner_id,
                instructions=distilled_instructions,
                execution_id=execution_id,
                semantic_diff_score=semantic_diff_score,
                few_shot_example=few_shot_example
            )
            
            # â‘  Context Cache ë¬´íš¨í™” (ìƒˆ ì§€ì¹¨ ë°˜ì˜ì„ ìœ„í•´)
            invalidate_instruction_cache(owner_id, workflow_id, node_id)
            
            logger.info(f"Distilled {len(distilled_instructions)} instructions for {node_id} (diff_score={semantic_diff_score:.2f})")
            
            # ë¹„ì¦ˆë‹ˆìŠ¤ ì´ë²¤íŠ¸: ì¦ë¥˜ ì„±ê³µ
            log_business_event(
                "instruction_distillation_completed",
                workflow_id=workflow_id,
                node_id=node_id,
                owner_id=owner_id,
                distilled_count=len(distilled_instructions),
                semantic_diff_score=semantic_diff_score,
                has_few_shot=few_shot_example is not None
            )
            
            # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ê¸°ë¡ (Powertools ì‚¬ìš© ì‹œ)
            if metrics:
                metrics.add_metric(name="instructions_distilled_count", unit="Count", value=len(distilled_instructions))
                if few_shot_example:
                    metrics.add_metric(name="few_shot_examples_extracted", unit="Count", value=1)
            
            return {
                "statusCode": 200,
                "body": json.dumps({
                    "distilled_count": len(distilled_instructions),
                    "instructions": distilled_instructions,
                    "semantic_diff_score": semantic_diff_score,
                    "reasoning": distillation_result.get("reasoning", ""),
                    "has_few_shot": few_shot_example is not None
                })
            }
        
        return {"statusCode": 200, "body": "No significant differences found"}
        
    except Exception as e:
        logger.error(f"Error in instruction distillation: {e}", exc_info=True)
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ì´ë²¤íŠ¸: ì¦ë¥˜ ì‹¤íŒ¨
        log_business_event(
            "instruction_distillation_failed",
            error_type=type(e).__name__,
            error_message=str(e)
        )
        
        return {"statusCode": 500, "body": str(e)}


@log_external_service_call("s3", "get_object")
def _load_from_s3_ref(s3_ref: str) -> Optional[str]:
    """
    S3 ì°¸ì¡°ì—ì„œ ì½˜í…ì¸  ë¡œë“œ
    s3://bucket/key í˜•ì‹ ì§€ì›
    """
    try:
        if s3_ref.startswith("s3://"):
            parts = s3_ref[5:].split("/", 1)
            bucket = parts[0]
            key = parts[1] if len(parts) > 1 else ""
        else:
            bucket = S3_BUCKET
            key = s3_ref
        
        response = s3_client.get_object(Bucket=bucket, Key=key)
        content = response["Body"].read().decode("utf-8")
        
        # JSONì¸ ê²½ìš° íŒŒì‹± í›„ ë¬¸ìì—´ë¡œ ë³€í™˜
        try:
            data = json.loads(content)
            if isinstance(data, dict):
                # output í•„ë“œê°€ ìˆìœ¼ë©´ ì¶”ì¶œ
                return data.get("output", json.dumps(data, ensure_ascii=False))
            return str(data)
        except json.JSONDecodeError:
            return content
            
    except Exception as e:
        logger.error(f"Failed to load from S3: {s3_ref}, error: {e}")
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í•µì‹¬ ì¦ë¥˜ í•¨ìˆ˜ (Gemini Native + Fallback)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _distill_instructions(
    original_output: str,
    corrected_output: str,
    node_id: str,
    workflow_id: str,
    owner_id: str,
    existing_instructions: Optional[List[str]] = None
) -> Dict[str, Any]:
    """
    Gemini Nativeë¥¼ ì‚¬ìš©í•œ ì„¸ë§Œí‹± ì¦ë¥˜ (Semantic Distillation)
    
    ì‚¬ìš©ìì˜ ìˆ˜ì • í–‰ìœ„ì—ì„œ ë°˜ë³µ ì ìš© ê°€ëŠ¥í•œ ìŠ¤íƒ€ì¼ ê°€ì´ë“œì™€ ê¸ˆì§€ ê·œì¹™ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    Geminiì˜ Structured Outputìœ¼ë¡œ íŒŒì‹± ì—ëŸ¬ 0%ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.
    
    Args:
        original_output: ì›ë³¸ AI ì¶œë ¥
        corrected_output: ì‚¬ìš©ì ìˆ˜ì •ë³¸
        node_id: ë…¸ë“œ ID
        workflow_id: ì›Œí¬í”Œë¡œìš° ID  
        owner_id: ì†Œìœ ì ID
        existing_instructions: ê¸°ì¡´ í™œì„± ì§€ì¹¨ (í†µí•©ìš©)
    
    Returns:
        {
            "instructions": ["ì§€ì¹¨1", "ì§€ì¹¨2", ...],
            "semantic_diff_score": 0.0~1.0,
            "reasoning": "ë¶„ì„ ê·¼ê±°",
            "is_typo_only": bool
        }
    """
    gemini = _get_gemini_client()
    
    if gemini:
        result = _distill_with_gemini(original_output, corrected_output, node_id, workflow_id, existing_instructions)
        if result:
            return result
        logger.warning("Gemini distillation failed, falling back to Bedrock")
    
    # Fallback: Bedrock Haiku
    return _distill_with_bedrock(original_output, corrected_output, node_id, workflow_id, existing_instructions)


@log_external_service_call("gemini", "distill_instructions")
def _distill_with_gemini(
    original_output: str,
    corrected_output: str,
    node_id: str,
    workflow_id: str,
    existing_instructions: Optional[List[str]] = None
) -> Optional[Dict[str, Any]]:
    """
    Gemini 1.5/2.0 Flashë¥¼ ì‚¬ìš©í•œ ì„¸ë§Œí‹± ì¦ë¥˜
    
    íŠ¹ì§•:
    - Structured Outputìœ¼ë¡œ JSON íŒŒì‹± ì—ëŸ¬ ì œê±°
    - ì˜ë„ ì¶”ë¡  (ì™œ ì´ë ‡ê²Œ ê³ ì³¤ëŠ”ê°€)
    - ì˜¤íƒ€ vs ì˜ë„ì  ë³€ê²½ êµ¬ë¶„
    - ì˜ë¯¸ì  ì°¨ì´ ì ìˆ˜ë¡œ ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì ˆ
    """
    gemini = _get_gemini_client()
    if not gemini:
        return None
    
    # ì„¸ë§Œí‹± ì¦ë¥˜ ì‹œìŠ¤í…œ ì§€ì¹¨
    system_instruction = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìˆ˜ì • íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ AI ì‘ë‹µ ê°œì„  ì§€ì¹¨ì„ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## í•µì‹¬ ì›ì¹™
1. ì‚¬ìš©ìì˜ ìˆ˜ì • í–‰ìœ„ëŠ” AIì˜ 'ì‹¤ìˆ˜'ë¥¼ êµì •í•˜ëŠ” í–‰ìœ„ì…ë‹ˆë‹¤.
2. ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ì—ì„œ **ë°˜ë³µì ìœ¼ë¡œ ì ìš© ê°€ëŠ¥í•œ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ**ì™€ **ê¸ˆì§€ ê·œì¹™**ì„ ì„¸ë§Œí‹±í•˜ê²Œ ì¶”ì¶œí•˜ì‹­ì‹œì˜¤.
3. íŠ¹ì • ê³ ìœ  ëª…ì‚¬(ì¸ëª…, íšŒì‚¬ëª…, ë‚ ì§œ ë“±)ëŠ” ì¶”ìƒí™”ëœ ì†ì„±ìœ¼ë¡œ ì¹˜í™˜í•˜ì‹­ì‹œì˜¤.
   - ì˜ˆ: "í™ê¸¸ë™ì—ê²Œ ë©”ì¼ ì¨ì¤˜" â†’ "ìˆ˜ì‹ ìì˜ ì´ë¦„ì„ ë³¸ë¬¸ì— í¬í•¨í•  ê²ƒ"
   - ì˜ˆ: "2026ë…„ 1ì›” 15ì¼" â†’ "êµ¬ì²´ì ì¸ ë‚ ì§œë¥¼ ëª…ì‹œí•  ê²ƒ"
4. ë‹¨ìˆœ ì˜¤íƒ€ ìˆ˜ì •ê³¼ ì˜ë„ì  ìŠ¤íƒ€ì¼ ë³€ê²½ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì‹­ì‹œì˜¤.

## ì§€ì¹¨ ì¹´í…Œê³ ë¦¬
- style: ë¬¸ì²´, ì–´ì¡°, ë§íˆ¬ ê´€ë ¨
- content: ë‚´ìš© ì¶”ê°€/ì‚­ì œ/ìˆ˜ì • ê´€ë ¨
- format: í˜•ì‹, êµ¬ì¡°, ë ˆì´ì•„ì›ƒ ê´€ë ¨
- tone: ê²©ì‹, ì¹œê·¼í•¨, ì „ë¬¸ì„± ì •ë„
- prohibition: ê¸ˆì§€ ì‚¬í•­, í”¼í•´ì•¼ í•  í‘œí˜„

## ì¶œë ¥ ìš”êµ¬ì‚¬í•­
- instructions: ìµœëŒ€ 10ê°œì˜ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì§€ì¹¨
- semantic_diff_score: ì›ë³¸ê³¼ ìˆ˜ì •ë³¸ì˜ ì˜ë¯¸ì  ì°¨ì´ (0.0=ê±°ì˜ ë™ì¼, 1.0=ì™„ì „íˆ ë‹¤ë¦„)
- reasoning: ì™œ ì´ëŸ¬í•œ ì§€ì¹¨ì„ ì¶”ì¶œí–ˆëŠ”ì§€ ë¶„ì„ ê·¼ê±°
- is_typo_only: ë³€ê²½ì´ ì˜¤íƒ€ ìˆ˜ì •ë¿ì´ë©´ true"""
    
    # ê¸°ì¡´ ì§€ì¹¨ì´ ìˆìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨
    existing_context = ""
    if existing_instructions:
        existing_context = f"\n\n## ê¸°ì¡´ í™œì„± ì§€ì¹¨ (ì°¸ê³ ìš©, ì¤‘ë³µ í”¼í•˜ê³  ë³´ì™„í•  ê²ƒ):\n" + "\n".join([f"- {inst}" for inst in existing_instructions])
    
    user_prompt = f"""## ë¶„ì„ ëŒ€ìƒ
- ë…¸ë“œ: {node_id}
- ì›Œí¬í”Œë¡œìš°: {workflow_id}

## ì›ë³¸ AI ì¶œë ¥:
```
{original_output[:3000]}
```

## ì‚¬ìš©ì ìˆ˜ì •ë³¸:
```
{corrected_output[:3000]}
```
{existing_context}

ìœ„ ë‘ í…ìŠ¤íŠ¸ë¥¼ ë¹„êµ ë¶„ì„í•˜ì—¬ í–¥í›„ AI ì‘ë‹µ ê°œì„ ì„ ìœ„í•œ ì§€ì¹¨ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”."""
    
    try:
        # Gemini ëª¨ë¸ ì„ íƒ (2.0 Flash ìš°ì„ , ì—†ìœ¼ë©´ 1.5 Flash)
        model_name = GEMINI_2_FLASH_MODEL
        try:
            model = gemini.GenerativeModel(
                model_name=model_name,
                system_instruction=system_instruction,
                generation_config={
                    "response_mime_type": "application/json",
                    "response_schema": GEMINI_DISTILLATION_SCHEMA,
                    "max_output_tokens": 1000,
                    "temperature": 0.3,  # ì¼ê´€ëœ ì¶”ì¶œì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„
                }
            )
        except Exception:
            # Fallback to 1.5 Flash
            model_name = GEMINI_FLASH_MODEL
            model = gemini.GenerativeModel(
                model_name=model_name,
                system_instruction=system_instruction,
                generation_config={
                    "response_mime_type": "application/json",
                    "response_schema": GEMINI_DISTILLATION_SCHEMA,
                    "max_output_tokens": 1000,
                    "temperature": 0.3,
                }
            )
        
        response = model.generate_content(user_prompt)
        
        # Structured Output ì§ì ‘ íŒŒì‹± (ì—ëŸ¬ ê°€ëŠ¥ì„± 0%)
        result = json.loads(response.text)
        
        # instructions ë°°ì—´ì—ì„œ textë§Œ ì¶”ì¶œ
        instructions = []
        for inst in result.get("instructions", []):
            if isinstance(inst, dict):
                text = inst.get("text", "").strip()
                if text and len(text) > 5:
                    instructions.append(text)
            elif isinstance(inst, str) and len(inst.strip()) > 5:
                instructions.append(inst.strip())
        
        # ê¸°ì¡´ ì§€ì¹¨ê³¼ í†µí•© (ì¤‘ë³µ ì œê±°)
        if existing_instructions:
            instructions = _consolidate_instructions_native(existing_instructions, instructions, node_id)
        
        logger.info(f"Gemini distillation successful: {len(instructions)} instructions, model={model_name}")
        
        return {
            "instructions": instructions[:MAX_INSTRUCTIONS_PER_NODE],
            "semantic_diff_score": result.get("semantic_diff_score", 0.5),
            "reasoning": result.get("reasoning", ""),
            "is_typo_only": result.get("is_typo_only", False)
        }
        
    except Exception as e:
        logger.error(f"Gemini distillation error: {e}")
        return None


def _distill_with_bedrock(
    original_output: str,
    corrected_output: str,
    node_id: str,
    workflow_id: str,
    existing_instructions: Optional[List[str]] = None
) -> Dict[str, Any]:
    """
    Bedrock Haiku Fallback ì¦ë¥˜
    
    Gemini ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ Haiku ê¸°ë°˜ ë¡œì§ ì‚¬ìš©
    """
    # ê¸°ì¡´ _extract_new_instructions í˜¸ì¶œ
    new_instructions = _extract_new_instructions(original_output, corrected_output, node_id, workflow_id)
    
    if not new_instructions:
        return {
            "instructions": [],
            "semantic_diff_score": 0.0,
            "reasoning": "No significant differences found",
            "is_typo_only": True
        }
    
    # ê¸°ì¡´ ì§€ì¹¨ê³¼ í†µí•©
    if existing_instructions:
        consolidated = _consolidate_instructions(existing_instructions, new_instructions, node_id)
    else:
        consolidated = new_instructions
    
    return {
        "instructions": consolidated[:MAX_INSTRUCTIONS_PER_NODE],
        "semantic_diff_score": 0.5,  # HaikuëŠ” ì ìˆ˜ ì œê³µ ì•ˆ í•¨, ê¸°ë³¸ê°’
        "reasoning": "Extracted via Bedrock Haiku fallback",
        "is_typo_only": False
    }


def _consolidate_instructions_native(
    existing: List[str],
    new_instructions: List[str],
    node_id: str
) -> List[str]:
    """
    Geminië¥¼ ì‚¬ìš©í•œ ì§€ëŠ¥í˜• ì§€ì¹¨ í†µí•©
    
    ë‹¨ìˆœ ì¤‘ë³µ ì œê±°ê°€ ì•„ë‹Œ ì˜ë¯¸ì  í†µí•© ìˆ˜í–‰
    """
    gemini = _get_gemini_client()
    if not gemini or not existing or not new_instructions:
        # Fallback: ë‹¨ìˆœ í•©ì§‘í•©
        combined = list(set(existing + new_instructions))
        return combined[:MAX_INSTRUCTIONS_PER_NODE]
    
    try:
        prompt = f"""ê¸°ì¡´ ì§€ì¹¨ê³¼ ì‹ ê·œ ì§€ì¹¨ì„ í†µí•©í•˜ì—¬ ìµœì í™”ëœ ì§€ì¹¨ ëª©ë¡ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

## ê¸°ì¡´ ì§€ì¹¨:
{json.dumps(existing, ensure_ascii=False)}

## ì‹ ê·œ ì§€ì¹¨:
{json.dumps(new_instructions, ensure_ascii=False)}

## ê·œì¹™:
1. ìƒì¶©ë˜ëŠ” ë‚´ìš©ì€ ì‹ ê·œ ì§€ì¹¨ ìš°ì„ 
2. ì˜ë¯¸ì ìœ¼ë¡œ ë™ì¼í•œ ì§€ì¹¨ì€ í•˜ë‚˜ë¡œ í†µí•©
3. ìµœëŒ€ {MAX_INSTRUCTIONS_PER_NODE}ê°œ ì´ë‚´

í†µí•©ëœ ì§€ì¹¨ì„ JSON ë¬¸ìì—´ ë°°ì—´ë¡œ ì¶œë ¥í•˜ì„¸ìš”."""
        
        model = gemini.GenerativeModel(
            model_name=GEMINI_FLASH_MODEL,
            generation_config={
                "response_mime_type": "application/json",
                "response_schema": {
                    "type": "array",
                    "items": {"type": "string"}
                },
                "max_output_tokens": 500,
                "temperature": 0.2,
            }
        )
        
        response = model.generate_content(prompt)
        result = json.loads(response.text)
        
        if isinstance(result, list):
            return [inst.strip() for inst in result if isinstance(inst, str) and len(inst.strip()) > 5][:MAX_INSTRUCTIONS_PER_NODE]
    
    except Exception as e:
        logger.warning(f"Native consolidation failed: {e}")
    
    # Fallback
    return list(set(existing + new_instructions))[:MAX_INSTRUCTIONS_PER_NODE]


def _check_instruction_conflicts(
    instructions: List[str]
) -> Tuple[bool, List[str]]:
    """
    ì§€ì¹¨ ê°„ ì¶©ëŒ ê²€ì‚¬ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
    
    ì„œë¡œ ìƒì¶©ë˜ëŠ” ì§€ì¹¨ì´ ìˆëŠ”ì§€ ê²€ì‚¬í•˜ê³  í•´ê²°í•©ë‹ˆë‹¤.
    ì˜ˆ: "ê²©ì‹ì²´ ì‚¬ìš©" vs "ì¹œê·¼í•œ ë§íˆ¬ ì‚¬ìš©"
    
    Returns:
        (has_conflicts, resolved_instructions)
    """
    if len(instructions) < 2:
        return False, instructions
    
    gemini = _get_gemini_client()
    if not gemini:
        return False, instructions  # ê²€ì‚¬ ë¶ˆê°€, ê·¸ëŒ€ë¡œ ë°˜í™˜
    
    try:
        prompt = f"""ë‹¤ìŒ ì§€ì¹¨ ëª©ë¡ì—ì„œ ì„œë¡œ ìƒì¶©ë˜ëŠ” ì§€ì¹¨ì´ ìˆëŠ”ì§€ ê²€ì‚¬í•˜ê³  í•´ê²°í•´ì£¼ì„¸ìš”.

## ì§€ì¹¨ ëª©ë¡:
{json.dumps(instructions, ensure_ascii=False)}

ì¶©ëŒì´ ìˆìœ¼ë©´ has_conflicts=true, ì—†ìœ¼ë©´ falseë¡œ ì‘ë‹µí•˜ì„¸ìš”.
ì¶©ëŒì´ ìˆëŠ” ê²½ìš° resolved_instructionsì— í•´ê²°ëœ ì§€ì¹¨ ëª©ë¡ì„ ì œê³µí•˜ì„¸ìš”."""
        
        model = gemini.GenerativeModel(
            model_name=GEMINI_FLASH_MODEL,
            generation_config={
                "response_mime_type": "application/json",
                "response_schema": GEMINI_CONFLICT_CHECK_SCHEMA,
                "max_output_tokens": 600,
                "temperature": 0.1,
            }
        )
        
        response = model.generate_content(prompt)
        result = json.loads(response.text)
        
        has_conflicts = result.get("has_conflicts", False)
        resolved = result.get("resolved_instructions", instructions)
        
        if has_conflicts:
            logger.info(f"Instruction conflicts detected and resolved: {len(result.get('conflicts', []))} conflicts")
        
        return has_conflicts, resolved
        
    except Exception as e:
        logger.warning(f"Conflict check failed: {e}")
        return False, instructions


@log_external_service_call("bedrock", "extract_instructions")
def _extract_new_instructions(
    original_output: str,
    corrected_output: str,
    node_id: str,
    workflow_id: str
) -> List[str]:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ê³¼ ìˆ˜ì •ë³¸ì˜ ì°¨ì´ì—ì„œ ì‹ ê·œ ì§€ì¹¨ ì¶”ì¶œ
    """
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìˆ˜ì • íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ í–¥í›„ AI ì‘ë‹µ ê°œì„ ì„ ìœ„í•œ ì§€ì¹¨ì„ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ì›ë³¸ AI ì¶œë ¥:
{original_output[:2000]}

## ì‚¬ìš©ì ìˆ˜ì •ë³¸:
{corrected_output[:2000]}

## ë¶„ì„ ëŒ€ìƒ ë…¸ë“œ: {node_id}
## ì›Œí¬í”Œë¡œìš°: {workflow_id}

ìœ„ ë‘ í…ìŠ¤íŠ¸ë¥¼ ë¹„êµí•˜ì—¬ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ìŠ¤íƒ€ì¼/ë‚´ìš©ì˜ ì°¨ì´ì ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
ê° ì§€ì¹¨ì€ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤.

ì¤‘ìš” ë³´ì•ˆ ì§€ì¹¨:
- íŠ¹ì • ì¸ëª…, ì£¼ì†Œ, ì—°ë½ì²˜ ë“± ê°œì¸ì •ë³´(PII)ëŠ” ì¼ë°˜ì ì¸ ê·œì¹™ìœ¼ë¡œ ì¶”ìƒí™”í•˜ì‹­ì‹œì˜¤.
- ì˜ˆ: "í™ê¸¸ë™ì—ê²Œ ë©”ì¼ ì¨ì¤˜" â†’ "ìˆ˜ì‹ ìì˜ ì´ë¦„ì„ ë³¸ë¬¸ì— í¬í•¨í•  ê²ƒ"

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ì„¸ìš” (ì„¤ëª… ì—†ì´):
["ì§€ì¹¨1", "ì§€ì¹¨2", "ì§€ì¹¨3"]

ì˜ˆì‹œ:
["ì •ì¤‘í•˜ê³  ê²©ì‹ì²´ë¥¼ ì‚¬ìš©í•  ê²ƒ", "êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë‚ ì§œë¥¼ í¬í•¨í•  ê²ƒ", "ì¸ì‚¬ë§ì„ ë¨¼ì € ì‘ì„±í•  ê²ƒ"]

JSON ë°°ì—´:"""
    
    try:
        payload = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 500,
            "messages": [
                {"role": "user", "content": prompt}
            ]
        }
        
        response = bedrock_client.invoke_model(
            body=json.dumps(payload),
            modelId=HAIKU_MODEL_ID
        )
        
        result = json.loads(response["body"].read())
        if "content" in result and result["content"]:
            response_text = result["content"][0].get("text", "").strip()
            
            # JSON ë°°ì—´ íŒŒì‹±
            try:
                # ```json ... ``` í˜•ì‹ ì²˜ë¦¬
                if "```" in response_text:
                    json_match = response_text.split("```")[1]
                    if json_match.startswith("json"):
                        json_match = json_match[4:]
                    response_text = json_match.strip()
                
                instructions = json.loads(response_text)
                if isinstance(instructions, list):
                    # ìœ íš¨í•œ ë¬¸ìì—´ë§Œ í•„í„°ë§
                    return [
                        inst.strip() for inst in instructions 
                        if isinstance(inst, str) and len(inst.strip()) > 5
                    ][:MAX_INSTRUCTIONS_PER_NODE]
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse instructions JSON: {response_text[:200]}")
                
    except ClientError as e:
        logger.error(f"Bedrock invocation failed: {e}")
    except Exception as e:
        logger.error(f"Unexpected error in distillation: {e}")
    
    return []


def _consolidate_instructions(
    existing_instructions: List[str],
    new_raw_instructions: List[str],
    node_id: str
) -> List[str]:
    """
    ê¸°ì¡´ ì§€ì¹¨ê³¼ ì‹ ê·œ ì§€ì¹¨ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ í†µí•© ì§€ì¹¨ ìƒì„±
    """
    if not new_raw_instructions:
        return existing_instructions
    
    if not existing_instructions:
        return new_raw_instructions
    
    prompt = f"""ë‹¹ì‹ ì€ AI ê°€ì´ë“œë¼ì¸ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ê¸°ì¡´ ì§€ì¹¨ê³¼ ì‚¬ìš©ìì˜ ìƒˆë¡œìš´ ìš”êµ¬ì‚¬í•­ì„ í†µí•©í•˜ì—¬ í•˜ë‚˜ì˜ ì •êµí•œ ì§€ì¹¨ ëª©ë¡ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

## ê¸°ì¡´ ì§€ì¹¨:
{json.dumps(existing_instructions, ensure_ascii=False)}

## ì‹ ê·œ ì¶”ê°€ ì‚¬í•­:
{json.dumps(new_raw_instructions, ensure_ascii=False)}

## ì œì•½ ì‚¬í•­:
1. ì„œë¡œ ìƒì¶©ë˜ëŠ” ë‚´ìš©ì´ ìˆë‹¤ë©´ 'ì‹ ê·œ ì¶”ê°€ ì‚¬í•­'ì„ ìš°ì„ í•˜ì‹­ì‹œì˜¤.
2. íŠ¹ì • ì¸ëª…, ì£¼ì†Œ, ì—°ë½ì²˜ ë“± ê°œì¸ì •ë³´(PII)ëŠ” ì¼ë°˜ì ì¸ ê·œì¹™ìœ¼ë¡œ ì¶”ìƒí™”í•˜ì‹­ì‹œì˜¤.
3. ìµœëŒ€ {MAX_INSTRUCTIONS_PER_NODE}ê°œ ì´ë‚´ì˜ ë¶ˆë ›í¬ì¸íŠ¸ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
4. ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON ë¬¸ìì—´ ë°°ì—´ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

í†µí•© ì§€ì¹¨:"""
    
    try:
        payload = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 600,
            "messages": [
                {"role": "user", "content": prompt}
            ]
        }
        
        response = bedrock_client.invoke_model(
            body=json.dumps(payload),
            modelId=HAIKU_MODEL_ID
        )
        
        result = json.loads(response["body"].read())
        if "content" in result and result["content"]:
            response_text = result["content"][0].get("text", "").strip()
            
            # JSON ë°°ì—´ íŒŒì‹±
            try:
                # ```json ... ``` í˜•ì‹ ì²˜ë¦¬
                if "```" in response_text:
                    json_match = response_text.split("```")[1]
                    if json_match.startswith("json"):
                        json_match = json_match[4:]
                    response_text = json_match.strip()
                
                instructions = json.loads(response_text)
                if isinstance(instructions, list):
                    # ìœ íš¨í•œ ë¬¸ìì—´ë§Œ í•„í„°ë§
                    return [
                        inst.strip() for inst in instructions 
                        if isinstance(inst, str) and len(inst.strip()) > 5
                    ][:MAX_INSTRUCTIONS_PER_NODE]
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse consolidated instructions JSON: {response_text[:200]}")
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ + ì‹ ê·œ ë‹¨ìˆœ ê²°í•©
                combined = existing_instructions + new_raw_instructions
                return list(set(combined))[:MAX_INSTRUCTIONS_PER_NODE]
                
    except ClientError as e:
        logger.error(f"Bedrock invocation failed in consolidation: {e}")
    except Exception as e:
        logger.error(f"Unexpected error in consolidation: {e}")
    
    # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ì§€ì¹¨ ìœ ì§€
    return existing_instructions


@log_external_service_call("dynamodb", "save_instructions")
def _save_distilled_instructions(
    workflow_id: str,
    node_id: str,
    owner_id: str,
    instructions: List[str],
    execution_id: str,
    semantic_diff_score: float = 0.5,
    few_shot_example: Optional[Dict[str, Any]] = None
) -> None:
    """
    ì¦ë¥˜ëœ ì§€ì¹¨ì„ DynamoDBì— ì €ì¥ (ë™ì  ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ ì ìš©)
    
    Geminiê°€ íŒë‹¨í•œ semantic_diff_scoreì— ë”°ë¼ ê°€ì¤‘ì¹˜ ê°ì‡ ëŸ‰ì„ ë™ì  ì¡°ì ˆí•©ë‹ˆë‹¤:
    - ë†’ì€ diff_score (ì˜ë¯¸ ë³€í™” í¼) â†’ ê°•í•œ ê°ì‡  (ê¸°ì¡´ ì§€ì¹¨ ì‹ ë¢°ë„ í•˜ë½)
    - ë‚®ì€ diff_score (ë¯¸ì„¸ ì¡°ì •) â†’ ì•½í•œ ê°ì‡ 
    
    Args:
        semantic_diff_score: 0.0~1.0 (Geminiê°€ ì‚°ì¶œí•œ ì˜ë¯¸ì  ì°¨ì´ ì ìˆ˜)
        few_shot_example: Few-shot ì˜ˆì‹œ (â‘¡ ê¸°ëŠ¥)
    """
    now = datetime.now(timezone.utc)
    timestamp = now.strftime("%Y%m%d%H%M%S")
    
    pk = f"{owner_id}#{workflow_id}"
    sk = f"{node_id}#{timestamp}"
    
    # ë™ì  ê°€ì¤‘ì¹˜ ê°ì‡ ëŸ‰ ê³„ì‚°: diff_scoreê°€ ë†’ì„ìˆ˜ë¡ ê°•í•œ ê°ì‡ 
    # 0.0 â†’ 0.1 ê°ì‡ , 0.5 â†’ 0.3 ê°ì‡ , 1.0 â†’ 0.5 ê°ì‡ 
    dynamic_decay = Decimal(str(round(0.1 + (semantic_diff_score * 0.4), 2)))
    
    try:
        # ê¸°ì¡´ ì§€ì¹¨ ì¡°íšŒ ë° ê°€ì¤‘ì¹˜ decay ì ìš©
        existing_weighted_instructions = _get_weighted_instructions(pk, node_id)
        
        # ê¸°ì¡´ Few-shot ì˜ˆì‹œ ì¡°íšŒ (ë” ì¢‹ì€ ê²ƒìœ¼ë¡œ êµì²´ ì—¬ë¶€ ê²°ì •)
        existing_few_shot = _get_best_few_shot_example(pk, node_id)
        
        # ë™ì  Decay ì ìš©: semantic_diff_score ê¸°ë°˜
        decayed_instructions = []
        for inst in existing_weighted_instructions:
            current_weight = Decimal(str(inst["weight"]))
            new_weight = current_weight - dynamic_decay
            if new_weight >= MIN_INSTRUCTION_WEIGHT:
                inst["weight"] = new_weight
                inst["last_decay_reason"] = f"semantic_diff={semantic_diff_score:.2f}"
                decayed_instructions.append(inst)
            else:
                logger.info(f"Instruction dropped below threshold: {inst.get('text', '')[:50]}...")
            # MIN_INSTRUCTION_WEIGHT ì´í•˜ì´ë©´ ì œê±° (is_active = False)
        
        # í†µí•© ì§€ì¹¨ì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜
        structured_instructions = []
        for inst_text in instructions:
            structured_instructions.append({
                "text": inst_text,
                "weight": DEFAULT_INSTRUCTION_WEIGHT,
                "created_at": now.isoformat(),
                "last_used": now.isoformat(),
                "usage_count": 0,
                "is_active": True
            })
        
        # ìƒˆ ì§€ì¹¨ ì €ì¥ (decayëœ ê¸°ì¡´ + ìƒˆ í†µí•©)
        final_instructions = decayed_instructions + structured_instructions
        
        # â‘¡ Few-shot ì˜ˆì‹œ ê²°ì •: í’ˆì§ˆì´ ë” ë†’ì€ ê²ƒ ì„ íƒ
        final_few_shot = None
        if few_shot_example:
            new_quality = few_shot_example.get("quality_score", 0)
            existing_quality = existing_few_shot.get("quality_score", 0) if existing_few_shot else 0
            
            if new_quality > existing_quality:
                final_few_shot = few_shot_example
                logger.info(f"Few-shot example updated: quality {existing_quality:.2f} â†’ {new_quality:.2f}")
            else:
                final_few_shot = existing_few_shot
        elif existing_few_shot:
            final_few_shot = existing_few_shot
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # [ì‹ ê·œ] ì¶©ëŒ ê°ì§€ ì„œë¹„ìŠ¤ í†µí•©
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        conflict_service = _get_conflict_service()
        conflict_resolved = False
        
        if conflict_service and final_instructions:
            try:
                # ê° ìƒˆ ì§€ì¹¨ì— ëŒ€í•´ ì¶©ëŒ ê²€ì‚¬ ìˆ˜í–‰
                for struct_inst in structured_instructions:
                    inst_text = struct_inst.get("text", "")
                    metadata_signature = {
                        "workflow_id": workflow_id,
                        "node_id": node_id,
                        "owner_id": owner_id
                    }
                    
                    # ì¶©ëŒ ê°ì§€ ë° ìë™ í•´ê²° ì‹œë„
                    result = conflict_service.create_instruction_with_conflict_check(
                        user_id=owner_id,
                        instruction_text=inst_text,
                        metadata_signature=metadata_signature,
                        category=None,  # ì¹´í…Œê³ ë¦¬ëŠ” ì„ íƒì 
                        context_scope=f"workflow#{workflow_id}#node#{node_id}",
                        source_correction_ids=[execution_id]
                    )
                    
                    if result and hasattr(result, 'requires_manual_review') and result.requires_manual_review:
                        logger.warning(
                            f"Instruction conflict requires manual review: {inst_text[:50]}..."
                        )
                    elif result:
                        conflict_resolved = True
                        logger.info(f"Instruction conflict resolved automatically: {inst_text[:50]}...")
                        
            except Exception as e:
                logger.error(f"Conflict service error (non-fatal): {e}")
                # ì¶©ëŒ ì„œë¹„ìŠ¤ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ - ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ ê³„ì† ì§„í–‰
        
        item = {
            "pk": pk,
            "sk": sk,
            "node_id": node_id,
            "workflow_id": workflow_id,
            "owner_id": owner_id,
            "instructions": [inst["text"] for inst in final_instructions],  # ë ˆê±°ì‹œ í˜¸í™˜
            "weighted_instructions": _convert_to_dynamodb_format(final_instructions),
            "source_execution_id": execution_id,
            "created_at": now.isoformat(),
            "is_active": True,
            "version": 1,
            "usage_count": 0,
            "conflict_checked": conflict_service is not None,  # ì¶©ëŒ ê²€ì‚¬ ìˆ˜í–‰ ì—¬ë¶€
            "conflict_resolved": conflict_resolved,  # ì¶©ëŒ ìë™ í•´ê²° ì—¬ë¶€
        }
        
        # â‘¡ Few-shot ì˜ˆì‹œ ì €ì¥
        if final_few_shot:
            item["few_shot_example"] = final_few_shot
        
        instructions_table.put_item(Item=item)
        
        # í•´ë‹¹ ë…¸ë“œì˜ ìµœì‹  í™œì„± ì§€ì¹¨ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        _update_latest_instruction_index(pk, node_id, sk)
        
        logger.info(f"Saved distilled instructions with weights: pk={pk}, sk={sk}, count={len(final_instructions)}, has_few_shot={final_few_shot is not None}")
        
    except ClientError as e:
        logger.error(f"DynamoDB error saving instructions: {e}")
        raise


def _merge_and_deduplicate_instructions(
    existing: List[dict], 
    new_instructions: List[str]
) -> List[str]:
    """
    ê¸°ì¡´ ì§€ì¹¨ê³¼ ìƒˆ ì§€ì¹¨ì„ ë³‘í•©í•˜ê³  ì˜ë¯¸ì  ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.
    """
    existing_texts = {inst.get("text", "").strip().lower() for inst in existing}
    
    merged = [inst.get("text", "") for inst in existing]
    
    for new_inst in new_instructions:
        normalized = new_inst.strip().lower()
        # ê°„ë‹¨í•œ ì¤‘ë³µ ê²€ì‚¬ (ì •í™• ì¼ì¹˜)
        if normalized not in existing_texts:
            merged.append(new_inst)
            existing_texts.add(normalized)
    
    return merged


def _deduplicate_weighted_instructions(instructions: List[dict]) -> List[dict]:
    """ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì§€ì¹¨ ì¤‘ë³µ ì œê±° (ë†’ì€ ê°€ì¤‘ì¹˜ ìš°ì„ )"""
    seen = {}
    
    for inst in instructions:
        text = inst.get("text", "").strip().lower()
        if text not in seen:
            seen[text] = inst
        elif inst.get("weight", 0) > seen[text].get("weight", 0):
            seen[text] = inst
    
    # ê°€ì¤‘ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬
    return sorted(seen.values(), key=lambda x: x.get("weight", 0), reverse=True)


def _convert_to_dynamodb_format(instructions: List[dict]) -> List[dict]:
    """DynamoDB ì €ì¥ì„ ìœ„í•´ Decimal ë³€í™˜"""
    result = []
    for inst in instructions:
        item = dict(inst)
        if "weight" in item:
            item["weight"] = Decimal(str(item["weight"]))
        result.append(item)
    return result



def _get_weighted_instructions(pk: str, node_id: str) -> List[Dict[str, Any]]:
    """
    íŠ¹ì • ë…¸ë“œì˜ ìµœì‹  ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì§€ì¹¨ ëª©ë¡ ì¡°íšŒ
    (ë‚´ë¶€ ë¡œì§ìš© - Decay ê³„ì‚° ì‹œ ì‚¬ìš©)
    """
    try:
        # ìµœì‹  ì§€ì¹¨ ì¸ë±ìŠ¤ ì¡°íšŒ
        response = instructions_table.get_item(
            Key={"pk": pk, "sk": f"LATEST#{node_id}"}
        )
        
        if "Item" not in response:
            return []
        
        latest_sk = response["Item"].get("latest_instruction_sk")
        if not latest_sk:
            return []
        
        # ì‹¤ì œ ì§€ì¹¨ ì¡°íšŒ
        response = instructions_table.get_item(
            Key={"pk": pk, "sk": latest_sk}
        )
        
        if "Item" in response:
            return response["Item"].get("weighted_instructions", [])
            
    except ClientError as e:
        logger.warning(f"Failed to get weighted instructions: {e}")
    
    return []

def _update_latest_instruction_index(pk: str, node_id: str, latest_sk: str) -> None:
    """
    ë…¸ë“œë³„ ìµœì‹  ì§€ì¹¨ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
    """
    try:
        instructions_table.put_item(
            Item={
                "pk": pk,
                "sk": f"LATEST#{node_id}",
                "latest_instruction_sk": latest_sk,
                "updated_at": datetime.now(timezone.utc).isoformat(),
            }
        )
    except ClientError as e:
        logger.warning(f"Failed to update latest index: {e}")


def get_active_instructions(
    owner_id: str,
    workflow_id: str,
    node_id: str
) -> List[str]:
    """
    íŠ¹ì • ë…¸ë“œì— ëŒ€í•œ í™œì„± ì§€ì¹¨ ì¡°íšŒ (ì™¸ë¶€ í˜¸ì¶œìš©)
    ê°€ì¤‘ì¹˜ê°€ MIN_INSTRUCTION_WEIGHT ì´ìƒì¸ ì§€ì¹¨ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        í™œì„±í™”ëœ ì§€ì¹¨ ëª©ë¡ (ê°€ì¤‘ì¹˜ ìˆœ)
    """
    pk = f"{owner_id}#{workflow_id}"
    
    try:
        # ìµœì‹  ì§€ì¹¨ ì¸ë±ìŠ¤ ì¡°íšŒ
        response = instructions_table.get_item(
            Key={"pk": pk, "sk": f"LATEST#{node_id}"}
        )
        
        if "Item" not in response:
            return []
        
        latest_sk = response["Item"].get("latest_instruction_sk")
        if not latest_sk:
            return []
        
        # ì‹¤ì œ ì§€ì¹¨ ì¡°íšŒ
        response = instructions_table.get_item(
            Key={"pk": pk, "sk": latest_sk}
        )
        
        if "Item" in response and response["Item"].get("is_active"):
            # ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•„í„°ë§
            weighted = response["Item"].get("weighted_instructions", [])
            
            if weighted:
                # ê°€ì¤‘ì¹˜ê°€ ì¶©ë¶„í•œ ì§€ì¹¨ë§Œ ì„ íƒ
                valid_instructions = [
                    inst["text"] for inst in weighted
                    if Decimal(str(inst.get("weight", 0))) >= MIN_INSTRUCTION_WEIGHT
                ]
                
                # ì‚¬ìš© íšŸìˆ˜ ì¦ê°€ (ë¹„ë™ê¸°)
                _increment_usage_count(pk, latest_sk)
                
                return valid_instructions
            
            # ë ˆê±°ì‹œ í˜•ì‹ í˜¸í™˜
            instructions = response["Item"].get("instructions", [])
            _increment_usage_count(pk, latest_sk)
            return instructions
            
    except ClientError as e:
        logger.error(f"Error getting active instructions: {e}")
    
    return []


def _increment_usage_count(pk: str, sk: str) -> None:
    """ì‚¬ìš© íšŸìˆ˜ ì¦ê°€ (ë¹„ë™ê¸°, ì‹¤íŒ¨ ë¬´ì‹œ)"""
    try:
        instructions_table.update_item(
            Key={"pk": pk, "sk": sk},
            UpdateExpression="SET usage_count = usage_count + :inc, total_applications = total_applications + :inc",
            ExpressionAttributeValues={":inc": 1}
        )
    except Exception:
        pass


def record_instruction_feedback(
    owner_id: str,
    workflow_id: str,
    node_id: str,
    is_positive: bool,
    instruction_text: Optional[str] = None
) -> None:
    """
    ì§€ì¹¨ ì ìš© ê²°ê³¼ì— ëŒ€í•œ í”¼ë“œë°±ì„ ê¸°ë¡í•˜ê³  ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.
    
    - is_positive=True: ì§€ì¹¨ì´ íš¨ê³¼ì ì´ì—ˆìŒ (ê°€ì¤‘ì¹˜ ìœ ì§€ ë˜ëŠ” ì¦ê°€)
    - is_positive=False: ì‚¬ìš©ìê°€ ë‹¤ì‹œ ìˆ˜ì •í•¨ (ê°€ì¤‘ì¹˜ ê°ì†Œ)
    
    Args:
        owner_id: ì‚¬ìš©ì ID
        workflow_id: ì›Œí¬í”Œë¡œìš° ID
        node_id: ë…¸ë“œ ID
        is_positive: ê¸ì •ì  í”¼ë“œë°± ì—¬ë¶€
        instruction_text: íŠ¹ì • ì§€ì¹¨ í…ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ ì „ì²´ ì ìš©)
    """
    pk = f"{owner_id}#{workflow_id}"
    
    try:
        response = instructions_table.get_item(
            Key={"pk": pk, "sk": f"LATEST#{node_id}"}
        )
        
        if "Item" not in response:
            return
        
        latest_sk = response["Item"].get("latest_instruction_sk")
        if not latest_sk:
            return
        
        response = instructions_table.get_item(
            Key={"pk": pk, "sk": latest_sk}
        )
        
        if "Item" not in response:
            return
        
        item = response["Item"]
        weighted = item.get("weighted_instructions", [])
        
        if not weighted:
            return
        
        updated = False
        for inst in weighted:
            # íŠ¹ì • ì§€ì¹¨ë§Œ ì—…ë°ì´íŠ¸í•˜ê±°ë‚˜, ì „ì²´ ì—…ë°ì´íŠ¸
            if instruction_text and inst.get("text") != instruction_text:
                continue
            
            current_weight = Decimal(str(inst.get("weight", DEFAULT_INSTRUCTION_WEIGHT)))
            
            if is_positive:
                # ê¸ì •ì  í”¼ë“œë°±: ê°€ì¤‘ì¹˜ ì†Œí­ ì¦ê°€ (ìµœëŒ€ 1.5)
                new_weight = min(current_weight + Decimal("0.1"), Decimal("1.5"))
            else:
                # ë¶€ì •ì  í”¼ë“œë°±: ê°€ì¤‘ì¹˜ ê°ì†Œ
                inst["rework_count"] = inst.get("rework_count", 0) + 1
                new_weight = max(current_weight - BASE_WEIGHT_DECAY, MIN_INSTRUCTION_WEIGHT)
            
            inst["weight"] = new_weight
            updated = True
        
        if updated:
            # ì„±ê³µë¥  ì—…ë°ì´íŠ¸
            total = item.get("total_applications", 1)
            current_success = item.get("success_rate", Decimal("0"))
            
            if is_positive:
                new_success_rate = ((current_success * (total - 1)) + 1) / total
            else:
                new_success_rate = (current_success * (total - 1)) / total
            
            instructions_table.update_item(
                Key={"pk": pk, "sk": latest_sk},
                UpdateExpression="SET weighted_instructions = :wi, success_rate = :sr",
                ExpressionAttributeValues={
                    ":wi": weighted,
                    ":sr": Decimal(str(round(new_success_rate, 2)))
                }
            )
            
            logger.info(f"Updated instruction weights for {node_id}: positive={is_positive}")
            
    except ClientError as e:
        logger.error(f"Error recording instruction feedback: {e}")


def merge_instructions_into_prompt(
    base_prompt: str,
    owner_id: str,
    workflow_id: str,
    node_id: str,
    injection_strategy: str = "system",
    check_conflicts: bool = True,
    compress_instructions: bool = True,
    include_few_shot: bool = True,
    use_context_cache: bool = True
) -> str:
    """
    ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ì— ì¦ë¥˜ëœ ì§€ì¹¨ì„ êµ¬ì¡°í™”í•˜ì—¬ ë³‘í•©
    
    Gemini Native Extra-Mile ìµœì í™”:
    - â‘  Context Caching: ë™ì¼ ì›Œí¬í”Œë¡œìš° ë°˜ë³µ ì‹œ í† í° ë¹„ìš© ì ˆê°
    - â‘¡ Few-shot ì˜ˆì‹œ: ì‹¤ì œ ìˆ˜ì • ì‚¬ë¡€ë¥¼ <example> íƒœê·¸ë¡œ ì£¼ì…
    - â‘¢ ì§€ì¹¨ ì••ì¶•: 10ê°œ ì´ìƒì˜ ì§€ì¹¨ì„ 3ê°œë¡œ ì••ì¶•
    - ì§€ì¹¨ ì¶©ëŒ ê²€ì‚¬ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
    
    Args:
        injection_strategy: "system" (ê¶Œì¥), "prefix", "suffix"
        check_conflicts: ì§€ì¹¨ ê°„ ì¶©ëŒ ê²€ì‚¬ ì—¬ë¶€
        compress_instructions: ì§€ì¹¨ ì••ì¶• í™œì„±í™” (â‘¢ ê¸°ëŠ¥)
        include_few_shot: Few-shot ì˜ˆì‹œ í¬í•¨ (â‘¡ ê¸°ëŠ¥)
        use_context_cache: Context Caching ì‚¬ìš© (â‘  ê¸°ëŠ¥)
    
    Returns:
        ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ ë˜ëŠ” cache_name (Context Caching ì‚¬ìš© ì‹œ)
    """
    instructions = get_active_instructions(owner_id, workflow_id, node_id)
    
    if not instructions:
        return base_prompt
    
    pk = f"{owner_id}#{workflow_id}"
    
    # ì§€ì¹¨ ì¶©ëŒ ê²€ì‚¬ (ì„ íƒì )
    if check_conflicts and len(instructions) > 1:
        has_conflicts, resolved = _check_instruction_conflicts(instructions)
        if has_conflicts:
            instructions = resolved
            logger.info(f"Resolved instruction conflicts for {node_id}")
    
    # â‘¢ ì§€ëŠ¥í˜• ì§€ì¹¨ ì••ì¶• (10ê°œ ì´ìƒì´ë©´ 3ê°œë¡œ ì••ì¶•)
    if compress_instructions and len(instructions) > COMPRESSED_INSTRUCTION_COUNT:
        instructions = _compress_instructions(instructions, COMPRESSED_INSTRUCTION_COUNT)
        logger.info(f"Compressed instructions to {len(instructions)} for {node_id}")
    
    # â‘¡ Few-shot ì˜ˆì‹œ ì¡°íšŒ
    few_shot_example = None
    if include_few_shot:
        few_shot_example = _get_best_few_shot_example(pk, node_id)
    
    # â‘  Context Caching ì‹œë„ (ìºì‹œ ê°€ëŠ¥í•œ ê²½ìš°)
    if use_context_cache and ENABLE_CONTEXT_CACHING:
        cache_name = _get_or_create_instruction_cache(
            owner_id=owner_id,
            workflow_id=workflow_id,
            node_id=node_id,
            instructions=instructions,
            few_shot_example=few_shot_example
        )
        if cache_name:
            # ìºì‹œê°€ ìˆìœ¼ë©´ System Instructionì€ ìºì‹œì— ìˆìœ¼ë¯€ë¡œ base_promptë§Œ ë°˜í™˜
            # í˜¸ì¶œìëŠ” cache_nameì„ ì‚¬ìš©í•˜ì—¬ Gemini í˜¸ì¶œ ì‹œ fromCache íŒŒë¼ë¯¸í„° ì‚¬ìš©
            logger.info(f"Using cached instructions: {cache_name}")
            # ë©”íƒ€ë°ì´í„°ë¡œ ìºì‹œ ì •ë³´ ì „ë‹¬ (ë§ˆì»¤ ì‚¬ìš©)
            return f"{{{{CONTEXT_CACHE:{cache_name}}}}}\n{base_prompt}"
    
    # ìºì‹œ ë¯¸ì‚¬ìš© ì‹œ ì§ì ‘ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    # XML íƒœê·¸ë¡œ ì§€ì¹¨ êµ¬ì¡°í™” (Gemini/Claude ìµœì í™”)
    instruction_block = "\n".join([f"  <rule>{inst}</rule>" for inst in instructions])
    
    # â‘¡ Few-shot ì˜ˆì‹œ ë¸”ë¡ êµ¬ì„±
    few_shot_block = ""
    if few_shot_example:
        few_shot_block = f"""
<example type="correction">
  <original>{few_shot_example.get('original', '')}</original>
  <corrected>{few_shot_example.get('corrected', '')}</corrected>
  <explanation>{few_shot_example.get('key_difference', 'ìœ„ì²˜ëŸ¼ ìˆ˜ì •í•˜ì„¸ìš”')}</explanation>
</example>
"""
    
    if injection_strategy == "system":
        enhanced_prompt = f"""<user_preferences>
{instruction_block}
</user_preferences>
{few_shot_block}
{base_prompt}

ìœ„ì˜ <user_preferences> ë‚´ì˜ ê·œì¹™ì„ ìµœìš°ì„ ìœ¼ë¡œ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€í•˜ì‹­ì‹œì˜¤.{' <example>ì˜ íŒ¨í„´ì„ ì°¸ê³ í•˜ì„¸ìš”.' if few_shot_example else ''}"""
    
    elif injection_strategy == "prefix":
        enhanced_prompt = f"""<user_preferences>
{instruction_block}
</user_preferences>
{few_shot_block}
---

{base_prompt}"""
    
    else:  # suffix (ê¸°ì¡´ ë°©ì‹, ë ˆê±°ì‹œ í˜¸í™˜)
        enhanced_prompt = f"""{base_prompt}

## ì‚¬ìš©ì ë§ì¶¤ ì§€ì¹¨ (ìë™ ì ìš©):
ë‹¤ìŒ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ë”°ë¼ì£¼ì„¸ìš”:
{instruction_block}
{few_shot_block}
"""
    
    return enhanced_prompt


def _format_instructions_block(instructions: List[str]) -> str:
    """
    ì§€ì¹¨ì„ êµ¬ì¡°í™”ëœ ë¸”ë¡ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
    XML íƒœê·¸ì™€ ë²ˆí˜¸ ë§¤ê¹€ì„ ì‚¬ìš©í•˜ì—¬ LLMì˜ ì´í–‰ë¥ ì„ ë†’ì…ë‹ˆë‹¤.
    """
    formatted_lines = []
    formatted_lines.append("ë‹¹ì‹ ì€ ë‹¤ìŒì˜ ì‚¬ìš©ì ë§ì¶¤ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:")
    formatted_lines.append("")
    
    for i, inst in enumerate(instructions, 1):
        formatted_lines.append(f"{i}. {inst}")
    
    formatted_lines.append("")
    formatted_lines.append("ìœ„ ì§€ì¹¨ì„ ëª¨ë“  ì‘ë‹µì— ì ìš©í•˜ì„¸ìš”.")
    
    return "\n".join(formatted_lines)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Thinking Level Control í†µí•© API
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_distillation_thinking_config(
    owner_id: str,
    workflow_id: str,
    node_id: str,
    user_request: str = "",
    current_workflow: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    ì§€ì¹¨ ì¦ë¥˜ ì‘ì—…ì„ ìœ„í•œ Thinking Level ì„¤ì •ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    ì§€ì¹¨ì˜ ìˆ˜, ì¶©ëŒ ì—¬ë¶€, ì›Œí¬í”Œë¡œìš° ë³µì¡ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
    ì ì ˆí•œ Thinking Budgetì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        owner_id: ì‚¬ìš©ì ID
        workflow_id: ì›Œí¬í”Œë¡œìš° ID
        node_id: ë…¸ë“œ ID
        user_request: ì‚¬ìš©ì ìš”ì²­
        current_workflow: í˜„ì¬ ì›Œí¬í”Œë¡œìš° ìƒíƒœ
    
    Returns:
        {
            "enable_thinking": bool,
            "thinking_budget_tokens": int,
            "thinking_level": str,
            "reasoning": str
        }
    """
    if not _USE_THINKING_CONTROL:
        return {
            "enable_thinking": False,
            "thinking_budget_tokens": 0,
            "thinking_level": "disabled",
            "reasoning": "Thinking control not available"
        }
    
    # ê¸°ì¡´ ì§€ì¹¨ ìˆ˜ í™•ì¸
    existing_instructions = get_active_instructions(owner_id, workflow_id, node_id)
    instruction_count = len(existing_instructions)
    
    # ì§€ì¹¨ì´ ë§ì„ìˆ˜ë¡ ë” ê¹Šì€ ì‚¬ê³  í•„ìš”
    if instruction_count >= 8:
        override_level = ThinkingLevel.DEEP
        reason = f"Many existing instructions ({instruction_count})"
    elif instruction_count >= 4:
        override_level = ThinkingLevel.STANDARD
        reason = f"Some existing instructions ({instruction_count})"
    else:
        override_level = None  # ìë™ ê³„ì‚°
        reason = None
    
    # Thinking Budget ê³„ì‚°
    budget, level, reasoning = calculate_thinking_budget(
        canvas_mode="co-design",  # ì§€ì¹¨ ì¦ë¥˜ëŠ” Co-design ìˆ˜ì¤€
        current_workflow=current_workflow or {},
        user_request=user_request,
        conflict_detected=instruction_count > 5,  # ì§€ì¹¨ ë§ìœ¼ë©´ ì¶©ëŒ ê°€ëŠ¥ì„±
        override_level=override_level
    )
    
    final_reasoning = f"{reason}: {reasoning}" if reason else reasoning
    
    return {
        "enable_thinking": True,
        "thinking_budget_tokens": budget,
        "thinking_level": level.value,
        "reasoning": final_reasoning,
        "instruction_count": instruction_count
    }
