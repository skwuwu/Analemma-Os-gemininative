"""
Co-design Assistant API Handler

ì–‘ë°©í–¥ í˜‘ì—… ì›Œí¬í”Œë¡œìš° ì„¤ê³„ë¥¼ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° API í•¸ë“¤ëŸ¬ì…ë‹ˆë‹¤.
Canvasê°€ ë¹„ì–´ìˆì„ ë•ŒëŠ” Agentic Designer ëª¨ë“œë¡œ ìë™ ì „í™˜ë©ë‹ˆë‹¤.

[v2.0 ê°œì„ ì‚¬í•­]
- Lambda Function URL RESPONSE_STREAM ëª¨ë“œ ì§€ì› (ì§„ì§œ ìŠ¤íŠ¸ë¦¬ë°)
- ì´ë²¤íŠ¸ ë£¨í”„ ìµœì í™” (ì „ì—­ ë£¨í”„ ì¬ì‚¬ìš©)
- UUID ê¸°ë°˜ ì„¸ì…˜ ID ìƒì„±
- Gemini API íŠ¹í™” ì—ëŸ¬ í•¸ë“¤ë§
"""

import json
import logging
import os
import asyncio
import uuid
from typing import Dict, Any, Optional, Generator, Callable
from functools import wraps

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš¨ [Critical Fix] Import ê²½ë¡œ ìˆ˜ì •
# ê¸°ì¡´: agentic_designer_handler (í•¨ìˆ˜ë“¤ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ)
# ìˆ˜ì •: ê° í•¨ìˆ˜/ìƒìˆ˜ê°€ ì‹¤ì œë¡œ ì •ì˜ëœ ëª¨ë“ˆì—ì„œ ì§ì ‘ import
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ì„œë¹„ìŠ¤ ì„í¬íŠ¸
try:
    from src.services.design.codesign_assistant import (
        stream_codesign_response,
        explain_workflow,
        generate_suggestions,
        apply_suggestion,
        get_or_create_context
    )
    # LLM í´ë¼ì´ì–¸íŠ¸ (bedrock_client.pyì—ì„œ import)
    from src.services.llm.bedrock_client import (
        invoke_bedrock_stream as invoke_bedrock_model_stream,
        MODEL_SONNET,
        is_mock_mode as _is_mock_mode,
        get_mock_workflow as _mock_workflow_json,
    )
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (prompts.pyì—ì„œ import)
    from src.services.design.prompts import SYSTEM_PROMPT
    # ëª¨ë¸ ë¼ìš°í„° ë° ì¸ì¦
    from src.common.model_router import get_model_for_canvas_mode
    from src.common.auth_utils import extract_owner_id_from_event
    _IMPORTS_OK = True
except ImportError as e:
    import logging
    logging.getLogger(__name__).warning(f"Import fallback activated: {e}")
    _IMPORTS_OK = False
    
    # Fallback imports (ê°œë³„ì ìœ¼ë¡œ ì‹œë„)
    try:
        from src.services.design.codesign_assistant import (
            stream_codesign_response,
            explain_workflow,
            generate_suggestions,
            apply_suggestion,
            get_or_create_context
        )
    except ImportError:
        stream_codesign_response = None
        explain_workflow = None
        generate_suggestions = None
        apply_suggestion = None
        get_or_create_context = None
    
    # LLM í´ë¼ì´ì–¸íŠ¸ fallback
    invoke_bedrock_model_stream = None
    MODEL_SONNET = "anthropic.claude-3-sonnet-20240229-v1:0"
    _is_mock_mode = lambda: os.getenv("MOCK_MODE", "false").lower() in {"true", "1", "yes", "on"}
    _mock_workflow_json = lambda: {"nodes": [], "edges": []}
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ fallback
    SYSTEM_PROMPT = "You are a workflow design assistant."
    
    try:
        from src.common.model_router import get_model_for_canvas_mode
    except ImportError:
        get_model_for_canvas_mode = None
    
    try:
        from src.common.auth_utils import extract_owner_id_from_event
    except ImportError:
        def extract_owner_id_from_event(*args, **kwargs):
            raise Exception("Unauthorized: auth_utils not available")

logger = logging.getLogger(__name__)
logger.setLevel(os.getenv("LOG_LEVEL", "INFO"))

# ============================================================================
# ì „ì—­ ì´ë²¤íŠ¸ ë£¨í”„ ê´€ë¦¬ (Lambda ì»¨í…Œì´ë„ˆ ì¬ì‚¬ìš© ìµœì í™”)
# ============================================================================
_global_loop: Optional[asyncio.AbstractEventLoop] = None


def get_or_create_event_loop() -> asyncio.AbstractEventLoop:
    """
    Lambda ì»¨í…Œì´ë„ˆ ì¬ì‚¬ìš©ì„ ìœ„í•œ ì „ì—­ ì´ë²¤íŠ¸ ë£¨í”„ ê´€ë¦¬.
    ë§¤ í˜¸ì¶œë§ˆë‹¤ new_event_loop()ë¥¼ ìƒì„±í•˜ëŠ” ì˜¤ë²„í—¤ë“œë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    """
    global _global_loop
    
    try:
        # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ë°˜í™˜
        loop = asyncio.get_running_loop()
        return loop
    except RuntimeError:
        pass
    
    # ì „ì—­ ë£¨í”„ê°€ ìˆê³  ìœ íš¨í•˜ë©´ ì¬ì‚¬ìš©
    if _global_loop is not None and not _global_loop.is_closed():
        return _global_loop
    
    # ìƒˆ ë£¨í”„ ìƒì„± ë° ì „ì—­ ì €ì¥
    _global_loop = asyncio.new_event_loop()
    asyncio.set_event_loop(_global_loop)
    return _global_loop


# ============================================================================
# ë¹„ì¦ˆë‹ˆìŠ¤ ì¹œí™”ì  ì—ëŸ¬ ë¶„ë¥˜ ë° ë³€í™˜
# ============================================================================
class APIErrorCode:
    """API ì—ëŸ¬ ì½”ë“œ ìƒìˆ˜"""
    SAFETY_FILTER_BLOCKED = "SAFETY_FILTER_BLOCKED"
    QUOTA_EXCEEDED = "QUOTA_EXCEEDED"
    MODEL_OVERLOADED = "MODEL_OVERLOADED"
    CONTEXT_TOO_LONG = "CONTEXT_TOO_LONG"
    INVALID_REQUEST = "INVALID_REQUEST"
    INTERNAL_ERROR = "INTERNAL_ERROR"
    STREAMING_INTERRUPTED = "STREAMING_INTERRUPTED"


def classify_and_format_error(error: Exception) -> Dict[str, Any]:
    """
    LLM API ì—ëŸ¬ë¥¼ ë¹„ì¦ˆë‹ˆìŠ¤ ì¹œí™”ì  ë©”ì‹œì§€ë¡œ ë³€í™˜.
    íˆ¬ìì/í•´ì»¤í†¤ ì‹¬ì‚¬ìœ„ì›ì´ ë³¼ ìˆ˜ ìˆëŠ” ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    error_str = str(error).lower()
    error_type = type(error).__name__
    
    # Gemini Safety Filter ì°¨ë‹¨
    if any(kw in error_str for kw in ["safety", "blocked", "harm", "dangerous"]):
        return {
            "error_code": APIErrorCode.SAFETY_FILTER_BLOCKED,
            "message": "ìš”ì²­ ë‚´ìš©ì´ ì•ˆì „ ì •ì±…ì— ì˜í•´ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "user_action": "ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "retryable": True,
            "status_code": 422
        }
    
    # Quota ì´ˆê³¼
    if any(kw in error_str for kw in ["quota", "rate limit", "too many requests", "429"]):
        return {
            "error_code": APIErrorCode.QUOTA_EXCEEDED,
            "message": "í˜„ì¬ ìš”ì²­ì´ ë§ì•„ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "user_action": "30ì´ˆ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "retryable": True,
            "retry_after_seconds": 30,
            "status_code": 429
        }
    
    # ëª¨ë¸ ê³¼ë¶€í•˜
    if any(kw in error_str for kw in ["overloaded", "capacity", "503", "service unavailable"]):
        return {
            "error_code": APIErrorCode.MODEL_OVERLOADED,
            "message": "AI ëª¨ë¸ì´ í˜„ì¬ ë°”ì©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "user_action": "1ë¶„ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "retryable": True,
            "retry_after_seconds": 60,
            "status_code": 503
        }
    
    # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì´ˆê³¼
    if any(kw in error_str for kw in ["context length", "token limit", "too long", "max tokens"]):
        return {
            "error_code": APIErrorCode.CONTEXT_TOO_LONG,
            "message": "ì›Œí¬í”Œë¡œìš°ê°€ ë„ˆë¬´ ë³µì¡í•©ë‹ˆë‹¤. ì¼ë¶€ ë…¸ë“œë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”.",
            "user_action": "ì›Œí¬í”Œë¡œìš°ë¥¼ ë‹¨ìˆœí™”í•˜ê±°ë‚˜ ë¶„í• í•´ì£¼ì„¸ìš”.",
            "retryable": False,
            "status_code": 413
        }
    
    # ì˜ëª»ëœ ìš”ì²­
    if any(kw in error_str for kw in ["invalid", "malformed", "bad request", "400"]):
        return {
            "error_code": APIErrorCode.INVALID_REQUEST,
            "message": "ìš”ì²­ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "user_action": "ì…ë ¥ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.",
            "retryable": False,
            "status_code": 400
        }
    
    # ê¸°íƒ€ ë‚´ë¶€ ì˜¤ë¥˜
    return {
        "error_code": APIErrorCode.INTERNAL_ERROR,
        "message": "ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "user_action": "ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.",
        "retryable": True,
        "status_code": 500,
        "debug_info": error_type if os.getenv("DEBUG_MODE") else None
    }


def _response(status_code: int, body: Any, headers: Dict = None) -> Dict:
    """Lambda proxy response ìƒì„±"""
    return {
        'statusCode': status_code,
        'headers': {
            'Content-Type': 'application/json',
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Headers': 'Content-Type,Authorization',
            'Access-Control-Allow-Methods': 'GET,POST,PUT,DELETE,OPTIONS',
            **(headers or {})
        },
        'body': json.dumps(body, ensure_ascii=False, default=str) if body else ''
    }


def _error_response(error: Exception) -> Dict:
    """
    ì—ëŸ¬ë¥¼ ë¹„ì¦ˆë‹ˆìŠ¤ ì¹œí™”ì  ì‘ë‹µìœ¼ë¡œ ë³€í™˜.
    Gemini API íŠ¹í™” ì—ëŸ¬ë¥¼ ì‚¬ìš©ìê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ë©”ì‹œì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    error_info = classify_and_format_error(error)
    return _response(
        error_info.get("status_code", 500),
        {
            "error": error_info["message"],
            "error_code": error_info["error_code"],
            "user_action": error_info.get("user_action"),
            "retryable": error_info.get("retryable", False),
            "retry_after_seconds": error_info.get("retry_after_seconds")
        }
    )


# ============================================================================
# Lambda Function URL RESPONSE_STREAM ì§€ì› (ì§„ì§œ ìŠ¤íŠ¸ë¦¬ë°)
# ============================================================================

def _is_streaming_invocation(event: Dict) -> bool:
    """
    Lambda Function URLì˜ RESPONSE_STREAM ëª¨ë“œì¸ì§€ í™•ì¸.
    InvokeMode: RESPONSE_STREAMì´ ì„¤ì •ëœ ê²½ìš° True ë°˜í™˜.
    """
    request_context = event.get('requestContext', {})
    # Function URLì˜ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ê°ì§€
    return (
        request_context.get('http', {}).get('method') is not None and  # Function URL í˜•ì‹
        os.getenv('LAMBDA_STREAMING_ENABLED', 'false').lower() == 'true'
    )


async def _stream_to_response_stream(
    generator: Generator[str, None, None],
    response_stream: Any
) -> None:
    """
    Generator ì¶œë ¥ì„ Lambda Response Streamì— ì§ì ‘ ì“°ê¸°.
    6MB í˜ì´ë¡œë“œ ì œí•œì„ ìš°íšŒí•˜ê³  ì§„ì§œ ìŠ¤íŠ¸ë¦¬ë°ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
    
    Args:
        generator: JSONL ì²­í¬ë¥¼ yieldí•˜ëŠ” ì œë„ˆë ˆì´í„°
        response_stream: awslambdaric.ResponseStream ê°ì²´
    """
    try:
        for chunk in generator:
            if chunk:
                # ì²­í¬ë¥¼ ì¦‰ì‹œ í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡
                response_stream.write(chunk.encode('utf-8'))
                # ê°•ì œ í”ŒëŸ¬ì‹œë¡œ ì§€ì—° ì—†ì´ ì „ì†¡
                if hasattr(response_stream, 'flush'):
                    response_stream.flush()
    except Exception as e:
        logger.error(f"Stream write error: {e}")
        error_chunk = json.dumps({
            "type": "error",
            "error_code": APIErrorCode.STREAMING_INTERRUPTED,
            "data": "ìŠ¤íŠ¸ë¦¬ë°ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        }) + "\n"
        response_stream.write(error_chunk.encode('utf-8'))
    finally:
        if hasattr(response_stream, 'close'):
            response_stream.close()


def _streaming_response(generator: Generator[str, None, None]) -> Dict:
    """
    ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (Fallback ëª¨ë“œ).
    
    [ì£¼ì˜] ì´ í•¨ìˆ˜ëŠ” Lambda Function URLì˜ RESPONSE_STREAM ëª¨ë“œê°€ 
    ë¹„í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì§„ì§œ ìŠ¤íŠ¸ë¦¬ë°ì´ ì•„ë‹Œ 
    ì²­í¬ ìˆ˜ì§‘ í›„ ì¼ê´„ ë°˜í™˜ ë°©ì‹ì…ë‹ˆë‹¤.
    
    í˜ì´ë¡œë“œ í¬ê¸° ì œí•œ:
    - AWS Lambda ì‘ë‹µ ì œí•œ: 6MB
    - 5MB ì´ˆê³¼ ì‹œ ê²½ê³  ë¡œê¹…
    """
    MAX_PAYLOAD_SIZE = 5 * 1024 * 1024  # 5MB ê²½ê³  ì„ê³„ê°’
    HARD_LIMIT = 6 * 1024 * 1024  # 6MB í•˜ë“œ ë¦¬ë°‹
    
    chunks = []
    total_size = 0
    truncated = False
    
    try:
        for chunk in generator:
            chunk_size = len(chunk.encode('utf-8'))
            
            # í˜ì´ë¡œë“œ í¬ê¸° ì²´í¬
            if total_size + chunk_size > HARD_LIMIT:
                logger.warning(f"Payload approaching 6MB limit, truncating response")
                truncated = True
                # ê²½ê³  ë©”ì‹œì§€ ì¶”ê°€
                chunks.append(json.dumps({
                    "type": "warning",
                    "data": "ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ì–´ ì¼ë¶€ê°€ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤."
                }) + "\n")
                break
            
            if total_size > MAX_PAYLOAD_SIZE:
                logger.warning(f"Payload exceeded 5MB: {total_size / 1024 / 1024:.2f}MB")
            
            chunks.append(chunk)
            total_size += chunk_size
            
    except Exception as e:
        logger.error(f"Streaming error: {e}")
        error_info = classify_and_format_error(e)
        chunks.append(json.dumps({
            "type": "error",
            "error_code": error_info["error_code"],
            "data": error_info["message"],
            "user_action": error_info.get("user_action")
        }) + "\n")
    
    response_body = ''.join(chunks)
    
    return {
        'statusCode': 200,
        'headers': {
            'Content-Type': 'application/x-ndjson',
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Headers': 'Content-Type,Authorization',
            'Access-Control-Allow-Methods': 'GET,POST,PUT,DELETE,OPTIONS',
            'X-Payload-Truncated': 'true' if truncated else 'false',
            'X-Payload-Size': str(total_size),
        },
        'body': response_body
    }


def _parse_body(event: Dict) -> tuple[Optional[Dict], Optional[str]]:
    """ìš”ì²­ ë°”ë”” íŒŒì‹±"""
    raw_body = event.get('body')
    if not raw_body:
        return {}, None
    
    try:
        parsed = json.loads(raw_body)
        if not isinstance(parsed, dict):
            return None, 'Request body must be a JSON object'
        return parsed, None
    except json.JSONDecodeError as e:
        return None, f'Invalid JSON: {str(e)}'


def _get_query_param(event: Dict, param: str, default: Any = None) -> Any:
    """ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì•ˆì „ ì¶”ì¶œ"""
    query_params = event.get('queryStringParameters') or {}
    return query_params.get(param, default)


def _generate_session_id(owner_id: str) -> str:
    """
    UUID ê¸°ë°˜ ì„¸ì…˜ ID ìƒì„±.
    
    ê¸°ì¡´ asyncio.get_event_loop().time() ë°©ì‹ì˜ ë¬¸ì œì :
    - ë‹¨ì¡° ì‹œê°„(Monotonic time)ìœ¼ë¡œ ì‹œìŠ¤í…œ ì¬ë¶€íŒ… ì‹œ ì¤‘ë³µ ê°€ëŠ¥
    - Lambda ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘ ì‹œ ì¶©ëŒ ìœ„í—˜
    
    UUID4ëŠ” 122ë¹„íŠ¸ ëœë¤ìœ¼ë¡œ ì¶©ëŒ í™•ë¥ ì´ ì‚¬ì‹¤ìƒ 0ì…ë‹ˆë‹¤.
    """
    return f"session_{owner_id[:8]}_{uuid.uuid4().hex[:16]}"


async def lambda_handler(event, context):
    """Co-design Assistant API Lambda í•¸ë“¤ëŸ¬"""
    
    # CORS preflight ì²˜ë¦¬
    http_method = event.get('httpMethod') or event.get('requestContext', {}).get('http', {}).get('method')
    if http_method == 'OPTIONS':
        return _response(200, None)
    
    # ì¸ì¦ í™•ì¸
    owner_id = extract_owner_id_from_event(event)
    if not owner_id:
        return _response(401, {'error': 'Authentication required'})
    
    # ê²½ë¡œ ë° ë©”ì†Œë“œ ì¶”ì¶œ
    path = event.get('path') or event.get('rawPath', '')
    
    logger.info(f"Co-design API: {http_method} {path} (owner={owner_id[:8]}...)")
    
    try:
        # ë¼ìš°íŒ…
        if path.endswith('/codesign') or path.endswith('/codesign/'):
            # POST /codesign - í˜‘ì—… ì›Œí¬í”Œë¡œìš° ì„¤ê³„
            if http_method == 'POST':
                return await handle_codesign_stream(owner_id, event)
            else:
                return _response(405, {'error': f'Method {http_method} not allowed'})
                
        elif path.endswith('/explain'):
            # POST /codesign/explain - ì›Œí¬í”Œë¡œìš° ì„¤ëª…
            if http_method == 'POST':
                return await handle_explain_workflow(owner_id, event)
            else:
                return _response(405, {'error': f'Method {http_method} not allowed'})
                
        elif path.endswith('/suggestions'):
            # POST /codesign/suggestions - ì œì•ˆ ìƒì„±
            if http_method == 'POST':
                return await handle_generate_suggestions(owner_id, event)
            else:
                return _response(405, {'error': f'Method {http_method} not allowed'})
                
        elif '/apply-suggestion' in path:
            # POST /codesign/apply-suggestion - ì œì•ˆ ì ìš©
            if http_method == 'POST':
                return await handle_apply_suggestion(owner_id, event)
            else:
                return _response(405, {'error': f'Method {http_method} not allowed'})
        else:
            return _response(404, {'error': 'Endpoint not found'})
                
    except Exception as e:
        logger.exception(f"Co-design API error: {e}")
        return _error_response(e)


def _generate_initial_workflow_stream(user_request: str, owner_id: str, session_id: str) -> Generator[str, None, None]:
    """
    ë¹ˆ Canvasì—ì„œ ì´ˆê¸° ì›Œí¬í”Œë¡œìš°ë¥¼ ìƒì„±í•˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜
    Agentic Designer ë¡œì§ì„ ì‚¬ìš©í•˜ì—¬ ì™„ì „í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    [ì¶œë ¥ í˜•ì‹ ì¼ê´€ì„± ë³´ì¥]
    stream_codesign_responseì™€ ë™ì¼í•œ JSONL í˜•ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:
    - {"type": "node", "data": {...}}
    - {"type": "edge", "data": {...}}
    - {"type": "status", "data": "done"}
    - {"type": "error", "error_code": "...", "data": "..."}
    """
    try:
        # Mock ëª¨ë“œ ì²˜ë¦¬
        if _is_mock_mode():
            logger.info("Mock mode: generating sample workflow")
            mock_workflow = _mock_workflow_json()
            
            # ë…¸ë“œë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°
            for node in mock_workflow.get("nodes", []):
                yield json.dumps({"type": "node", "data": node}, ensure_ascii=False) + "\n"
            
            # ì—£ì§€ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°
            for edge in mock_workflow.get("edges", []):
                yield json.dumps({"type": "edge", "data": edge}, ensure_ascii=False) + "\n"
            
            # ì™„ë£Œ ì‹ í˜¸
            yield json.dumps({"type": "status", "data": "done"}) + "\n"
            return
        
        # ë™ì  ëª¨ë¸ ì„ íƒ (Agentic Designer ëª¨ë“œ)
        selected_model_id = get_model_for_canvas_mode(
            canvas_mode="agentic-designer",
            current_workflow={"nodes": [], "edges": []},
            user_request=user_request
        )
        
        logger.info(f"Using model for Agentic Designer: {selected_model_id}")
        
        # ì‹¤ì œ Bedrock ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        system_prompt = SYSTEM_PROMPT
        enhanced_prompt = f"""ì‚¬ìš©ì ìš”ì²­: {user_request}

ìœ„ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì™„ì „í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. 
ê° ë…¸ë“œì™€ ì—£ì§€ë¥¼ JSONL í˜•ì‹ìœ¼ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ì¶œë ¥í•˜ê³ , 
ë§ˆì§€ë§‰ì— {{"type": "status", "data": "done"}}ìœ¼ë¡œ ì™„ë£Œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.

ë ˆì´ì•„ì›ƒ ê·œì¹™ì„ ì¤€ìˆ˜í•˜ì—¬ X=150, YëŠ” 50ë¶€í„° ì‹œì‘í•´ì„œ 100ì”© ì¦ê°€ì‹œì¼œì£¼ì„¸ìš”."""
        
        # ì„ íƒëœ ëª¨ë¸ë¡œ Bedrock ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        for chunk in invoke_bedrock_model_stream(system_prompt, enhanced_prompt):
            yield chunk
            
    except Exception as e:
        logger.error(f"Initial workflow generation failed: {e}")
        # ì—ëŸ¬ë¥¼ ë¹„ì¦ˆë‹ˆìŠ¤ ì¹œí™”ì  ë©”ì‹œì§€ë¡œ ë³€í™˜
        error_info = classify_and_format_error(e)
        error_response = {
            "type": "error",
            "error_code": error_info["error_code"],
            "data": error_info["message"],
            "user_action": error_info.get("user_action"),
            "retryable": error_info.get("retryable", False)
        }
        yield json.dumps(error_response, ensure_ascii=False) + "\n"


async def handle_codesign_stream(owner_id: str, event: Dict) -> Dict:
    """í˜‘ì—… ì›Œí¬í”Œë¡œìš° ì„¤ê³„ ìŠ¤íŠ¸ë¦¬ë° (Canvas ìƒíƒœì— ë”°ë¥¸ ìë™ ëª¨ë“œ ì „í™˜)"""
    body, error = _parse_body(event)
    if error:
        return _response(400, {'error': error})
    
    # í•„ìˆ˜ í•„ë“œ ê²€ì¦
    user_request = body.get('user_request')
    current_workflow = body.get('current_workflow', {"nodes": [], "edges": []})
    
    if not user_request:
        return _response(400, {'error': 'user_request is required'})
    
    try:
        # Canvas ìƒíƒœ í™•ì¸ - ë¹„ì–´ìˆëŠ”ì§€ íŒë‹¨ (ê°œì„ ëœ ë¡œì§)
        nodes = current_workflow.get('nodes', [])
        edges = current_workflow.get('edges', [])
        recent_changes = body.get('recent_changes', [])
        
        # UUID ê¸°ë°˜ ì„¸ì…˜ ID ìƒì„± (ê¸°ì¡´ time() ë°©ì‹ ëŒ€ì²´)
        session_id = body.get('session_id') or _generate_session_id(owner_id)
        
        # ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ í™•ì¸ (ëŒ€í™” ê¸°ë¡ì´ ìˆëŠ”ì§€)
        has_conversation_history = len(recent_changes) > 0
        is_physically_empty = len(nodes) == 0 and len(edges) == 0
        
        # ì •êµí•œ ëª¨ë“œ ê²°ì • ë¡œì§
        if is_physically_empty and not has_conversation_history:
            # ì§„ì§œ ë¹ˆ Canvas - Agentic Designer ëª¨ë“œ
            canvas_mode = "agentic-designer"
            logger.info("Empty canvas with no history - using Agentic Designer mode")
        elif is_physically_empty and has_conversation_history:
            # CanvasëŠ” ë¹„ì–´ìˆì§€ë§Œ ëŒ€í™” ê¸°ë¡ ì¡´ì¬ - Co-design ëª¨ë“œ ìœ ì§€
            canvas_mode = "co-design"
            logger.info("Empty canvas but has conversation history - maintaining Co-design mode")
        else:
            # ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° ì¡´ì¬ - Co-design ëª¨ë“œ
            canvas_mode = "co-design"
            logger.info("Existing workflow detected - using Co-design mode")
        
        logger.info(f"Canvas analysis: nodes={len(nodes)}, edges={len(edges)}, "
                   f"changes={len(recent_changes)}, mode={canvas_mode}")
        
        if canvas_mode == "agentic-designer":
            # Canvasê°€ ë¹„ì–´ìˆê³  ëŒ€í™” ê¸°ë¡ë„ ì—†ìŒ - Agentic Designer ëª¨ë“œë¡œ ì „í™˜
            logger.info(f"Empty canvas detected - switching to Agentic Designer mode")
            generator = _generate_initial_workflow_stream(
                user_request=user_request,
                owner_id=owner_id,
                session_id=session_id
            )
        else:
            # Canvasì— ë‚´ìš©ì´ ìˆê±°ë‚˜ ëŒ€í™” ê¸°ë¡ì´ ìˆìŒ - Co-design ëª¨ë“œ ì‚¬ìš©
            logger.info(f"Using Co-design mode (canvas_mode={canvas_mode})")
            
            # Co-design ëª¨ë“œìš© ëª¨ë¸ ì„ íƒ
            selected_model_id = get_model_for_canvas_mode(
                canvas_mode="co-design",
                current_workflow=current_workflow,
                user_request=user_request,
                recent_changes=recent_changes
            )
            logger.info(f"Using model for Co-design: {selected_model_id}")
            
            generator = stream_codesign_response(
                user_request=user_request,
                current_workflow=current_workflow,
                recent_changes=recent_changes,
                session_id=session_id,
                connection_ids=None  # WebSocket ì—°ê²° IDëŠ” ë³„ë„ ì²˜ë¦¬
            )
        
        logger.info(f"Started workflow streaming for session {session_id[:8]}...")
        return _streaming_response(generator)
        
    except Exception as e:
        logger.error(f"Failed to start workflow streaming: {e}")
        return _error_response(e)


async def handle_explain_workflow(owner_id: str, event: Dict) -> Dict:
    """ì›Œí¬í”Œë¡œìš° ì„¤ëª… ìƒì„±"""
    body, error = _parse_body(event)
    if error:
        return _response(400, {'error': error})
    
    # í•„ìˆ˜ í•„ë“œ ê²€ì¦
    workflow = body.get('workflow')
    if not workflow:
        return _response(400, {'error': 'workflow is required'})
    
    try:
        explanation = explain_workflow(workflow)
        
        logger.info(f"Generated workflow explanation")
        return _response(200, explanation)
        
    except Exception as e:
        logger.error(f"Failed to explain workflow: {e}")
        return _error_response(e)


async def handle_generate_suggestions(owner_id: str, event: Dict) -> Dict:
    """ì›Œí¬í”Œë¡œìš° ìµœì í™” ì œì•ˆ ìƒì„±"""
    body, error = _parse_body(event)
    if error:
        return _response(400, {'error': error})
    
    # í•„ìˆ˜ í•„ë“œ ê²€ì¦
    workflow = body.get('workflow')
    if not workflow:
        return _response(400, {'error': 'workflow is required'})
    
    try:
        max_suggestions = body.get('max_suggestions', 5)
        suggestions = []
        
        # ì œì•ˆ ìƒì„± (Generatorë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜)
        for suggestion in generate_suggestions(workflow, max_suggestions):
            suggestions.append(suggestion)
        
        response = {
            'suggestions': suggestions,
            'count': len(suggestions)
        }
        
        logger.info(f"Generated {len(suggestions)} suggestions")
        return _response(200, response)
        
    except Exception as e:
        logger.error(f"Failed to generate suggestions: {e}")
        return _error_response(e)


async def handle_apply_suggestion(owner_id: str, event: Dict) -> Dict:
    """ì œì•ˆ ì ìš©"""
    body, error = _parse_body(event)
    if error:
        return _response(400, {'error': error})
    
    # í•„ìˆ˜ í•„ë“œ ê²€ì¦
    workflow = body.get('workflow')
    suggestion = body.get('suggestion')
    
    if not workflow or not suggestion:
        return _response(400, {'error': 'workflow and suggestion are required'})
    
    try:
        modified_workflow = apply_suggestion(workflow, suggestion)
        
        response = {
            'modified_workflow': modified_workflow,
            'applied_suggestion': suggestion
        }
        
        logger.info(f"Applied suggestion {suggestion.get('id', 'unknown')}")
        return _response(200, response)
        
    except Exception as e:
        logger.error(f"Failed to apply suggestion: {e}")
        return _error_response(e)


# ============================================================================
# Lambda ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ (ìµœì í™”ëœ ì´ë²¤íŠ¸ ë£¨í”„ ê´€ë¦¬)
# ============================================================================

# ë£¨í”„ ìƒíƒœ ì¶”ì  (í´ë°± ì‹¤í–‰ ëª¨ë‹ˆí„°ë§)
_fallback_execution_count = 0
_FALLBACK_WARNING_THRESHOLD = 3


def lambda_handler_sync(event, context):
    """
    ë™ê¸° Lambda í•¸ë“¤ëŸ¬ ë˜í¼ (ìµœì í™”ëœ ì´ë²¤íŠ¸ ë£¨í”„ ê´€ë¦¬).
    
    [v2.0 ê°œì„ ì‚¬í•­]
    - ì „ì—­ ì´ë²¤íŠ¸ ë£¨í”„ ì¬ì‚¬ìš©ìœ¼ë¡œ ì»¨í…Œì´ë„ˆ ì¬í™œìš© ì‹œ ì˜¤ë²„í—¤ë“œ ì œê±°
    - ë§¤ í˜¸ì¶œë§ˆë‹¤ new_event_loop() ìƒì„±/ì‚­ì œ ë¹„ìš© ì ˆê°
    - ê³ ë¶€í•˜ ìƒí™©ì—ì„œ ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜(Resource Exhaustion) ë°©ì§€
    
    [v2.1 ì•ˆì „ì„± ê°•í™”]
    - í´ë°± ì‹¤í–‰ íšŸìˆ˜ ëª¨ë‹ˆí„°ë§ ë° ê²½ê³ 
    - ë£¨í”„ ìƒíƒœ ì‚¬ì „ ê²€ì¦ìœ¼ë¡œ í´ë°± ë°©ì§€
    """
    global _fallback_execution_count
    
    # ì‚¬ì „ ê²€ì¦: ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
    try:
        running_loop = asyncio.get_running_loop()
        # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ í´ë°± ê²½ë¡œë¡œ ì§„ì…í•  ê²ƒì„ì„ ì‚¬ì „ ê°ì§€
        logger.warning(
            f"âš ï¸ Running loop detected before execution - "
            f"this should not happen in Lambda. Loop: {running_loop}"
        )
    except RuntimeError:
        # ì •ìƒ ê²½ë¡œ: ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ ì—†ìŒ
        pass
    
    loop = get_or_create_event_loop()
    
    try:
        return loop.run_until_complete(lambda_handler(event, context))
    except RuntimeError as e:
        # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆëŠ” ê²½ìš° (ì¤‘ì²© í˜¸ì¶œ) - í´ë°± ê²½ë¡œ
        if "This event loop is already running" in str(e):
            _fallback_execution_count += 1
            
            # í´ë°± ì‹¤í–‰ íšŸìˆ˜ê°€ ì„ê³„ê°’ì„ ë„˜ìœ¼ë©´ ê°•ë ¥ ê²½ê³ 
            if _fallback_execution_count >= _FALLBACK_WARNING_THRESHOLD:
                logger.error(
                    f"ğŸš¨ CRITICAL: ThreadPoolExecutor fallback executed {_fallback_execution_count} times! "
                    f"This indicates an event loop lifecycle issue that MUST be fixed before production. "
                    f"Risk: Latency spikes, OOM errors under high load."
                )
            else:
                logger.warning(
                    f"âš ï¸ ThreadPoolExecutor fallback triggered (count: {_fallback_execution_count}). "
                    f"Event loop was unexpectedly running."
                )
            
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(
                    asyncio.run, 
                    lambda_handler(event, context)
                )
                return future.result(timeout=300)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
        raise


# ============================================================================
# Lambda Function URL RESPONSE_STREAM í•¸ë“¤ëŸ¬ (AWS ê³µì‹ ê·œê²©)
# ============================================================================

async def lambda_handler_streaming(event, response_stream, context):
    """
    AWS Lambda Function URL RESPONSE_STREAM ëª¨ë“œìš© í•¸ë“¤ëŸ¬.
    
    [v2.1 ìˆ˜ì •ì‚¬í•­ - AWS ê³µì‹ ìŠ¤íŠ¸ë¦¬ë° ê·œê²© ì¤€ìˆ˜]
    í•¸ë“¤ëŸ¬ê°€ response_stream ê°ì²´ë¥¼ ì§ì ‘ ì¸ìë¡œ ìˆ˜ì‹ í•©ë‹ˆë‹¤.
    ì´ ê°ì²´ì— .write()ë¡œ ì²­í¬ë¥¼ ì¦‰ì‹œ ì „ì†¡í•˜ê³  .close()ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.
    
    ì„¤ì • ìš”êµ¬ì‚¬í•­:
    1. Lambda Function URL: InvokeMode: RESPONSE_STREAM
    2. í™˜ê²½ë³€ìˆ˜: LAMBDA_STREAMING_ENABLED=true
    3. template.yaml:
        CodesignFunction:
          Type: AWS::Serverless::Function
          Properties:
            Handler: src.handlers.core.codesign_api_handler.lambda_handler_streaming
            FunctionUrlConfig:
              AuthType: AWS_IAM
              InvokeMode: RESPONSE_STREAM
    
    Args:
        event: Lambda ì´ë²¤íŠ¸ ê°ì²´
        response_stream: AWS Lambda ResponseStream ê°ì²´ (ì§ì ‘ write/close)
        context: Lambda ì»¨í…ìŠ¤íŠ¸ ê°ì²´
    """
    try:
        # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ í™•ì¸ (í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜)
        if not _is_streaming_invocation(event):
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œê°€ ì•„ë‹ ê²½ìš° ê¸°ì¡´ ë™ê¸° ë¡œì§ìœ¼ë¡œ ë¸Œë¦¿ì§€
            logger.info("Non-streaming invocation detected, bridging to sync handler")
            result = await lambda_handler(event, context)
            response_stream.write(json.dumps(result, ensure_ascii=False).encode('utf-8'))
            response_stream.close()
            return
        
        # â”€â”€ ìŠ¤íŠ¸ë¦¬ë° ë©”íƒ€ë°ì´í„° ì„¤ì • â”€â”€
        # awslambdaricì˜ StreamingResponseëŠ” ì²« write ì „ì— ë©”íƒ€ë°ì´í„° ì„¤ì • ê°€ëŠ¥
        if hasattr(response_stream, 'set_content_type'):
            response_stream.set_content_type('application/x-ndjson')
        
        # â”€â”€ ìš”ì²­ íŒŒì‹± ë° ê²€ì¦ â”€â”€
        http_method = event.get('httpMethod') or event.get('requestContext', {}).get('http', {}).get('method')
        
        # CORS preflight
        if http_method == 'OPTIONS':
            response_stream.write(json.dumps({"type": "cors", "data": "ok"}).encode('utf-8') + b"\n")
            response_stream.close()
            return
        
        # ì¸ì¦ í™•ì¸
        owner_id = extract_owner_id_from_event(event)
        if not owner_id:
            error_chunk = json.dumps({
                "type": "error",
                "error_code": APIErrorCode.INVALID_REQUEST,
                "data": "Authentication required",
                "user_action": "Please log in and try again."
            }, ensure_ascii=False).encode('utf-8') + b"\n"
            response_stream.write(error_chunk)
            response_stream.close()
            return
        
        # ê²½ë¡œ í™•ì¸
        path = event.get('path') or event.get('rawPath', '')
        
        # â”€â”€ /codesign ì—”ë“œí¬ì¸íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ â”€â”€
        if (path.endswith('/codesign') or path.endswith('/codesign/')) and http_method == 'POST':
            body, error = _parse_body(event)
            if error:
                error_chunk = json.dumps({
                    "type": "error",
                    "error_code": APIErrorCode.INVALID_REQUEST,
                    "data": error
                }, ensure_ascii=False).encode('utf-8') + b"\n"
                response_stream.write(error_chunk)
                response_stream.close()
                return
            
            user_request = body.get('user_request')
            if not user_request:
                error_chunk = json.dumps({
                    "type": "error",
                    "error_code": APIErrorCode.INVALID_REQUEST,
                    "data": "user_request is required"
                }, ensure_ascii=False).encode('utf-8') + b"\n"
                response_stream.write(error_chunk)
                response_stream.close()
                return
            
            current_workflow = body.get('current_workflow', {"nodes": [], "edges": []})
            recent_changes = body.get('recent_changes', [])
            session_id = body.get('session_id') or _generate_session_id(owner_id)
            
            nodes = current_workflow.get('nodes', [])
            edges = current_workflow.get('edges', [])
            is_physically_empty = len(nodes) == 0 and len(edges) == 0
            has_conversation_history = len(recent_changes) > 0
            
            logger.info(f"ğŸš€ Streaming started: session={session_id[:12]}, "
                       f"nodes={len(nodes)}, empty={is_physically_empty}, history={has_conversation_history}")
            
            # ëª¨ë“œ ê²°ì • ë° ì œë„ˆë ˆì´í„° ì„ íƒ
            if is_physically_empty and not has_conversation_history:
                generator = _generate_initial_workflow_stream(
                    user_request=user_request,
                    owner_id=owner_id,
                    session_id=session_id
                )
            else:
                generator = stream_codesign_response(
                    user_request=user_request,
                    current_workflow=current_workflow,
                    recent_changes=recent_changes,
                    session_id=session_id,
                    connection_ids=None
                )
            
            # â”€â”€ ì§„ì§œ ìŠ¤íŠ¸ë¦¬ë°: ì²­í¬ë¥¼ ì¦‰ì‹œ response_streamì— write â”€â”€
            chunk_count = 0
            try:
                for chunk in generator:
                    if chunk:
                        response_stream.write(chunk.encode('utf-8'))
                        # ê°•ì œ í”ŒëŸ¬ì‹œë¡œ ì¦‰ì‹œ ì „ì†¡ (TTFT ìµœì í™”)
                        if hasattr(response_stream, 'flush'):
                            response_stream.flush()
                        chunk_count += 1
                
                logger.info(f"âœ… Streaming completed: {chunk_count} chunks sent")
                
            except Exception as stream_error:
                logger.error(f"âŒ Stream error after {chunk_count} chunks: {stream_error}")
                error_info = classify_and_format_error(stream_error)
                error_chunk = json.dumps({
                    "type": "error",
                    "error_code": error_info["error_code"],
                    "data": error_info["message"],
                    "user_action": error_info.get("user_action"),
                    "chunks_sent_before_error": chunk_count
                }, ensure_ascii=False).encode('utf-8') + b"\n"
                response_stream.write(error_chunk)
        
        else:
            # ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì› ì—”ë“œí¬ì¸íŠ¸ëŠ” ê¸°ì¡´ í•¸ë“¤ëŸ¬ë¡œ ìœ„ì„
            result = await lambda_handler(event, context)
            response_stream.write(json.dumps(result, ensure_ascii=False).encode('utf-8'))
        
    except Exception as e:
        logger.exception(f"âŒ Streaming handler fatal error: {e}")
        error_info = classify_and_format_error(e)
        try:
            error_chunk = json.dumps({
                "type": "error",
                "error_code": error_info["error_code"],
                "data": error_info["message"],
                "user_action": error_info.get("user_action")
            }, ensure_ascii=False).encode('utf-8') + b"\n"
            response_stream.write(error_chunk)
        except:
            pass  # response_stream ìì²´ê°€ ë§ê°€ì§„ ê²½ìš° ë¬´ì‹œ
    
    finally:
        # í•­ìƒ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
        try:
            response_stream.close()
        except:
            pass


def lambda_handler_streaming_sync(event, response_stream, context):
    """
    ë™ê¸° ë˜í¼: Lambda ëŸ°íƒ€ì„ì´ asyncë¥¼ ì§ì ‘ ì§€ì›í•˜ì§€ ì•Šì„ ê²½ìš° ì‚¬ìš©.
    
    ëŒ€ë¶€ë¶„ì˜ ê²½ìš° lambda_handler_streamingì„ ì§ì ‘ ì‚¬ìš©í•˜ë©´ ë˜ì§€ë§Œ,
    íŠ¹ì • Lambda ëŸ°íƒ€ì„ ë²„ì „ì—ì„œëŠ” ì´ ë™ê¸° ë˜í¼ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    loop = get_or_create_event_loop()
    return loop.run_until_complete(
        lambda_handler_streaming(event, response_stream, context)
    )


# Lambda ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ (ê¸°ë³¸: ë™ê¸° ëª¨ë“œ)
lambda_handler = lambda_handler_sync
