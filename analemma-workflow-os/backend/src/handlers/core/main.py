import json
import time
import re
import os
import copy
import operator
import concurrent.futures
import logging
import random
from typing import TypedDict, Dict, Any, List, Optional, Annotated, Union, Callable, Tuple, Literal
from functools import partial
import socket
import ipaddress
from collections import ChainMap
from collections.abc import Mapping
from urllib.parse import urlparse

from pydantic import BaseModel, Field, conlist, constr, ValidationError, field_validator, ConfigDict, model_validator

import boto3
from botocore.config import Config
from botocore.exceptions import ReadTimeoutError
from src.langchain_core_custom.outputs import LLMResult, Generation
from .token_utils import (
    extract_token_usage,
    aggregate_tokens_from_branches,
    aggregate_tokens_from_iterations,
    aggregate_tokens_from_nested,
    accumulate_tokens_in_state,
    calculate_cost_usd
)

# Quality Kernel - SlopDetector Integration (lazy import after logger is defined)
QUALITY_KERNEL_AVAILABLE = False  # Will be set True after successful import below

# -----------------------------------------------------------------------------
# 1. Imports & Constants
# -----------------------------------------------------------------------------

# LangGraph imports for state management
try:
    from langgraph.graph.message import add_messages
except ImportError:
    # Fallback logic mainly for basic testing without full deps
    def add_messages(left, right):
        if not isinstance(left, list): left = [left] if left else []
        if not isinstance(right, list): right = [right] if right else []
        return left + right

# Ïª§Ïä§ÌÖÄ ÏòàÏô∏: Step FunctionsÍ∞Ä Error ÌïÑÎìúÎ°ú ÏâΩÍ≤å Í∞êÏßÄ Í∞ÄÎä• (ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨Ïö©)
class AsyncLLMRequiredException(Exception):
    """Exception raised when async LLM processing is required"""
    pass

# HITP (Human in the Loop) Ïó£ÏßÄ ÌÉÄÏûÖÎì§
HITP_EDGE_TYPES = {"hitp", "human_in_the_loop", "pause"}

# Configure basic logging
logger = logging.getLogger("workflow")
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(message)s"))
    logger.addHandler(handler)
logger.setLevel(logging.INFO)

# Check for LangGraph availability and warn if using fallback
try:
    from langgraph.graph.message import add_messages
except ImportError:
    logger.warning("LangGraph not available, using fallback message reducer. This may cause issues with complex workflows.")

# ============================================================================
# [Quality Kernel] SlopDetector Integration (v2.5)
# ============================================================================
try:
    from src.services.quality_kernel import (
        KernelMiddlewareInterceptor,
        ContentDomain,
        InterceptorAction
    )
    QUALITY_KERNEL_AVAILABLE = True
    logger.info("Quality Kernel loaded successfully - slop detection enabled")
except ImportError as e:
    QUALITY_KERNEL_AVAILABLE = False
    logger.warning(f"Quality Kernel not available, slop detection disabled: {e}")

# ============================================================================
# [Configuration] Model Mapping and Timeouts
# ============================================================================

# Gemini to Bedrock model fallback mapping
# Can be overridden via environment variable BEDROCK_MODEL_MAP (JSON format)
DEFAULT_BEDROCK_MODEL_MAP = {
    "gemini-2.0-flash": "anthropic.claude-3-haiku-20240307-v1:0",
    "gemini-1.5-pro": "anthropic.claude-3-sonnet-20240229-v1:0",
    "gemini-1.5-flash": "anthropic.claude-3-haiku-20240307-v1:0",
    "gemini-1.5-flash-8b": "anthropic.claude-3-haiku-20240307-v1:0",
}

def get_bedrock_model_map() -> Dict[str, str]:
    """
    Get Bedrock model mapping with environment variable override support.
    
    Returns:
        Dictionary mapping Gemini model names to Bedrock model IDs
    """
    try:
        env_map = os.environ.get('BEDROCK_MODEL_MAP')
        if env_map:
            custom_map = json.loads(env_map)
            logger.info(f"Using custom Bedrock model map from environment: {custom_map}")
            return custom_map
    except Exception as e:
        logger.warning(f"Failed to parse BEDROCK_MODEL_MAP from environment: {e}")
    
    return DEFAULT_BEDROCK_MODEL_MAP.copy()

# ============================================================================
# [Cancellation] Execution Cancellation Support
# ============================================================================

def check_execution_cancelled(execution_arn: str) -> bool:
    """
    Check if the Step Functions execution has been cancelled/stopped.
    
    Args:
        execution_arn: The ARN of the Step Functions execution
        
    Returns:
        True if execution is cancelled/stopped, False otherwise
    """
    try:
        # Get Step Functions client
        sfn_client = boto3.client('stepfunctions', region_name=os.environ.get('AWS_REGION', 'us-east-1'))
        
        # Describe execution to get current status
        response = sfn_client.describe_execution(executionArn=execution_arn)
        status = response.get('status', 'RUNNING')
        
        # Check if execution is in a terminal state (not running)
        if status in ['SUCCEEDED', 'FAILED', 'TIMED_OUT', 'ABORTED']:
            logger.warning(f"Execution {execution_arn} is in terminal state: {status}")
            return True
            
        return False
    except Exception as e:
        logger.warning(f"Failed to check execution status for {execution_arn}: {e}")
        # If we can't check, assume not cancelled to avoid false positives
        return False

# Lambda early exit threshold (milliseconds)
# If remaining Lambda execution time < this value, trigger AsyncLLMRequiredException
LAMBDA_EARLY_EXIT_THRESHOLD_MS = int(os.environ.get('LAMBDA_EARLY_EXIT_MS', '10000'))  # 10 seconds default

# LLM default configuration values (environment variable overrides)
# [Fix] Changed default from gpt-3.5-turbo to gemini-2.0-flash (Bedrock compatible via fallback mapping)
DEFAULT_LLM_MODEL = os.environ.get('DEFAULT_LLM_MODEL', 'gemini-2.0-flash')
DEFAULT_MAX_TOKENS = int(os.environ.get('DEFAULT_MAX_TOKENS', '1024'))
DEFAULT_TEMPERATURE = float(os.environ.get('DEFAULT_TEMPERATURE', '0.7'))

# Retry configuration defaults
DEFAULT_RETRY_BASE_DELAY = float(os.environ.get('DEFAULT_RETRY_BASE_DELAY', '1.0'))  # seconds
DEFAULT_RETRY_MAX_DELAY = float(os.environ.get('DEFAULT_RETRY_MAX_DELAY', '30.0'))  # seconds

# Bedrock timeout configuration
DEFAULT_BEDROCK_TIMEOUT = int(os.environ.get('DEFAULT_BEDROCK_TIMEOUT', '90'))  # seconds
LAMBDA_TIMEOUT_BUFFER_MS = int(os.environ.get('LAMBDA_TIMEOUT_BUFFER_MS', '5000'))  # milliseconds

# -----------------------------------------------------------------------------


# -----------------------------------------------------------------------------
# 2. State Schema Definition
# -----------------------------------------------------------------------------

# NOTE: WorkflowState TypedDictÎäî ÌÉÄÏûÖ ÌûåÌä∏/Î¨∏ÏÑúÌôî Î™©Ï†ÅÏúºÎ°úÎßå Ïú†ÏßÄÎê©ÎãàÎã§.
# Ïã§Ï†ú LangGraph 1.0+ Ïã§Ìñâ ÏãúÏóêÎäî DynamicWorkflowBuilderÏóêÏÑú 
# Annotated[Dict[str, Any], merge_state_dict] Ïä§ÌÇ§ÎßàÎ•º ÏÇ¨Ïö©ÌïòÏó¨
# ÎèôÏ†Å ÌÇ§Î•º ÏôÑÎ≤ΩÌïòÍ≤å ÏßÄÏõêÌï©ÎãàÎã§.
#
# Ïù¥ TypedDictÎäî IDE ÏûêÎèôÏôÑÏÑ± Î∞è Ï†ïÏ†Å Î∂ÑÏÑùÏóê ÌôúÏö©Îê©ÎãàÎã§.
class WorkflowState(TypedDict, total=False):
    user_query: str
    user_api_keys: Dict[str, str]
    step_history: List[str]
    # Messages list that accumulates instead of overwriting
    messages: Annotated[List[Dict[str, Any]], add_messages]
    # Common dynamic fields
    item: Any  # For for_each operations
    result: Any  # General result storage
    
    # --- Skills Integration ---
    injected_skills: List[str]
    active_context_ref: str
    active_skills: Dict[str, Any]
    skill_execution_log: Annotated[List[Dict[str, Any]], operator.add]


# -----------------------------------------------------------------------------
# 3. Core Helper Functions (Template, S3 Check, Bedrock)
# -----------------------------------------------------------------------------

# --- Pydantic Schemas for workflow config validation ---

# üõ°Ô∏è [P2] ÌóàÏö©Îêú ÎÖ∏Îìú ÌÉÄÏûÖ Î™©Î°ù - NODE_REGISTRYÏóê Ìï∏Îì§Îü¨Í∞Ä Îì±Î°ùÎêú Ïã§Ìñâ Í∞ÄÎä•Ìïú ÌÉÄÏûÖÎì§Îßå Ìè¨Ìï®
# ‚ö†Ô∏è Ï£ºÏùò: branch, router, join, hitp, pause Îì±ÏùÄ EdgeÎ°ú Ï≤òÎ¶¨ÎêòÎØÄÎ°ú ÎÖ∏Îìú ÌÉÄÏûÖÏóêÏÑú Ï†úÏô∏
ALLOWED_NODE_TYPES = {
    # Core execution types (NODE_REGISTRYÏóê Ìï∏Îì§Îü¨ Îì±Î°ùÎê®)
    "operator", "operator_custom", "operator_official",
    "llm_chat",
    # Flow control (ÎÖ∏ÎìúÎ°ú Ïã§ÌñâÎê®)
    "parallel_group", "aggregator", "for_each", "nested_for_each",
    "loop",  # Convergence support (v3.8)
    "route_draft_quality",  # Quality routing for draft detection
    # Subgraph (Ïû¨Í∑ÄÏ†Å ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ïã§Ìñâ)
    "subgraph",
    # Infrastructure & Data
    "api_call", "db_query",
    # Multimodal & Skills
    "vision", "video_chunker", "skill_executor",
}

# üîó EdgeÎ°ú Ï≤òÎ¶¨ÎêòÎäî ÌÉÄÏûÖÎì§ - ÎÖ∏ÎìúÍ∞Ä ÏïÑÎãå Ïó£ÏßÄ ÏÜçÏÑ±ÏúºÎ°ú Ï†ïÏùòÎê®
# conditional_edge.router_func, edge.type="hitp" Îì±ÏúºÎ°ú Ï≤òÎ¶¨
EDGE_HANDLED_TYPES = {
    "branch", "router", "join",  # conditional_edgeÎ°ú Ï≤òÎ¶¨
    "hitp", "pause",              # edge.typeÏúºÎ°ú Ï≤òÎ¶¨ (HITP_EDGE_TYPES)
}

# üìå UI Ï†ÑÏö© ÎßàÏª§ ÎÖ∏Îìú - Ïã§ÌñâÎêòÏßÄ ÏïäÏùå (ÌîÑÎ°†Ìä∏ÏóîÎìúÏóêÏÑúÎßå ÏÇ¨Ïö©)
# Ïù¥ ÌÉÄÏûÖÎì§ÏùÄ partition_serviceÏóêÏÑú Î¨¥ÏãúÎêòÍ±∞ÎÇò passthroughÎê®
UI_MARKER_TYPES = {
    "input", "output", "start", "end", "trigger",  # trigger Ï∂îÍ∞Ä (API requestÎäî startÎ°ú Îß§Ìïë)
}

# üöÄ Trigger ÎÖ∏Îìú ÌÉÄÏûÖ Îß§Ìïë
# - API request trigger ‚Üí start (ÌòÑÏû¨ Íµ¨ÌòÑÎê®)
# - Time trigger ‚Üí TODO: Íµ¨ÌòÑ ÏòàÏ†ï (cron Í∏∞Î∞ò Ïä§ÏºÄÏ§ÑÎßÅ)
# - External event trigger ‚Üí TODO: Íµ¨ÌòÑ ÏòàÏ†ï (EventBridge/SNS Ïó∞Îèô)
TRIGGER_TYPE_MAPPING = {
    "request": "start",          # API request trigger ‚Üí start nodeÎ°ú Îß§Ìïë
    "api_request": "start",     # Î≥ÑÏπ≠
    # "time": "time_trigger",    # TODO: Íµ¨ÌòÑ ÏòàÏ†ï
    # "schedule": "time_trigger", # TODO: Íµ¨ÌòÑ ÏòàÏ†ï  
    # "event": "event_trigger",  # TODO: Íµ¨ÌòÑ ÏòàÏ†ï
    # "webhook": "event_trigger", # TODO: Íµ¨ÌòÑ ÏòàÏ†ï
}

#  Î≥ÑÏπ≠(Alias) Îß§Ìïë - field_validatorÏóêÏÑú Ï†ïÍ∑ú ÌÉÄÏûÖÏúºÎ°ú Î≥ÄÌôòÎê®
NODE_TYPE_ALIASES = {
    "code": "operator",      # 'code'Îäî 'operator'Ïùò Î≥ÑÏπ≠
    "aimodel": "llm_chat",   # [Fix] map to canonical 'llm_chat' (lowercase variant)
    "llm": "llm_chat",       # [Fix] legacy support
    "chat": "llm_chat",
    "genai": "llm_chat",
    "gpt": "llm_chat",
    "claude": "llm_chat",
    "gemini": "llm_chat",
    # Note: openai_chat not implemented yet (only Gemini/Bedrock supported)
    "function": "operator",
    "lambda": "operator",
    "task": "operator",
    # Note: The following aliases are registered directly in NODE_REGISTRY, not here:
    # - safe_operator, aiModel, parallel, image_analysis
    "map": "for_each",
    "foreach": "for_each",
    # Note: "loop" is a separate node type with loop_runner, not an alias
    "chunker": "video_chunker",
    "group": "subgraph",
    "map_in_map": "nested_for_each",
}

class EdgeModel(BaseModel):
    source: constr(min_length=1, max_length=128)
    target: constr(min_length=1, max_length=128)
    type: constr(min_length=1, max_length=64) = "edge"
    # conditional_edge ÏßÄÏõê ÌïÑÎìú
    router_func: Optional[str] = None        # ÎùºÏö∞ÌÑ∞ Ìï®ÏàòÎ™Ö (NODE_REGISTRYÏóê Îì±Î°ùÎêú)
    mapping: Optional[Dict[str, str]] = None  # ÎùºÏö∞ÌÑ∞ Î∞òÌôòÍ∞í -> ÌÉÄÍ≤ü ÎÖ∏Îìú Îß§Ìïë
    condition: Optional[str] = None           # Ï°∞Í±¥ ÌëúÌòÑÏãù (partition_serviceÏóêÏÑú ÏÇ¨Ïö©)
    
    class Config:
        extra = "ignore"


class NodeModel(BaseModel):
    id: constr(min_length=1, max_length=128)
    type: constr(min_length=1, max_length=64)
    label: Optional[constr(min_length=0, max_length=256)] = None
    action: Optional[constr(min_length=0, max_length=256)] = None
    hitp: Optional[bool] = None
    config: Optional[Dict[str, Any]] = None
    next: Optional[str] = None
    # [Fix] parallel_group support - branchesÏôÄ resource_policy ÌïÑÎìú Ï∂îÍ∞Ä
    # extra="ignore"Î°ú Ïù∏Ìï¥ Ïù¥ ÌïÑÎìúÎì§Ïù¥ ÎàÑÎùΩÎêòÏñ¥ NoneType ÏóêÎü¨ Î∞úÏÉùÌñàÏùå
    branches: Optional[List[Dict[str, Any]]] = None
    resource_policy: Optional[Dict[str, Any]] = None
    # [Fix] subgraph support
    subgraph_ref: Optional[str] = None
    subgraph_inline: Optional[Dict[str, Any]] = None
    
    @field_validator('type', mode='before')
    @classmethod
    def alias_and_validate_node_type(cls, v):
        """
        üõ°Ô∏è [P2] Validate and alias node types.
        - Aliases are converted to canonical types (e.g., 'code' -> 'operator')
        - Unknown types are rejected with clear error message
        - trigger ÎÖ∏ÎìúÎäî trigger_typeÏóê Îî∞Îùº startÎ°ú Îß§ÌïëÎê® (API requestÎßå ÌòÑÏû¨ ÏßÄÏõê)
        """
        if not isinstance(v, str):
            raise ValueError(f"Node type must be string, got {type(v).__name__}")
        
        v = v.strip().lower()
        
        # Apply alias mapping first
        if v in NODE_TYPE_ALIASES:
            return NODE_TYPE_ALIASES[v]
        
        # üõ°Ô∏è All accepted types: executable nodes + UI markers (passthrough)
        all_valid_types = ALLOWED_NODE_TYPES | UI_MARKER_TYPES
        
        # Validate against allowed types
        if v not in all_valid_types:
            raise ValueError(
                f"Unknown node type: '{v}'. "
                f"Allowed types: {sorted(all_valid_types)}. "
                f"Aliases: {NODE_TYPE_ALIASES}"
            )
        
        return v
    
    class Config:
        extra = "ignore"


class WorkflowConfigModel(BaseModel):
    workflow_name: Optional[constr(min_length=0, max_length=256)] = None
    description: Optional[constr(min_length=0, max_length=512)] = None
    nodes: conlist(NodeModel, min_length=0, max_length=500)
    edges: conlist(EdgeModel, min_length=0, max_length=1000)
    start_node: Optional[constr(min_length=1, max_length=128)] = None


# -----------------------------------------------------------------------------
# PII Masking Helpers for Glass-Box logging
# -----------------------------------------------------------------------------
PII_REGEX_PATTERNS = [
    (r"\bsk-[a-zA-Z0-9]{20,}\b", "[API_KEY_REDACTED]"),
    (r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b", "[EMAIL_REDACTED]"),
    (r"\d{3}-\d{3,4}-\d{4}", "[PHONE_REDACTED]"),
]

# -----------------------------------------------------------------------------
# üõ°Ô∏è State Pollution Safeguards - Kernel Protection Layer
# -----------------------------------------------------------------------------
RESERVED_STATE_KEYS = {
    # System Context (Both Case Styles) - ownerId Ï†úÏô∏ (Step Functions JSONPath Ìò∏ÌôòÏÑ±)
    "workflowId", "workflow_id", "owner_id", 
    "execution_id", "user_id", "idempotency_key",
    
    # Flow Control (Most dangerous manipulation points - Loop/Segment control)
    "loop_counter", "max_loop_iterations", "segment_id", 
    "segment_to_run", "total_segments", "segment_type",
    
    # State & Infrastructure (S3 offloading integrity protection)
    "current_state", "final_state", "state_s3_path", "final_state_s3_path",
    "partition_map", "partition_map_s3_path", "__s3_offloaded", "__s3_path",
    
    # Telemetry & Logs (Traceability protection)
    "step_history", "execution_logs", "__new_history_logs", 
    "skill_execution_log", "__kernel_actions",
    
    # Scheduling & Guardrails (Prevent scheduler/guardrail bypass)
    "scheduling_metadata", "__scheduling_metadata", 
    "guardrail_verified", "__guardrail_verified",
    "batch_count_actual", "state_size_threshold",
    
    # Sensitive Credentials (Prevent credential exposure)
    "user_api_keys", "aws_credentials",
    
    # Response Envelope (Step Functions JSONPath Ï†ïÌï©ÏÑ± Ïú†ÏßÄ)
    "status", "error_info"
}

def _validate_output_keys(output: Dict[str, Any], node_id: str) -> Dict[str, Any]:
    """
    üõ°Ô∏è [Guard] Validate and filter output keys to prevent state pollution.

    This function completes the isolation layer of 'user mode vs kernel mode'
    to prevent user-defined code (Operators) from invading the kernel's domain.

    Especially when MOCK_MODE is turned off and real LLM is deployed, this prevents
    accidents where the model generates arbitrary JSON keys and overwrites system metadata.

    Blocking targets:
    - Flow Control variables (loop_counter, segment_id, etc.) ‚Üí Prevent infinite loops/wrong jumps
    - State Infrastructure (state_s3_path, __s3_offloaded, etc.) ‚Üí Protect S3 integrity
    - Telemetry (step_history, execution_logs, etc.) ‚Üí Protect traceability
    - Response Envelope (status, error_info) ‚Üí Maintain Step Functions JSONPath integrity

    Args:
        output: Dictionary returned by the node
        node_id: Node identifier (for logging)

    Returns:
        Safe dictionary with system reserved keys removed
    """
    if not isinstance(output, dict):
        return output
        
    # üõ°Ô∏è [Guard] Kernel domain intrusion check
    forbidden_attempts = [k for k in output.keys() if k in RESERVED_STATE_KEYS]
    
    if forbidden_attempts:
        logger.warning(
            f"üö® [Pollution Blocked] Node '{node_id}' tried to overwrite system keys: {forbidden_attempts}. "
            f"These keys are in the kernel domain and access by user code is prohibited."
        )
        
        # Force data diet: Filter only safe data excluding system keys
        safe_output = {k: v for k, v in output.items() if k not in RESERVED_STATE_KEYS}
        
        # [Telemetry] ÏúÑÎ∞ò ÏãúÎèÑ Í∏∞Î°ù (ÏÑ†ÌÉùÏ†Å)
        # safe_output["__safeguard_violations"] = {
        #     "node_id": node_id,
        #     "blocked_keys": forbidden_attempts,
        #     "timestamp": time.time()
        # }
        
        return safe_output
            
    return output



# -----------------------------------------------------------------------------
# üõ°Ô∏è Pydantic Schema Validation Layer - Type Safety for State
# -----------------------------------------------------------------------------
class SafeStateOutput(BaseModel):
    """
    üõ°Ô∏è [Guard] Pydantic Î™®Îç∏ Í∏∞Î∞ò Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù Î†àÏù¥Ïñ¥
    
    ÎÖ∏Îìú Ï∂úÎ†•Í∞íÏùò ÌÉÄÏûÖ ÏïàÏ†ÑÏÑ±ÏùÑ Î≥¥Ïû•ÌïòÍ≥†, ÏòàÏïΩ ÌÇ§ Ïπ®Î≤îÏùÑ Ïù¥Ï§ëÏúºÎ°ú Ï∞®Îã®Ìï©ÎãàÎã§.
    Ïù¥ Î†àÏù¥Ïñ¥Îäî _validate_output_keysÏùò "Î∞±ÏóÖ Í∞ÄÎìú"Î°ú ÏûëÎèôÌï©ÎãàÎã§.
    
    Ïû•Ï†ê:
    1. ÌÉÄÏûÖ ÏïàÏ†ÑÏÑ±: ÏûòÎ™ªÎêú ÌÉÄÏûÖÏùò Í∞íÏù¥ stateÏóê Ïú†ÏûÖÎêòÎäî Í≤ÉÏùÑ Î∞©ÏßÄ
    2. Ïä§ÌÇ§Îßà Í∞ïÏ†ú: ÏãúÏä§ÌÖú ÌïÑÏàò ÌïÑÎìúÏùò Íµ¨Ï°∞ Í≤ÄÏ¶ù
    3. ÏûêÎèô Î≥ÄÌôò: PydanticÏùò coercion Í∏∞Îä•ÏúºÎ°ú Ìò∏Ìôò Í∞ÄÎä•Ìïú ÌÉÄÏûÖ ÏûêÎèô Î≥ÄÌôò
    
    ÏïÑÌÇ§ÌÖçÏ≤ò Í∞úÏÑ† (2Îã®Í≥Ñ Î∞©Ïñ¥):
    - model_validator(mode='before'): extra ÌïÑÎìúÎ•º Ìè¨Ìï®Ìïú Ï†ÑÏ≤¥ ÏûÖÎ†• Ïä§Ï∫î
    - ÏòàÏïΩ ÌÇ§ Î∞úÍ≤¨ Ïãú None Î∞òÌôò ÎåÄÏã† ÎîïÏÖîÎÑàÎ¶¨ÏóêÏÑú Ï†úÍ±∞ (ÏÉÅÌÉú Ïò§Ïóº Î∞©ÏßÄ)
    """
    model_config = ConfigDict(
        extra='allow',  # ÏÇ¨Ïö©Ïûê Ï†ïÏùò ÌÇ§Îäî ÌóàÏö©
        validate_assignment=True,  # Ìï†Îãπ ÏãúÎßàÎã§ Í≤ÄÏ¶ù
        arbitrary_types_allowed=True
    )
    
    # ÏãúÏä§ÌÖú ÌïÑÏàò ÌïÑÎìú (ÏùΩÍ∏∞ Ï†ÑÏö©, ÎÖ∏ÎìúÍ∞Ä ÏÑ§Ï†ï Î∂àÍ∞Ä)
    workflowId: Optional[str] = Field(None, frozen=True)
    ownerId: Optional[str] = Field(None, frozen=True)
    execution_id: Optional[str] = Field(None, frozen=True)
    
    # Flow Control (ÎÖ∏ÎìúÍ∞Ä Ï†àÎåÄ Î≥ÄÍ≤ΩÌïòÎ©¥ Ïïà ÎêòÎäî ÌïÑÎìú)
    loop_counter: Optional[int] = Field(None, frozen=True, ge=0)
    max_loop_iterations: Optional[int] = Field(None, frozen=True, ge=1)
    segment_id: Optional[int] = Field(None, frozen=True)
    segment_to_run: Optional[int] = Field(None, frozen=True)
    
    @model_validator(mode='before')
    @classmethod
    def block_reserved_keys_globally(cls, data: Any) -> Any:
        """
        üõ°Ô∏è [Critical Fix] Ï†ÑÏó≠ ÏòàÏïΩ ÌÇ§ Ï∞®Îã® (extra ÌïÑÎìú Ìè¨Ìï®)
        
        field_validator('*')Îäî Î™ÖÏãúÏ†ÅÏúºÎ°ú Ï†ïÏùòÎêú ÌïÑÎìúÏóêÎßå Ï†ÅÏö©ÎêòÎØÄÎ°ú,
        model_validatorÎ•º ÏÇ¨Ïö©ÌïòÏó¨ extra ÌïÑÎìúÍπåÏßÄ Ìè¨Ìï®Ìïú Ï†ÑÏ≤¥ ÏûÖÎ†•ÏùÑ Ïä§Ï∫îÌï©ÎãàÎã§.
        
        Ï§ëÏöî: return NoneÏù¥ ÏïÑÎãå ÌÇ§ ÏÇ≠Ï†ú(pop)Î•º ÌÜµÌï¥ ÏÉÅÌÉú Ïò§Ïóº Î∞©ÏßÄ
        - None Î∞òÌôò Ïãú: loop_counter=5 ‚Üí NoneÏúºÎ°ú ÎçÆÏñ¥ÏîÄ (Ïò§Ïóº Î∞úÏÉù)
        - ÌÇ§ ÏÇ≠Ï†ú Ïãú: loop_counterÎäî ÏïÑÏòà Ï∂úÎ†•ÏóêÏÑú Ï†úÏô∏ (Í∏∞Ï°¥ Í∞í Ïú†ÏßÄ)
        """
        if not isinstance(data, dict):
            return data
            
        # ÏòàÏïΩ ÌÇ§ ÌÉêÏßÄ
        forbidden_keys = [k for k in data.keys() if k in RESERVED_STATE_KEYS]
        
        if forbidden_keys:
            logger.warning(
                f"üö® [Pydantic Model Guard] Detected reserved keys in extra fields: {forbidden_keys}. "
                f"Ïù¥ ÌÇ§Îì§ÏùÄ ÎîïÏÖîÎÑàÎ¶¨ÏóêÏÑú Ï†úÍ±∞ÎêòÏñ¥ Ïª§ÎÑê ÏÉÅÌÉúÎ•º Î≥¥Ìò∏Ìï©ÎãàÎã§."
            )
            
            # üõ°Ô∏è [Critical Fix] None Î∞òÌôòÏù¥ ÏïÑÎãå ÌÇ§ ÏÇ≠Ï†ú (ÏÉÅÌÉú Ïò§Ïóº Î∞©ÏßÄ)
            # ÏòàÏïΩ ÌÇ§Î•º ÎîïÏÖîÎÑàÎ¶¨ÏóêÏÑú Ï†úÍ±∞ÌïòÏó¨ ÏÉÅÌÉú Î≥ëÌï© Ïãú Í∏∞Ï°¥ Í∞íÏù¥ Ïú†ÏßÄÎêòÎèÑÎ°ù Ìï®
            cleaned_data = {k: v for k, v in data.items() if k not in RESERVED_STATE_KEYS}
            return cleaned_data
            
        return data


def validate_state_with_schema(output: Dict[str, Any], node_id: str) -> Dict[str, Any]:
    """
    üõ°Ô∏è [Guard] Pydantic Ïä§ÌÇ§ÎßàÎ•º ÏÇ¨Ïö©Ìïú Ï∂îÍ∞Ä Í≤ÄÏ¶ù Î†àÏù¥Ïñ¥
    
    _validate_output_keys Ïù¥ÌõÑ Ïã§ÌñâÎêòÏñ¥ ÌÉÄÏûÖ ÏïàÏ†ÑÏÑ±Í≥º Ïä§ÌÇ§Îßà Ï†ïÌï©ÏÑ±ÏùÑ Î≥¥Ïû•Ìï©ÎãàÎã§.
    
    2Îã®Í≥Ñ Î∞©Ïñ¥ ÏãúÏä§ÌÖú:
    1. Layer 1 (_validate_output_keys): ÏòàÏïΩ ÌÇ§ ÌïÑÌÑ∞ÎßÅ (Í∏∞Î≥∏ Î∞©Ïñ¥ÏÑ†)
    2. Layer 2 (validate_state_with_schema): Pydantic ÌÉÄÏûÖ Í≤ÄÏ¶ù + extra ÌïÑÎìú Ïä§Ï∫î (Î∞±ÏóÖ Î∞©Ïñ¥ÏÑ†)
    
    Args:
        output: ÎÖ∏ÎìúÍ∞Ä Î∞òÌôòÌïú Ï∂úÎ†• ÎîïÏÖîÎÑàÎ¶¨ (Layer 1 ÌÜµÍ≥º ÌõÑ)
        node_id: ÎÖ∏Îìú ÏãùÎ≥ÑÏûê
        
    Returns:
        Ïä§ÌÇ§Îßà Í≤ÄÏ¶ùÏùÑ ÌÜµÍ≥ºÌïú ÏïàÏ†ÑÌïú ÎîïÏÖîÎÑàÎ¶¨
    """
    try:
        # Pydantic Î™®Îç∏Î°ú Î≥ÄÌôòÌïòÏó¨ Í≤ÄÏ¶ù
        # model_validatorÏóêÏÑú ÏòàÏïΩ ÌÇ§Í∞Ä Ï†úÍ±∞ÎêòÍ≥† ÌÉÄÏûÖ Í≤ÄÏ¶ùÏù¥ ÏàòÌñâÎê®
        validated = SafeStateOutput(**output)
        
        # Í≤ÄÏ¶ùÎêú Îç∞Ïù¥ÌÑ∞Îßå Ï∂îÏ∂ú (exclude_noneÏúºÎ°ú None ÌïÑÎìúÎäî Ï†úÏô∏)
        safe_dict = validated.model_dump(
            exclude_none=True,  # None Í∞í Ï†úÏô∏ (ÏÉÅÌÉú Ïò§Ïóº Î∞©ÏßÄ)
            exclude_unset=True,  # ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏùÄ ÌïÑÎìú Ï†úÏô∏
            mode='python'  # Python ÎÑ§Ïù¥Ìã∞Î∏å ÌÉÄÏûÖÏúºÎ°ú Î≥ÄÌôò
        )
        
        # ÏõêÎ≥∏ outputÏóê ÏûàÎçò ÏÇ¨Ïö©Ïûê Ï†ïÏùò ÌÇ§Îäî Î≥¥Ï°¥ (ÏòàÏïΩ ÌÇ§Í∞Ä ÏïÑÎãå Í≤ΩÏö∞Îßå)
        for key, value in output.items():
            if key not in RESERVED_STATE_KEYS and key not in safe_dict:
                safe_dict[key] = value
                
        return safe_dict
        
    except ValidationError as e:
        logger.error(
            f"üö® [Schema Validation Failed] Node '{node_id}' output failed Pydantic validation: {e}"
        )
        # Í≤ÄÏ¶ù Ïã§Ìå® Ïãú ÏõêÎ≥∏ Î∞òÌôò (Ïù¥ÎØ∏ _validate_output_keysÎ•º ÌÜµÍ≥ºÌñàÏúºÎØÄÎ°ú)
        return output
    except Exception as e:
        logger.error(
            f"üö® [Schema Validation Error] Unexpected error in Pydantic validation for node '{node_id}': {e}"
        )
        return output


def mask_pii(text: Any) -> Any:
    if not isinstance(text, str):
        return text
    masked = text
    for pattern, repl in PII_REGEX_PATTERNS:
        masked = re.sub(pattern, repl, masked)
    return masked


def humanize_llm_error(error: Exception, provider: str, node_id: str) -> str:
    """
    üõ°Ô∏è [User-Friendly Error Messages] Convert technical errors to user-friendly messages

    Convert LLM API errors to messages that general users can understand.
    Sensitive information is masked for security.

    Args:
        error: Original exception object
        provider: "gemini" or "bedrock"
        node_id: Node identifier

    Returns:
        User-friendly error message
    """
    error_msg = str(error).lower()
    error_type = type(error).__name__

    # Rate Limit ÏóêÎü¨
    if any(keyword in error_msg for keyword in ["429", "quota", "rate limit", "resource exhausted", "throttling"]):
        return (
            f"üö¶ AI service is currently busy. Please try again in a moment. "
            f"(Rate limit exceeded on {provider})"
        )

    # Authentication/Authorization ÏóêÎü¨
    elif any(keyword in error_msg for keyword in ["403", "forbidden", "unauthorized", "access denied", "permission"]):
        return (
            f"üîê You don't have access to the AI service. Please contact your administrator. "
            f"(Authentication error on {provider})"
        )

    # Timeout ÏóêÎü¨
    elif any(keyword in error_msg for keyword in ["timeout", "deadline", "read timeout"]):
        return (
            f"‚è±Ô∏è AI service response is delayed. Please try again. "
            f"(Timeout on {provider})"
        )

    # Content Safety ÏóêÎü¨
    elif any(keyword in error_msg for keyword in ["blocked", "safety", "content", "inappropriate"]):
        return (
            f"üö´ Your request violates AI safety policies. Please modify your content. "
            f"(Content safety filter on {provider})"
        )

    # Model Not Found ÏóêÎü¨
    elif any(keyword in error_msg for keyword in ["not found", "unavailable", "model", "resource not found"]):
        return (
            f"ü§ñ The requested AI model is not available. Please select a different model. "
            f"(Model unavailable on {provider})"
        )

    # Network/Connection ÏóêÎü¨
    elif any(keyword in error_msg for keyword in ["connection", "network", "dns", "unreachable"]):
        return (
            f"üåê There is a network connection issue. Please check your internet connection. "
            f"(Network error on {provider})"
        )

    # Validation ÏóêÎü¨
    elif any(keyword in error_msg for keyword in ["validation", "invalid", "malformed"]):
        return (
            f"üìù The input data format is incorrect. Please check your content. "
            f"(Validation error on {provider})"
        )

    # Í∏∞ÌÉÄ ÏóêÎü¨
    else:
        return (
            f"‚ö†Ô∏è A temporary error occurred in the AI service. Please try again in a moment. "
            f"(Unexpected error on {provider})"
        )


def _get_nested_value(state: Dict[str, Any], path: str, default: Any = "") -> Any:
    """Retrieve nested value from state using dot-separated path. Handles Mapping types."""
    if not path: return default
    if path not in state and '.' not in path: return state.get(path, default)
    
    parts = path.split('.')
    cur: Any = state
    try:
        for p in parts:
            if isinstance(cur, Mapping) and p in cur:
                cur = cur[p]
            else:
                return default
        return cur
    except Exception:
        return default


def _set_nested_value(state: Dict[str, Any], path: str, value: Any):
    """Set a nested value in state using dot-separated path. Mutates state."""
    if not path: return
    parts = path.split('.')
    
    # 1. Handle non-nested case
    if len(parts) == 1:
        state[parts[0]] = value
        return
        
    # 2. Traverse or create path
    cur = state
    for i, p in enumerate(parts[:-1]):
        if p not in cur or not isinstance(cur[p], dict):
            cur[p] = {}
        cur = cur[p]
    
    # 3. Set the leaf value
    cur[parts[-1]] = value


def _render_template(template: Any, state: Dict[str, Any]) -> Any:
    """Render {{variable}} templates against the provided state. Support basic Jinja2 conditionals."""
    if template is None: return None
    if isinstance(template, str):
        # 1. Handle Basic Jinja2 Conditionals {% if ... %} ... {% else %} ... {% endif %} (Lightweight)
        # Note: This is not a full Jinja2 parser, but supports common patterns used in test definitions.
        while "{% if" in template and "{% endif %}" in template:
            match = re.search(r"\{%\s*if\s+(.+?)\s*%\}(.+?)(?:\{%\s*else\s*%\}(.+?))?\{%\s*endif\s*%\}", template, re.DOTALL)
            if not match: break
            
            full_block = match.group(0)
            condition = match.group(1).strip()
            true_block = match.group(2)
            false_block = match.group(3) or ""
            
            # Evaluate condition
            result = False
            try:
                # Check for 'is undefined' / 'is defined'
                if " is undefined" in condition:
                    var_name = condition.split(" is undefined")[0].strip()
                    val = _get_nested_value(state, var_name, None)
                    result = (val is None)
                elif " is defined" in condition:
                    var_name = condition.split(" is defined")[0].strip()
                    val = _get_nested_value(state, var_name, None)
                    result = (val is not None)
                else:
                    # Comparison logic (e.g., attempt_count < 2)
                    # Safe eval: Only allow simple comparison
                    # Replace variable names with values
                    eval_cond = condition
                    # Match variable names (simple alphanumeric & dots)
                    for var_match in re.finditer(r"\b([a-zA-Z_][a-zA-Z0-9_.]*)\b", condition):
                        var_name = var_match.group(1)
                        if var_name in ("and", "or", "not", "True", "False", "None"): continue
                        val = _get_nested_value(state, var_name, None)
                        if val is None: val = 0 # Default for numeric comparison
                        if isinstance(val, str): val = f"'{val}'"
                        eval_cond = eval_cond.replace(var_name, str(val), 1)
                    
                    # Very restricted eval
                    allowed_chars = set("0123456789.+-*/()<>=! '\"andornotTrueFalse")
                    if set(eval_cond).issubset(allowed_chars):
                        result = eval(eval_cond)
            except Exception as e:
                logger.warning(f"Template condition eval failed: {condition} -> {e}")
                result = False
            
            replacement = true_block if result else false_block
            template = template.replace(full_block, replacement, 1)

        # 2. Variable Substitution {{ var }}
        def _repl(m):
            key = m.group(1).strip()
            # Support basic filters (ignore them for now except | tojson)
            if "|" in key:
                parts = key.split("|")
                key = parts[0].strip()
                filter_name = parts[1].strip()
            else:
                filter_name = None

            if key == "__state_json":
                try: return json.dumps(state, ensure_ascii=False)
                except: return str(state)
            
            val = _get_nested_value(state, key, "")
            
            # Simple handling for | tojson
            if filter_name == "tojson":
                if isinstance(val, (dict, list)):
                    return json.dumps(val, ensure_ascii=False)
            
            if isinstance(val, (dict, list)):
                try: return json.dumps(val, ensure_ascii=False)
                except: return str(val)
            return str(val)
            
        return re.sub(r"\{\{\s*(.+?)\s*\}\}", _repl, template)
        
    if isinstance(template, dict):
        return {k: _render_template(v, state) for k, v in template.items()}
    if isinstance(template, list):
        return [_render_template(v, state) for v in template]
    return template


# --- Bedrock & LLM Helpers ---
_bedrock_client = None
# Base retry/timeout config for reuse (reduces cold-start overhead)
_bedrock_base_config = Config(
    retries={"max_attempts": 3, "mode": "standard"},
    read_timeout=int(os.environ.get("BEDROCK_READ_TIMEOUT_SECONDS", "60")),
    connect_timeout=int(os.environ.get("BEDROCK_CONNECT_TIMEOUT_SECONDS", "5")),
)

def get_bedrock_client():
    global _bedrock_client
    if _bedrock_client:
        return _bedrock_client
    try:
        region = os.environ.get("AWS_REGION", "us-east-1")
        _bedrock_client = boto3.client("bedrock-runtime", region_name=region, config=_bedrock_base_config)
        return _bedrock_client
    except Exception:
        logger.warning("Failed to create Bedrock client")
        return None

def _is_mock_mode() -> bool:
    return os.getenv("MOCK_MODE", "false").strip().lower() in {"true", "1", "yes", "on"}

def _build_mock_llm_text(model_id: str, prompt: str) -> str:
    return f"[MOCK_MODE] Response from {model_id}. Prompt: {prompt[:50]}..."

def invoke_bedrock_model(model_id: str, system_prompt: str | None, user_prompt: str, max_tokens: int | None = None, temperature: float | None = None, read_timeout_seconds: int | None = None) -> Any:
    if _is_mock_mode():
        logger.info(f"MOCK_MODE: Skipping Bedrock call for {model_id}")
        # Mock realistic token usage for testing cost guardrails
        mock_input_tokens = min(500, len(user_prompt) // 4)  # Rough estimate: ~4 chars per token
        mock_output_tokens = 200  # Typical response length
        return {
            "content": [{"text": _build_mock_llm_text(model_id, user_prompt)}],
            "usage": {
                "input_tokens": mock_input_tokens,
                "output_tokens": mock_output_tokens
            }
        }

    try:
        # Prefer shared client; only create ad-hoc client when caller explicitly needs longer timeout
        if read_timeout_seconds and read_timeout_seconds != _bedrock_base_config.read_timeout:
            client = boto3.client("bedrock-runtime", config=_bedrock_base_config.merge(Config(read_timeout=read_timeout_seconds)))
        else:
            client = get_bedrock_client()

        if not client:
            # Fallback if client creation failed
            return {"content": [{"text": "[Error] Bedrock client unavailable"}]}

        messages = [{"role": "user", "content": [{"type": "text", "text": user_prompt}]}]
        payload = {"messages": messages}
        
        if "gemini" not in (model_id or "").lower():
            payload["anthropic_version"] = "bedrock-2023-05-31"
        if system_prompt: payload["system"] = system_prompt
        if max_tokens: payload["max_tokens"] = int(max_tokens)
        if temperature: payload["temperature"] = float(temperature)

        resp = client.invoke_model(body=json.dumps(payload), modelId=model_id)
        return json.loads(resp['body'].read())

    except ReadTimeoutError:
        logger.warning(f"Bedrock read timeout for {model_id}")
        raise AsyncLLMRequiredException("SDK read timeout")
    except Exception as e:
        logger.exception(f"Bedrock invocation failed for {model_id}")
        raise e

def extract_text_from_bedrock_response(resp: Any) -> str:
    """Extract text from src.standard Bedrock response format."""
    try:
        if isinstance(resp, dict):
            c = resp.get("content")
            if isinstance(c, list) and c:
                if "text" in c[0]: return c[0]["text"]
        return str(resp)
    except Exception:
        return str(resp)


def clean_llm_json_response(text: str) -> str:
    """
    Clean LLM response to extract valid JSON.
    
    LLMs often wrap JSON in markdown code blocks or add explanatory text.
    This function strips common artifacts:
    - Markdown code fences: ```json ... ```
    - Leading/trailing whitespace
    - Text before first { or [
    - Text after last } or ]
    
    Returns:
        Cleaned JSON string ready for parsing
    """
    if not isinstance(text, str):
        return str(text)
    
    # Remove markdown code fences
    text = text.strip()
    
    # Pattern 1: ```json\n{...}\n```
    if text.startswith("```json"):
        text = text[7:]  # Remove ```json
        if text.endswith("```"):
            text = text[:-3]  # Remove closing ```
    elif text.startswith("```"):
        text = text[3:]  # Remove generic ```
        if text.endswith("```"):
            text = text[:-3]
    
    text = text.strip()
    
    # Pattern 2: Text before/after JSON object or array
    # Find first { or [ and last } or ]
    start_obj = text.find('{')
    start_arr = text.find('[')
    
    # Determine actual start (whichever comes first, or -1 if neither found)
    if start_obj == -1 and start_arr == -1:
        return text  # No JSON structure found, return as-is
    elif start_obj == -1:
        start = start_arr
    elif start_arr == -1:
        start = start_obj
    else:
        start = min(start_obj, start_arr)
    
    # Find corresponding end
    if text[start] == '{':
        end = text.rfind('}')
    else:
        end = text.rfind(']')
    
    if end != -1 and end > start:
        text = text[start:end+1]
    
    return text.strip()


def parse_llm_json_response(text: str, fallback_value: Any = None) -> Any:
    """
    Attempt to parse LLM response as JSON with robust error handling.
    
    Args:
        text: Raw LLM response text
        fallback_value: Value to return if parsing fails (default: original text)
    
    Returns:
        Parsed JSON object, or fallback_value if parsing fails
    """
    if not isinstance(text, str):
        return fallback_value if fallback_value is not None else text
    
    try:
        # First attempt: direct parsing
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    
    try:
        # Second attempt: clean markdown artifacts
        cleaned = clean_llm_json_response(text)
        return json.loads(cleaned)
    except json.JSONDecodeError as e:
        logger.warning(f"Failed to parse LLM response as JSON: {e}. Returning raw text.")
        return fallback_value if fallback_value is not None else text
    except Exception as e:
        logger.error(f"Unexpected error parsing LLM JSON: {e}")
        return fallback_value if fallback_value is not None else text

def normalize_llm_usage(usage: Dict[str, Any], provider: str) -> Dict[str, Any]:
    """
    Normalize token usage statistics from different LLM providers.
    
    Analemma ÎåÄÏãúÎ≥¥ÎìúÏóêÏÑú ÏùºÍ¥ÄÎêú ÎπÑÏö© Ï∞®Ìä∏Î•º ÌëúÏãúÌïòÍ∏∞ ÏúÑÌï¥ 
    GeminiÏôÄ BedrockÏùò Îã§Î•∏ ÌïÑÎìúÎ™ÖÏùÑ ÌëúÏ§Ä Ïù∏ÌÑ∞ÌéòÏù¥Ïä§Î°ú ÌÜµÌï©Ìï©ÎãàÎã§.
    
    Args:
        usage: Raw usage dictionary from provider
        provider: Provider name ("gemini" or "bedrock")
    
    Returns:
        Normalized usage dict with standard keys for dashboard compatibility:
        {
            "input_tokens": int,      # ÏûÖÎ†• ÌÜ†ÌÅ∞ Ïàò
            "output_tokens": int,     # Ï∂úÎ†• ÌÜ†ÌÅ∞ Ïàò
            "total_tokens": int,      # Ï¥ù ÌÜ†ÌÅ∞ Ïàò
            "cached_tokens": int,     # Context CacheÎ°ú Ï†àÍ∞êÎêú ÌÜ†ÌÅ∞ (Gemini only)
            "estimated_cost_usd": float,  # ÏòàÏÉÅ ÎπÑÏö© (USD)
            "provider": str,          # Ï†úÍ≥µÏûê Ïù¥Î¶Ñ
            "cost_saved_usd": float,  # Ï∫êÏã±ÏúºÎ°ú Ï†àÍ∞êÎêú ÎπÑÏö© (Gemini only)
        }
    """
    normalized = {
        "input_tokens": 0,
        "output_tokens": 0,
        "total_tokens": 0,
        "cached_tokens": 0,
        "estimated_cost_usd": 0.0,
        "cost_saved_usd": 0.0,
        "provider": provider,
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
    }
    
    try:
        if provider == "gemini":
            # Gemini structure: {input_tokens, output_tokens, cached_tokens, total_tokens, estimated_cost_usd}
            normalized["input_tokens"] = usage.get("input_tokens", 0)
            normalized["output_tokens"] = usage.get("output_tokens", 0)
            normalized["cached_tokens"] = usage.get("cached_tokens", 0)
            # [FIX] Always calculate total_tokens to handle Mock responses with total_tokens=0
            raw_total = usage.get("total_tokens", 0)
            calculated_total = normalized["input_tokens"] + normalized["output_tokens"]
            normalized["total_tokens"] = raw_total if raw_total > 0 else calculated_total
            normalized["estimated_cost_usd"] = usage.get("estimated_cost_usd", 0.0)
            
            # Calculate cost savings from caching (configurable reduction for cached tokens)
            cached = normalized["cached_tokens"]
            if cached > 0:
                # Gemini cached tokens cost reduction (default 75% less)
                normalized["cost_saved_usd"] = round(cached * CACHE_COST_REDUCTION * APPROXIMATE_TOKEN_COST_USD, 6)
                
        elif provider == "bedrock":
            # Bedrock structure: {inputTokens, outputTokens} or {input_tokens, output_tokens}
            normalized["input_tokens"] = usage.get("inputTokens") or usage.get("input_tokens", 0)
            normalized["output_tokens"] = usage.get("outputTokens") or usage.get("output_tokens", 0)
            normalized["total_tokens"] = normalized["input_tokens"] + normalized["output_tokens"]
            
            # Bedrock Claude model pricing (approximate)
            # Claude 3 Sonnet: $3/1M input, $15/1M output
            input_cost = (normalized["input_tokens"] / 1_000_000) * 3.0
            output_cost = (normalized["output_tokens"] / 1_000_000) * 15.0
            normalized["estimated_cost_usd"] = round(input_cost + output_cost, 6)
            
    except Exception as e:
        logger.debug(f"Failed to normalize usage from {provider}: {e}")
    
    return normalized


def _auto_upload_image_to_s3(image_data: Union[str, bytes], state: Dict[str, Any], content_type: str = "image/png") -> Optional[str]:
    """
    Base64 ÎòêÎäî bytes Ïù¥ÎØ∏ÏßÄÎ•º S3Ïóê ÏûêÎèô ÏóÖÎ°úÎìúÌïòÍ≥† URIÎ•º Î∞òÌôòÌï©ÎãàÎã§.
    
    Args:
        image_data: Base64 Î¨∏ÏûêÏó¥ ÎòêÎäî bytes
        state: ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏÉÅÌÉú (execution_id Îì± Ï∂îÏ∂úÏö©)
        content_type: MIME ÌÉÄÏûÖ (Í∏∞Î≥∏ image/png)
    
    Returns:
        S3 URI (s3://bucket/key) ÎòêÎäî None (Ïã§Ìå® Ïãú)
    """
    import base64
    import uuid
    
    try:
        # bytes Î≥ÄÌôò
        if isinstance(image_data, str):
            # Base64 Î¨∏ÏûêÏó¥ - data URI prefix Ï†úÍ±∞
            if image_data.startswith('data:'):
                # data:image/png;base64,xxxx ÌòïÏãù
                parts = image_data.split(',', 1)
                if len(parts) == 2:
                    header, image_data = parts
                    # MIME ÌÉÄÏûÖ Ï∂îÏ∂ú
                    if 'image/jpeg' in header or 'image/jpg' in header:
                        content_type = 'image/jpeg'
                    elif 'image/png' in header:
                        content_type = 'image/png'
                    elif 'image/webp' in header:
                        content_type = 'image/webp'
                    elif 'image/gif' in header:
                        content_type = 'image/gif'
            
            image_bytes = base64.b64decode(image_data)
        else:
            image_bytes = image_data
        
        # ÌôïÏû•Ïûê Í≤∞Ï†ï
        ext_map = {
            'image/jpeg': '.jpg',
            'image/png': '.png',
            'image/webp': '.webp',
            'image/gif': '.gif',
        }
        ext = ext_map.get(content_type, '.png')
        
        # S3 ÌÇ§ ÏÉùÏÑ±
        execution_id = state.get('execution_id') or state.get('llm_execution_id') or uuid.uuid4().hex[:8]
        s3_key = f"auto-uploaded/{execution_id}/image_{uuid.uuid4().hex[:8]}{ext}"
        
        # S3 Î≤ÑÌÇ∑ Í≤∞Ï†ï (ÌôòÍ≤Ω Î≥ÄÏàò ÎòêÎäî Í∏∞Î≥∏Í∞í)
        s3_bucket = os.environ.get('WORKFLOW_STATE_BUCKET') or os.environ.get('S3_BUCKET') or 'analemma-workflows-dev'
        
        # S3 ÏóÖÎ°úÎìú
        s3_client = boto3.client('s3', region_name=os.environ.get('AWS_REGION', 'ap-northeast-2'))
        s3_client.put_object(
            Bucket=s3_bucket,
            Key=s3_key,
            Body=image_bytes,
            ContentType=content_type
        )
        
        s3_uri = f"s3://{s3_bucket}/{s3_key}"
        logger.info(f"‚úÖ [Auto Upload] Image uploaded to S3: {s3_uri} ({len(image_bytes)} bytes)")
        return s3_uri
        
    except Exception as e:
        logger.error(f"‚ùå [Auto Upload] Failed to upload image to S3: {e}")
        return None


def _preprocess_image_inputs(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏÉÅÌÉúÏóêÏÑú Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞Î•º Ï†ÑÏ≤òÎ¶¨Ìï©ÎãàÎã§.
    
    ÏßÄÏõê ÌòïÏãù:
    - image_data: Base64 Î¨∏ÏûêÏó¥ ÎòêÎäî data URI
    - image_bytes: bytes Í∞ùÏ≤¥
    - image_base64: Base64 Î¨∏ÏûêÏó¥
    
    Ïù¥ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÏúºÎ©¥ S3Ïóê ÏóÖÎ°úÎìúÌïòÍ≥† image_uriÎ°ú Î≥ÄÌôòÌï©ÎãàÎã§.
    
    Args:
        state: ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏÉÅÌÉú
    
    Returns:
        ÏóÖÎç∞Ïù¥Ìä∏Îêú ÏÉÅÌÉú (image_uri Ï∂îÍ∞ÄÎê®)
    """
    # Ïù¥ÎØ∏ image_uriÍ∞Ä ÏûàÏúºÎ©¥ Ï≤òÎ¶¨ Î∂àÌïÑÏöî
    if state.get('image_uri') and state['image_uri'].startswith('s3://'):
        return state
    
    # Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§ ÌôïÏù∏
    image_data = state.get('image_data') or state.get('image_base64') or state.get('image_bytes')
    
    if not image_data:
        return state
    
    logger.info(f"üñºÔ∏è [Image Preprocess] Detected inline image data, auto-uploading to S3...")
    
    # MIME ÌÉÄÏûÖ Ï∂îÎ°†
    content_type = state.get('image_content_type') or state.get('image_mime_type') or 'image/png'
    
    # S3 ÏóÖÎ°úÎìú
    s3_uri = _auto_upload_image_to_s3(image_data, state, content_type)
    
    if s3_uri:
        state['image_uri'] = s3_uri
        state['_image_auto_uploaded'] = True
        logger.info(f"üñºÔ∏è [Image Preprocess] Auto-converted to S3 URI: {s3_uri}")
    else:
        logger.warning(f"üñºÔ∏è [Image Preprocess] Failed to upload image, image_uri not set")
    
    return state


def prepare_multimodal_content(prompt: str, state: Dict[str, Any]) -> Tuple[str, List[Dict[str, Any]]]:
    """
    Extract S3 URIs from prompt and prepare multimodal content for Gemini Vision API.
    
    [v3.0] Now supports two modes:
    1. Legacy: Extract S3 URIs from prompt text
    2. Explicit: Use _explicit_media_inputs from state (image_inputs/video_inputs)
    
    Gemini Vision APIÎäî ÌÖçÏä§Ìä∏ + Ïù¥ÎØ∏ÏßÄ/ÎπÑÎîîÏò§Î•º contents Î¶¨Ïä§Ìä∏Î°ú Î∞õÏïÑÏïº Ìï©ÎãàÎã§.
    Ïù¥ Ìï®ÏàòÎäî ÌîÑÎ°¨ÌîÑÌä∏ÏóêÏÑú S3 URIÎ•º Ï∂îÏ∂úÌïòÍ±∞ÎÇò Î™ÖÏãúÏ†Å media inputsÎ•º Ï≤òÎ¶¨Ìï©ÎãàÎã§.
    
    Args:
        prompt: User prompt that may contain S3 URIs (e.g., "Ïù¥ Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑùÌï¥Ï§ò s3://bucket/image.jpg")
        state: Execution state with potential hydrated binary data or _explicit_media_inputs
    
    Returns:
        Tuple of (cleaned_prompt, multimodal_parts)
        - cleaned_prompt: Prompt with S3 URIs replaced with placeholder
        - multimodal_parts: List of dicts for invoke_with_images:
            [{"source": "s3://..." or bytes, "mime_type": "image/jpeg", "source_uri": "s3://..."}]
    """
    import re
    
    multimodal_parts = []
    
    # [v3.0] Priority 1: Check for explicit media inputs
    explicit_inputs = state.get("_explicit_media_inputs", [])
    if explicit_inputs:
        logger.info(f"üñºÔ∏è [Multimodal] Processing {len(explicit_inputs)} explicit media inputs")
        
        # MIME type mapping (Gemini ÏßÄÏõê ÌòïÏãù)
        MIME_TYPE_MAP = {
            # Images
            ".jpg": "image/jpeg",
            ".jpeg": "image/jpeg",
            ".png": "image/png",
            ".webp": "image/webp",
            ".gif": "image/gif",
            ".heic": "image/heic",
            ".heif": "image/heif",
            ".bmp": "image/bmp",
            # Videos
            ".mp4": "video/mp4",
            ".mov": "video/quicktime",
            ".avi": "video/x-msvideo",
            ".webm": "video/webm",
            ".mkv": "video/x-matroska",
            # Documents (Gemini 1.5+)
            ".pdf": "application/pdf",
        }
        
        def get_mime_type(uri: str) -> str:
            """ÌôïÏû•ÏûêÏóêÏÑú MIME ÌÉÄÏûÖ Ï∂îÏ∂ú"""
            uri_lower = uri.lower()
            for ext, mime in MIME_TYPE_MAP.items():
                if uri_lower.endswith(ext):
                    return mime
            return "image/jpeg" if "image" in uri_lower else "video/mp4"  # smart default
        
        for media_item in explicit_inputs:
            try:
                source = media_item.get("source")
                media_type = media_item.get("type", "image")
                
                if not source:
                    continue
                
                # Determine MIME type
                mime_type = get_mime_type(str(source))
                
                # Check if source is S3 URI and hydrated
                if isinstance(source, str) and source.startswith("s3://"):
                    s3_key = f"hydrated_{source.replace('s3://', '').replace('/', '_')}"
                    
                    if s3_key in state:
                        # Binary data already hydrated
                        binary_data = state[s3_key]
                        multimodal_parts.append({
                            "source": binary_data,
                            "data": binary_data,
                            "mime_type": mime_type,
                            "source_uri": source,
                            "hydrated": True
                        })
                        logger.info(f"  ‚úì Hydrated {media_type}: {source} ({mime_type})")
                    else:
                        # Not hydrated - pass S3 URI
                        multimodal_parts.append({
                            "source": source,
                            "mime_type": mime_type,
                            "source_uri": source,
                            "hydrated": False
                        })
                        logger.info(f"  ‚úì S3 URI {media_type}: {source} ({mime_type})")
                else:
                    # Already bytes or other format
                    multimodal_parts.append({
                        "source": source,
                        "mime_type": mime_type,
                        "source_uri": str(source)[:100],  # truncate for logging
                        "hydrated": isinstance(source, bytes)
                    })
                    logger.info(f"  ‚úì Direct {media_type}: {mime_type}")
                    
            except Exception as e:
                logger.warning(f"Failed to process explicit media input: {e}")
        
        # Early return - explicit inputs take precedence
        return prompt, multimodal_parts
    
    # [Legacy Mode] Pattern to detect S3 URIs in prompt
    s3_uri_pattern = r's3://[a-zA-Z0-9\-_.]+/[a-zA-Z0-9\-_./]+'
    s3_uris = re.findall(s3_uri_pattern, prompt)
    
    if not s3_uris:
        return prompt, multimodal_parts
    
    # MIME type mapping (Gemini ÏßÄÏõê ÌòïÏãù)
    MIME_TYPE_MAP = {
        # Images
        ".jpg": "image/jpeg",
        ".jpeg": "image/jpeg",
        ".png": "image/png",
        ".webp": "image/webp",
        ".gif": "image/gif",
        ".heic": "image/heic",
        ".heif": "image/heif",
        ".bmp": "image/bmp",
        # Videos
        ".mp4": "video/mp4",
        ".mov": "video/quicktime",
        ".avi": "video/x-msvideo",
        ".webm": "video/webm",
        ".mkv": "video/x-matroska",
        # Documents (Gemini 1.5+)
        ".pdf": "application/pdf",
    }
    
    def get_mime_type(uri: str) -> str:
        """ÌôïÏû•ÏûêÏóêÏÑú MIME ÌÉÄÏûÖ Ï∂îÏ∂ú"""
        uri_lower = uri.lower()
        for ext, mime in MIME_TYPE_MAP.items():
            if uri_lower.endswith(ext):
                return mime
        return "image/jpeg"  # default
    
    # Extract multimodal data
    for s3_uri in s3_uris:
        try:
            mime_type = get_mime_type(s3_uri)
            
            # Check if hydrated data exists in state
            s3_key = f"hydrated_{s3_uri.replace('s3://', '').replace('/', '_')}"
            
            if s3_key in state:
                # Binary data already hydrated - use directly
                binary_data = state[s3_key]
                multimodal_parts.append({
                    "source": binary_data,  # bytes for invoke_with_images
                    "data": binary_data,    # backward compat
                    "mime_type": mime_type,
                    "source_uri": s3_uri,
                    "hydrated": True
                })
                logger.info(f"Prepared hydrated multimodal part: {s3_uri} ({mime_type})")
            else:
                # Not hydrated - pass S3 URI directly (GeminiService will download)
                multimodal_parts.append({
                    "source": s3_uri,       # S3 URI for invoke_with_images
                    "mime_type": mime_type,
                    "source_uri": s3_uri,
                    "hydrated": False
                })
                logger.info(f"Prepared S3 URI multimodal part: {s3_uri} ({mime_type})")
                
        except Exception as e:
            logger.warning(f"Failed to prepare multimodal data for {s3_uri}: {e}")
    
    return prompt, multimodal_parts

# Async processing threshold (configurable via environment variable)
# [DISABLED] Fargate async is NOT IMPLEMENTED yet. Set very high threshold to disable.
# Original: ASYNC_TOKEN_THRESHOLD = 2000 (caused false positives with max_tokens > 2000)
ASYNC_TOKEN_THRESHOLD = int(os.getenv('ASYNC_TOKEN_THRESHOLD', '999999'))  # Effectively disabled
ASYNC_HEAVY_MODELS = os.getenv('ASYNC_HEAVY_MODELS', 'claude-3-opus,gpt-4').split(',')

# HTTP request timeout configuration
DEFAULT_HTTP_TIMEOUT = float(os.environ.get('DEFAULT_HTTP_TIMEOUT', '10.0'))  # seconds
MAX_HTTP_TIMEOUT = float(os.environ.get('MAX_HTTP_TIMEOUT', '30.0'))  # seconds

# Token cost estimation (USD per million tokens)
CACHE_COST_REDUCTION = float(os.environ.get('CACHE_COST_REDUCTION', '0.75'))  # 75% savings
APPROXIMATE_TOKEN_COST_USD = float(os.environ.get('APPROXIMATE_TOKEN_COST_USD', '0.000001'))  # $1 per 1M tokens

def should_use_async_llm(config: Dict[str, Any]) -> bool:
    """
    Heuristic to check if async processing is needed.
    
    [DISABLED] Fargate async worker is NOT IMPLEMENTED.
    Always return False until ECS/Fargate infrastructure is deployed.
    """
    # [TODO] Re-enable when Fargate async worker is implemented
    # max_tokens = config.get("max_tokens", 0)
    # model = config.get("model", "")
    # force_async = config.get("force_async", False)
    # 
    # high_token_count = max_tokens > ASYNC_TOKEN_THRESHOLD
    # heavy_model = any(heavy in model for heavy in ASYNC_HEAVY_MODELS)
    # 
    # if high_token_count or heavy_model or force_async:
    #     logger.info(f"Async required: tokens={max_tokens}, model={model}, force={force_async}")
    #     return True
    return False  # Always sync until Fargate is implemented


# -----------------------------------------------------------------------------
# 4. Node Runners Implementation
# -----------------------------------------------------------------------------


# -----------------------------------------------------------------------------
# S3 Hydration Helper
# -----------------------------------------------------------------------------
def _hydrate_s3_value(value: Any) -> Any:
    """
    If value is an S3 pointer string (s3://bucket/key), download and return content.
    Otherwise return value as-is.
    """
    if not isinstance(value, str) or not value.startswith("s3://"):
        return value
    
    try:
        bucket, key = value.replace("s3://", "").split("/", 1)
        s3 = boto3.client("s3")
        obj = s3.get_object(Bucket=bucket, Key=key)
        content = obj["Body"].read().decode("utf-8")
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            return content
    except Exception as e:
        logger.warning(f"Failed to hydrate S3 value {value}: {e}")
        return value

def _hydrate_state_for_config(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """Hydrate keys referenced in input_variables if present. Supports nested paths."""
    # Create a deep copy to avoid modifying original state if needed, 
    # but _set_nested_value mutates, so we copy once at the start.
    hydrated_state = state.copy()
    input_vars = config.get("input_variables", [])
    if isinstance(input_vars, list):
        for key in input_vars:
            val = _get_nested_value(hydrated_state, key)
            hydrated_val = _hydrate_s3_value(val)
            if hydrated_val != val:
                logger.info(f"üíß [Hydration] Hydrated nested key: {key}")
                _set_nested_value(hydrated_state, key, hydrated_val)
    return hydrated_state

# -----------------------------------------------------------------------------
# 4. Node Runners Implementation
# -----------------------------------------------------------------------------

def llm_chat_runner(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """Standard LLM Chat Runner with Async detection and Retry/Hydration support."""
    
    # [FIX] Support both config structures:
    # 1. Flattened (from builder.py): {id, type, prompt, model, ...}
    # 2. Nested (from for_each/loop): {id, type, config: {prompt, model, ...}}
    # Use fallback pattern: check for nested config first, then use flat
    if 'config' in config and isinstance(config['config'], dict):
        node_id = config.get('id', 'llm')
        actual_config = config['config'].copy()
        actual_config['id'] = node_id  # Preserve node_id
    else:
        actual_config = config
    
    # 0. Hydrate Data (Pre-execution)
    # Ensure S3 pointers defined in input_variables are downloaded
    exec_state = _hydrate_state_for_config(state, actual_config)
    
    # [v2.6] Auto-upload inline image data to S3
    # Supports: image_data (base64), image_bytes, image_base64
    # Converts to image_uri (s3://...) for Vision API
    if actual_config.get("vision_enabled", False):
        exec_state = _preprocess_image_inputs(exec_state)
    
    # [v3.0] Explicit Multimodal Inputs (vision_runner style)
    # Support image_inputs and video_inputs arrays for better UX
    # Allows declarative media specification instead of embedding URIs in prompt
    explicit_media_inputs = []
    
    # Process image_inputs (same as vision_runner)
    raw_image_inputs = actual_config.get("image_inputs", [])
    if isinstance(raw_image_inputs, str):
        raw_image_inputs = [raw_image_inputs]
    
    for img_input in raw_image_inputs:
        resolved = _render_template(img_input, exec_state)
        # Check if state key reference
        if resolved and not resolved.startswith(("s3://", "gs://", "http://", "https://", "data:")):
            state_val = exec_state.get(resolved)
            if state_val:
                resolved = state_val
        
        if resolved:
            explicit_media_inputs.append({"type": "image", "source": resolved})
    
    # Process video_inputs (same as vision_runner)
    raw_video_inputs = actual_config.get("video_inputs", [])
    if isinstance(raw_video_inputs, str):
        raw_video_inputs = [raw_video_inputs]
    
    for vid_input in raw_video_inputs:
        resolved = _render_template(vid_input, exec_state)
        # Check if state key reference
        if resolved and not resolved.startswith(("s3://", "gs://", "http://", "https://")):
            state_val = exec_state.get(resolved)
            if state_val:
                resolved = state_val
        
        if resolved:
            explicit_media_inputs.append({"type": "video", "source": resolved})
    
    # If explicit inputs provided, inject into state for prepare_multimodal_content
    if explicit_media_inputs:
        logger.info(f"üñºÔ∏è [Multimodal] {len(explicit_media_inputs)} explicit media inputs detected")
        # Inject as special state keys for prepare_multimodal_content to process
        exec_state["_explicit_media_inputs"] = explicit_media_inputs
    
    # [FIX] Override MOCK_MODE from state if present (payload takes precedence over Lambda env var)
    # This allows LLM Simulator to force MOCK_MODE=false even when Lambda default is true
    if "MOCK_MODE" in exec_state:
        # [CRITICAL] Convert boolean False to lowercase "false" string
        # Python's str(False) -> "False" which is truthy, need "false" for is_mock_mode()
        mock_mode_value = exec_state["MOCK_MODE"]
        if isinstance(mock_mode_value, bool):
            mock_mode_str = "true" if mock_mode_value else "false"
        else:
            mock_mode_str = str(mock_mode_value).lower()
        
        os.environ["MOCK_MODE"] = mock_mode_str
        logger.info(f"üîÑ MOCK_MODE overridden from state: {exec_state['MOCK_MODE']} -> {mock_mode_str}")
    
    # [Cancellation] Check if execution has been cancelled before starting LLM call
    execution_arn = exec_state.get("execution_arn") or exec_state.get("ExecutionArn")
    if execution_arn and check_execution_cancelled(execution_arn):
        logger.warning(f"üö® Execution cancelled, aborting LLM call for node {actual_config.get('id', 'llm')}")
        raise Exception("Execution cancelled by user")
    
    # 1. Get Retry Config
    # [Fix] None defense: actual_config['retry_config']Í∞Ä NoneÏùº Ïàò ÏûàÏùå
    retry_config = actual_config.get("retry_config") or {}
    max_retries = retry_config.get("max_retries", 0)  # Default 0 means single attempt
    base_delay = retry_config.get("base_delay", DEFAULT_RETRY_BASE_DELAY)
    
    # Initialize retry loop variables
    attempt = 0
    last_error = None
    
    # [Critical] Get Lambda context for timeout management
    # Lambda context is available via global _lambda_context if set by handler
    lambda_context = globals().get('_lambda_context')
    
    while attempt <= max_retries:
        # [Cancellation] Check if execution has been cancelled before each attempt
        if execution_arn and check_execution_cancelled(execution_arn):
            logger.warning(f"üö® Execution cancelled during retry attempt {attempt+1}, aborting LLM call for node {actual_config.get('id', 'llm')}")
            raise Exception("Execution cancelled by user")
        
        try:
            # [Critical] Lambda Early Exit: Check remaining execution time
            if lambda_context and hasattr(lambda_context, 'get_remaining_time_in_millis'):
                remaining_ms = lambda_context.get_remaining_time_in_millis()
                if remaining_ms < LAMBDA_EARLY_EXIT_THRESHOLD_MS:
                    logger.warning(f"üö® Lambda timeout imminent ({remaining_ms}ms remaining). "
                                 f"Triggering async mode to prevent forceful termination.")
                    raise AsyncLLMRequiredException(
                        f"Insufficient Lambda time ({remaining_ms}ms < {LAMBDA_EARLY_EXIT_THRESHOLD_MS}ms)"
                    )
            
            # [Critical] State Isolation: Deep copy to prevent attempt_count pollution
            # Each retry attempt should have clean state to avoid template rendering contamination
            current_attempt_state = exec_state.copy()
            current_attempt_state["attempt_count"] = attempt + 1
            
            # Render prompts with current attempt count
            prompt_template = actual_config.get("prompt_content") or actual_config.get("user_prompt_template") or actual_config.get("prompt_template", "")
            
            # üîß [Workflow Chain Support] ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶øÏù¥ ÏóÜÏùÑ Îïå ÏûêÎèô ÏÉùÏÑ±
            if not prompt_template.strip():
                # ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ï§ëÍ∞Ñ ÎÖ∏ÎìúÎ•º ÏúÑÌïú Í∏∞Î≥∏ ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø ÏÉùÏÑ±
                node_id = actual_config.get('id', 'llm')
                logger.warning(f"üîß [Auto-Prompt] No prompt template for node {node_id}, generating default...")
                
                # 1. messagesÍ∞Ä ÏûàÎäî Í≤ΩÏö∞ (chat ÌòïÌÉú)
                if 'messages' in current_attempt_state and isinstance(current_attempt_state['messages'], list):
                    prompt_template = "Continue the conversation based on the previous messages: {{messages | tojson}}"
                
                # 2. Ïù¥Ï†Ñ ÎÖ∏ÎìúÏùò Ï∂úÎ†•Ïù¥ ÏûàÎäî Í≤ΩÏö∞
                elif any(k.endswith('_output') or k == 'previous_output' or k.startswith('output_') for k in current_attempt_state.keys()):
                    output_keys = [k for k in current_attempt_state.keys() if k.endswith('_output') or k == 'previous_output' or k.startswith('output_')]
                    main_output_key = output_keys[0] if output_keys else 'previous_output'
                    prompt_template = f"Process and analyze the following input:\n\n{{{{ {main_output_key} }}}}"
                
                # 3. initial_state.user_promptÍ∞Ä ÏûàÎäî Í≤ΩÏö∞ (Ï≤´ Î≤àÏß∏ ÎÖ∏Îìú)
                elif 'initial_state' in current_attempt_state:
                    prompt_template = "Analyze and process the following content:\n\n{{ initial_state.user_prompt }}"
                
                # 4. Í∏∞Î≥∏ fallback
                else:
                    prompt_template = "Please provide a helpful response based on the available context: {{ __state_json }}"
                
                logger.info(f"üîß [Auto-Prompt] Generated template for {node_id}: {prompt_template[:100]}...")
            
            prompt = _render_template(prompt_template, current_attempt_state)
            
            # [DEBUG] Log rendered prompt for troubleshooting empty prompt issues
            if not prompt or len(prompt.strip()) < 10:
                logger.error(f"‚ö†Ô∏è [PROMPT DEBUG] Empty or very short prompt detected for node {actual_config.get('id', 'llm')}")
                logger.error(f"‚ö†Ô∏è [PROMPT DEBUG] prompt_template: {prompt_template[:200] if prompt_template else 'NONE'}...")
                logger.error(f"‚ö†Ô∏è [PROMPT DEBUG] rendered prompt: {prompt[:200] if prompt else 'EMPTY'}")
                logger.error(f"‚ö†Ô∏è [PROMPT DEBUG] state keys: {list(current_attempt_state.keys())}")
                # Check for input_text specifically
                input_text_val = current_attempt_state.get('input_text', '__NOT_FOUND__')
                logger.error(f"‚ö†Ô∏è [PROMPT DEBUG] input_text value: {str(input_text_val)[:100] if input_text_val != '__NOT_FOUND__' else 'NOT IN STATE'}")
            
            system_prompt_tmpl = actual_config.get("system_prompt", "")
            system_prompt = _render_template(system_prompt_tmpl, current_attempt_state)
            
            node_id = actual_config.get("id", "llm")

            # [Test Logic] Simulate Rate Limit Error for retry testing
            if prompt and "SIMULATE_RATE_LIMIT_ERROR" in prompt:
                logger.warning(f"üß™ Simulation triggered: Raising Rate Limit Error for node {node_id}")
                # Check provider for appropriate exception type
                provider = actual_config.get("provider", "gemini")
                if provider == "gemini":
                    # Simulate Gemini rate limit (will be caught and retried)
                    raise Exception("429 Resource Exhausted: Quota exceeded for Gemini API")
                else:
                    # Simulate Bedrock ThrottlingException
                    from botocore.exceptions import ClientError
                    raise ClientError(
                        {"Error": {"Code": "ThrottlingException", "Message": "Simulated Rate Limit"}},
                        "InvokeModel"
                    )
            
            # 2. Check Async Conditions
            # node_id already defined above check async
            # [Fix] None defense: actual_config['llm_config']Í∞Ä NoneÏùº Ïàò ÏûàÏùå
            model = actual_config.get("model") or (actual_config.get("llm_config") or {}).get("model_id") or DEFAULT_LLM_MODEL
            max_tokens = actual_config.get("max_tokens") or (actual_config.get("llm_config") or {}).get("max_tokens", DEFAULT_MAX_TOKENS)
            temperature = actual_config.get("temperature") or (actual_config.get("llm_config") or {}).get("temperature", DEFAULT_TEMPERATURE)
            # [Fix] Extract response_schema for structured output (JSON mode)
            response_schema = actual_config.get("response_schema") or (actual_config.get("llm_config") or {}).get("response_schema")
            
            if should_use_async_llm(actual_config):
                logger.warning(f"üö® Async required by heuristic for node {node_id}")
                raise AsyncLLMRequiredException("Resource-intensive processing required")

            # 3. Invoke - Provider Selection (Gemini or Bedrock)
            meta = {
                "model": model, 
                "max_tokens": max_tokens, 
                "attempt": attempt + 1, 
                "provider": "gemini",
                "multimodal": False,  # Will be updated if multimodal_parts detected
                "image_count": 0,
                "video_count": 0
            }
            
            # [Fix] Manually trigger callbacks since we are using Boto3 directly
            callbacks = actual_config.get("callbacks", [])
            if callbacks:
                for cb in callbacks:
                    if hasattr(cb, 'on_llm_start'):
                        try:
                            cb.on_llm_start(serialized={"name": node_id}, prompts=[prompt])
                        except Exception:
                            pass

            # Provider selection: Gemini (default) or Bedrock (fallback)
            provider = actual_config.get("provider", "gemini")
            
            if provider == "gemini":
                # Use Gemini Service (Native SDK)
                try:
                    from src.services.llm.gemini_service import GeminiService, GeminiConfig, GeminiModel
                    
                    # Map model string to GeminiModel enum
                    model_mapping = {
                        "gemini-2.0-flash": GeminiModel.GEMINI_2_0_FLASH,
                        "gemini-1.5-pro": GeminiModel.GEMINI_1_5_PRO,
                        "gemini-1.5-flash": GeminiModel.GEMINI_1_5_FLASH,
                        "gemini-1.5-flash-8b": GeminiModel.GEMINI_1_5_FLASH_8B,
                    }
                    
                    gemini_model = model_mapping.get(model, GeminiModel.GEMINI_1_5_FLASH)
                    
                    gemini_config = GeminiConfig(
                        model=gemini_model,
                        max_output_tokens=max_tokens,
                        temperature=temperature,
                        system_instruction=system_prompt
                    )
                    
                    # Prepare multimodal content if S3 URIs present
                    cleaned_prompt, multimodal_parts = prepare_multimodal_content(prompt, exec_state)
                    
                    service = GeminiService(config=gemini_config)
                    
                    # Use multimodal invocation if images/videos detected
                    if multimodal_parts:
                        # [v3.0] Update meta with multimodal info
                        meta["multimodal"] = True
                        for part in multimodal_parts:
                            mime_type = part.get("mime_type", "")
                            if mime_type.startswith("image/"):
                                meta["image_count"] += 1
                            elif mime_type.startswith("video/"):
                                meta["video_count"] += 1
                        
                        logger.info(f"Invoking Gemini Vision API with {len(multimodal_parts)} multimodal parts "
                                  f"({meta['image_count']} images, {meta['video_count']} videos)")
                        
                        # Extract sources from multimodal_parts using unified 'source' key
                        # source can be: bytes (hydrated) or S3 URI string
                        image_sources = [p["source"] for p in multimodal_parts]
                        mime_types = [p.get("mime_type", "image/jpeg") for p in multimodal_parts]
                        
                        # Log multimodal details for debugging
                        for i, part in enumerate(multimodal_parts):
                            hydrated = part.get("hydrated", False)
                            source_type = "bytes" if hydrated else "S3 URI"
                            logger.debug(f"  Part {i+1}: {part['mime_type']} ({source_type})")
                        
                        # Call invoke_with_images for multimodal processing
                        # [Cancellation] Check before expensive multimodal call
                        if execution_arn and check_execution_cancelled(execution_arn):
                            logger.warning(f"üö® Execution cancelled before multimodal LLM call for node {node_id}")
                            raise Exception("Execution cancelled by user")
                        
                        resp = service.invoke_with_images(
                            user_prompt=cleaned_prompt,
                            image_sources=image_sources,
                            mime_types=mime_types,
                            system_instruction=system_prompt,
                            max_output_tokens=max_tokens,
                            temperature=temperature,
                            response_schema=response_schema  # [Fix] Enable JSON mode for Vision
                        )
                    else:
                        # Standard text-only invocation
                        # [Cancellation] Check before LLM call
                        if execution_arn and check_execution_cancelled(execution_arn):
                            logger.warning(f"üö® Execution cancelled before LLM call for node {node_id}")
                            raise Exception("Execution cancelled by user")
                        
                        resp = service.invoke_model(
                            user_prompt=prompt,
                            system_instruction=system_prompt,
                            max_output_tokens=max_tokens,
                            temperature=temperature,
                            response_schema=response_schema  # [Fix] Enable JSON mode when schema provided
                        )
                    
                    # Extract text from Gemini response structure
                    text = ""
                    if "content" in resp and isinstance(resp["content"], list) and resp["content"]:
                        text = resp["content"][0].get("text", "")
                    elif "text" in resp:
                        text = resp["text"]
                    else:
                        # [FIX] JSON Serialization: Convert Python dict to proper JSON string
                        # This ensures json_parse operator can process the output
                        text = json.dumps(resp, default=str)
                    
                    # Normalize usage statistics
                    # [Fix] None defense: resp['metadata']Í∞Ä NoneÏùº Ïàò ÏûàÏùå
                    raw_usage = (resp.get("metadata") or {}).get("token_usage", {})
                    usage = normalize_llm_usage(raw_usage, "gemini")
                    meta["provider"] = "gemini"
                    meta["multimodal"] = len(multimodal_parts) > 0
                    
                except Exception as gemini_error:
                    # üõ°Ô∏è [Enhanced Fallback Logging] Gemini Ïã§Ìå® Ïãú ÏÉÅÏÑ∏Ìïú Ìè¥Î∞± Î°úÍπÖ
                    error_type = type(gemini_error).__name__
                    error_msg = str(gemini_error).lower()
                    
                    # ÏóêÎü¨ Ïπ¥ÌÖåÍ≥†Î¶¨ Î∂ÑÎ•ò
                    if any(keyword in error_msg for keyword in ["429", "quota", "rate limit", "resource exhausted"]):
                        error_category = "RATE_LIMIT"
                        log_level = logger.warning
                        fallback_reason = "Rate limit exceeded"
                    elif any(keyword in error_msg for keyword in ["403", "forbidden", "permission", "unauthorized"]):
                        error_category = "AUTHENTICATION"
                        log_level = logger.error
                        fallback_reason = "Authentication/permission error"
                    elif any(keyword in error_msg for keyword in ["timeout", "deadline", "unavailable"]):
                        error_category = "TIMEOUT"
                        log_level = logger.warning
                        fallback_reason = "Timeout/network error"
                    elif any(keyword in error_msg for keyword in ["empty", "must not be empty", "required"]):
                        # Input validation error - distinct from content safety
                        error_category = "VALIDATION_ERROR"
                        log_level = logger.error
                        fallback_reason = "Empty or invalid input"
                    elif any(keyword in error_msg for keyword in ["blocked", "safety", "content filter", "harm"]):
                        # More specific patterns for actual content safety issues
                        error_category = "CONTENT_SAFETY"
                        log_level = logger.warning
                        fallback_reason = "Content safety filter"
                    else:
                        error_category = "UNKNOWN"
                        log_level = logger.warning
                        fallback_reason = "Unknown error"
                    
                    log_level(
                        f"üö® [Gemini Fallback Triggered] Node: {node_id}, "
                        f"Category: {error_category}, Reason: {fallback_reason}, "
                        f"Error: {gemini_error}, Falling back to Bedrock"
                    )
                    
                    # Ïû¨ÏãúÎèÑ Í∞ÄÎä•Ìïú ÏóêÎü¨Ïù∏ÏßÄ ÌôïÏù∏
                    is_retryable = any(keyword in error_msg for keyword in [
                        "429", "quota", "rate limit", "resource exhausted",
                        "timeout", "deadline exceeded", "unavailable"
                    ])
                    
                    if is_retryable:
                        # Ïû¨ÏãúÎèÑ Í∞ÄÎä•Ìïú ÏóêÎü¨ - Ïû¨ÏãúÎèÑ Î£®ÌîÑÏóê Ï†ÑÎã¨
                        logger.info(f"Retryable Gemini error, propagating to retry loop: {gemini_error}")
                        raise gemini_error
                    else:
                        # ÎπÑÏû¨ÏãúÎèÑ ÏóêÎü¨ - Bedrock Ìè¥Î∞± ÏãúÎèÑ
                        # [Critical] Vision/multimodal ÏöîÏ≤≠ÏùÄ BedrockÏúºÎ°ú Ìè¥Î∞± Î∂àÍ∞Ä
                        if multimodal_parts:
                            logger.error(
                                f"üö´ [Vision Fallback Blocked] Node: {node_id}, "
                                f"Multimodal requests cannot fall back to Bedrock. "
                                f"Original error: {gemini_error}"
                            )
                            raise RuntimeError(
                                f"Gemini Vision failed and Bedrock fallback not supported for multimodal requests. "
                                f"Original error: {gemini_error}"
                            )
                        
                        logger.warning(
                            f"Non-retryable Gemini error, falling back to Bedrock: {gemini_error}"
                        )
                        provider = "bedrock"
            
            if provider == "bedrock":
                # Bedrock fallback (existing logic)
                meta["provider"] = "bedrock"
                
                # [Critical] Use configurable model mapping instead of hardcoded map
                bedrock_model_map = get_bedrock_model_map()
                bedrock_model_id = bedrock_model_map.get(model, model)  # Use original if not in map
                
                # Log model mapping if applied
                if bedrock_model_id != model:
                    logger.info(f"Model mapped for Bedrock: {model} -> {bedrock_model_id}")
                
                # [Critical] Calculate safe timeout: remaining Lambda time - safety buffer
                read_timeout = DEFAULT_BEDROCK_TIMEOUT
                if lambda_context and hasattr(lambda_context, 'get_remaining_time_in_millis'):
                    remaining_ms = lambda_context.get_remaining_time_in_millis()
                    # Set timeout to remaining time minus buffer (in seconds)
                    max_safe_timeout = max(10, (remaining_ms - LAMBDA_TIMEOUT_BUFFER_MS) / 1000)
                    read_timeout = min(read_timeout, max_safe_timeout)
                    logger.info(f"Adjusted Bedrock timeout: {read_timeout}s (Lambda remaining: {remaining_ms}ms)")
                
                # [Cancellation] Check before Bedrock call
                if execution_arn and check_execution_cancelled(execution_arn):
                    logger.warning(f"üö® Execution cancelled before Bedrock LLM call for node {node_id}")
                    raise Exception("Execution cancelled by user")
                
                resp = invoke_bedrock_model(
                    model_id=bedrock_model_id,
                    system_prompt=system_prompt,
                    user_prompt=prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    read_timeout_seconds=read_timeout
                )
                text = extract_text_from_bedrock_response(resp)
                
                # Extract and normalize usage stats
                raw_usage = {}
                if isinstance(resp, dict) and "usage" in resp:
                    raw_usage = resp["usage"]
                usage = normalize_llm_usage(raw_usage, "bedrock")
            
            # [Fix] Manually trigger on_llm_end
            if callbacks:
                llm_result = LLMResult(generations=[[Generation(text=text)]], llm_output={"usage": usage})
                for cb in callbacks:
                    if hasattr(cb, 'on_llm_end'):
                        try:
                            cb.on_llm_end(response=llm_result)
                        except Exception:
                            pass
            
            # Update history
            current_history = state.get("step_history", [])
            new_history = current_history + [f"{node_id}:llm_call"]
            
            # [Critical] Parse JSON response if parse_json flag is set
            # Enables structured output validation and schema enforcement
            parse_json = config.get("parse_json", False)
            output_value = text
            
            if parse_json:
                logger.info(f"[LLM Response] Parsing JSON for node {node_id}")
                output_value = parse_llm_json_response(text, fallback_value=text)
                
                # Log parsing result
                if isinstance(output_value, (dict, list)):
                    logger.info(f"[LLM Response] Successfully parsed JSON ({type(output_value).__name__})")
                else:
                    logger.warning(f"[LLM Response] JSON parsing failed, using raw text")
            
            # [FIX] Output Key Priority: Use user-specified output_key first, fall back to node_id pattern
            # This ensures test workflows can verify results using exact key names
            out_key = config.get("output_key") or config.get("writes_state_key") or f"{node_id}_output"
            logger.debug(f"[LLM Response] Output key resolved: {out_key} (from config: {config.get('output_key')})")
            
            # [DEBUG] Provider Tracking - Ìè¥Î∞± ÏóÜÏù¥ bedrock ÌëúÏãú Î¨∏Ï†ú ÎîîÎ≤ÑÍπÖ
            logger.info(f"üîç [Provider Debug] Node: {node_id}, meta.provider: {meta.get('provider')}, usage.provider: {usage.get('provider')}")
            
            # ================================================================
            # üõ°Ô∏è [Quality Kernel] SlopDetector Integration (v2.5)
            # LLM ÏùëÎãµÏóê ÎåÄÌïú ÏûêÎèô Ïä¨Î°≠ ÌÉêÏßÄ Î∞è ÌíàÏßà Í≤ÄÏ¶ù
            # Í∏∞Î≥∏: ÌôúÏÑ±Ìôî (QUALITY_KERNEL_AVAILABLE=TrueÏùº Îïå)
            # ÎπÑÌôúÏÑ±Ìôî: disable_kernel_quality_check=true ÏÑ§Ï†ï
            # ================================================================
            kernel_quality_result = None
            
            # ÎπÑÌôúÏÑ±Ìôî Ï°∞Í±¥ ÌôïÏù∏ (Î™ÖÏãúÏ†ÅÏúºÎ°ú ÎÅÑÎäî Í≤ΩÏö∞)
            disable_quality_check = (
                actual_config.get("disable_kernel_quality_check", False) or
                actual_config.get("quality_gate", {}).get("enabled") == False or
                exec_state.get("disable_kernel_quality_check", False)
            )
            
            # Í∏∞Î≥∏ ÌôúÏÑ±Ìôî (Quality Kernel ÏÇ¨Ïö© Í∞ÄÎä• + ÎπÑÌôúÏÑ±Ìôî ÏïàÎê®)
            enable_quality_check = QUALITY_KERNEL_AVAILABLE and not disable_quality_check
            
            if enable_quality_check and isinstance(text, str) and len(text) > 20:
                try:
                    # ÎèÑÎ©îÏù∏ Ï∂îÎ°† (quality_domain alias ÏßÄÏõê)
                    domain_hint = (
                        actual_config.get("quality_domain") or 
                        actual_config.get("content_domain") or 
                        exec_state.get("content_domain", "general_text")
                    )
                    domain_map = {
                        "technical": ContentDomain.TECHNICAL_REPORT,
                        "technical_report": ContentDomain.TECHNICAL_REPORT,
                        "creative": ContentDomain.CREATIVE_WRITING,
                        "code": ContentDomain.CODE_DOCUMENTATION,
                        "api": ContentDomain.API_RESPONSE,
                        "workflow": ContentDomain.WORKFLOW_OUTPUT,
                        "document_analysis": ContentDomain.GENERAL_TEXT,
                    }
                    content_domain = domain_map.get(str(domain_hint).lower(), ContentDomain.GENERAL_TEXT)
                    
                    # Ïù∏ÌÑ∞ÏÖâÌÑ∞ ÏÉùÏÑ± Î∞è Ïã§Ìñâ
                    interceptor = KernelMiddlewareInterceptor(
                        domain=content_domain,
                        slop_threshold=actual_config.get("slop_threshold", 0.5),
                        enable_distillation=False,  # ÌòÑÏû¨Îäî ÌÉêÏßÄÎßå
                        enable_stage2=False  # Stage 2 LLM Í≤ÄÏ¶ùÏùÄ ÎπÑÌôúÏÑ±Ìôî
                    )
                    
                    workflow_id = exec_state.get("workflow_id") or exec_state.get("execution_id", "unknown")
                    
                    # Add response_schema to context for kernel validation
                    kernel_context = exec_state.copy()
                    if response_schema:
                        kernel_context['response_schema'] = response_schema
                    
                    intercept_result = interceptor.post_process_node(
                        node_output=text,
                        node_id=node_id,
                        workflow_id=workflow_id,
                        context=kernel_context
                    )
                    
                    # [Defensive Coding] Use getattr to prevent attribute errors (InterceptorResult v3.8)
                    kernel_quality_result = {
                        "action": getattr(intercept_result, "action", InterceptorAction.PASS).value,
                        "slop_score": getattr(intercept_result.slop_result, "slop_score", 0.0) if getattr(intercept_result, "slop_result", None) else 0.0,
                        "is_slop": getattr(intercept_result.slop_result, "is_slop", False) if getattr(intercept_result, "slop_result", None) else False,
                        "combined_score": getattr(intercept_result, "combined_score", 0.0),
                        "recommendation": getattr(intercept_result, "recommendation", "")
                    }
                    
                    logger.info(
                        f"üõ°Ô∏è [Quality Kernel] Node: {node_id}, "
                        f"Action: {intercept_result.action.value}, "
                        f"Slop Score: {kernel_quality_result['slop_score']:.3f}, "
                        f"Combined: {intercept_result.combined_score:.3f}"
                    )
                    
                    # REJECT Ïï°ÏÖòÏù∏ Í≤ΩÏö∞ Í≤ΩÍ≥† Î°úÍπÖ (ÌòÑÏû¨Îäî Ï∞®Îã®ÌïòÏßÄ ÏïäÏùå)
                    if intercept_result.action == InterceptorAction.REJECT:
                        logger.warning(
                            f"‚ö†Ô∏è [Quality Kernel REJECT] Node: {node_id}, "
                            f"Slop detected but not blocking. Score: {kernel_quality_result['slop_score']:.3f}"
                        )
                        
                except Exception as qk_error:
                    logger.warning(f"Quality Kernel check failed for node {node_id}: {qk_error}")
                    kernel_quality_result = {"error": str(qk_error)}
            
            # üõ°Ô∏è [Guard] Layer 1: Validate output keys (Reserved key check)
            raw_output = {out_key: output_value, f"{node_id}_meta": meta, "step_history": new_history, "usage": usage}
            
            # Quality Kernel Í≤∞Í≥º Ï∂îÍ∞Ä
            if kernel_quality_result:
                raw_output["_kernel_quality_check"] = kernel_quality_result
                raw_output["_kernel_action"] = kernel_quality_result.get("action", "PASS")
            
            validated_output = _validate_output_keys(raw_output, node_id)
            # üõ°Ô∏è [Guard] Layer 2: Schema validation (Type safety)
            return validate_state_with_schema(validated_output, node_id)
            
        except Exception as e:
            last_error = e
            logger.warning(f"LLM execution attempt {attempt+1}/{max_retries+1} failed: {e}")
            
            # [Fix] Manually trigger on_llm_error
            callbacks = config.get("callbacks", [])
            if callbacks:
                for cb in callbacks:
                    if hasattr(cb, 'on_llm_error'):
                        try:
                            cb.on_llm_error(error=e)
                        except Exception:
                            pass

            if isinstance(e, AsyncLLMRequiredException):
                # Bubble up to let orchestrator pause
                raise
            
            # Retry logic
            if attempt < max_retries:
                # Calculate backoff with jitter
                delay = min(DEFAULT_RETRY_MAX_DELAY, base_delay * (2 ** attempt)) * (0.5 + random.random())
                time.sleep(delay)
                attempt += 1
                continue
            else:
                # üõ°Ô∏è [Enhanced Final Error Handling] Î™®Îì† Ïû¨ÏãúÎèÑ Ïã§Ìå® Ïãú ÏÇ¨Ïö©Ïûê ÏπúÌôîÏ†Å ÏóêÎü¨
                human_readable_error = humanize_llm_error(last_error, meta.get("provider", "unknown"), node_id)
                
                logger.error(
                    f"üö® [LLM Execution Failed] Node: {node_id}, "
                    f"Attempts: {max_retries+1}, Final error: {last_error}, "
                    f"Human readable: {human_readable_error}"
                )
                
                # ÏÇ¨Ïö©Ïûê ÏπúÌôîÏ†ÅÏù∏ Î©îÏãúÏßÄÎ•º Ìè¨Ìï®Ìïú ÏÉàÎ°úÏö¥ ÏòàÏô∏ Î∞úÏÉù
                from src.common.exceptions import LLMServiceError
                raise LLMServiceError(
                    message=human_readable_error,
                    original_error=last_error,
                    provider=meta.get("provider", "unknown"),
                    node_id=node_id,
                    attempts=max_retries+1
                ) from last_error

    # Should not reach here
    raise last_error if last_error else RuntimeError("LLM execution failed unexpectedly")

    # Should not reach here
    raise last_error if last_error else RuntimeError("LLM execution failed unexpectedly")


def aggregator_runner(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """Aggregates results from parallel branches or iterations, with special token usage aggregation."""
    node_id = config.get("id", "aggregator")
    # [Fix] Support both flattened and nested config structures
    inner_config = config.get("config") or config
    
    # [Token Aggregation] Ïó¨Îü¨ ÏÜåÏä§Ïùò ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ Ìï©ÏÇ∞
    total_input_tokens = 0
    total_output_tokens = 0
    aggregation_sources = []
    
    # 1. Î≥ëÎ†¨ Í∑∏Î£π Í≤∞Í≥º Ìï©ÏÇ∞
    # [FIX] Support flattened structure from parallel_group_runner (payload size optimization)
    # Strategy 1: Use pre-computed branch_token_details if available
    if 'branch_token_details' in state:
        branch_token_details = state['branch_token_details']
        logger.info(f"Aggregator using pre-computed branch_token_details: {len(branch_token_details)} branches")
        for detail in branch_token_details:
            total_input_tokens += detail.get('input_tokens', 0)
            total_output_tokens += detail.get('output_tokens', 0)
            aggregation_sources.append({
                'source': 'parallel_branch',
                'branch_id': detail.get('branch_id', 'unknown'),
                'input_tokens': detail['input_tokens'],
                'output_tokens': detail['output_tokens'],
                'total_tokens': detail['total_tokens']
            })
    else:
        # Strategy 2: Scan for flattened branch results (branch_* keys)
        logger.info(f"Aggregator scanning for flattened branch results (branch_* keys)")
        for key, value in state.items():
            if key.startswith('branch_') and isinstance(value, dict):
                # Extract branch_id from key (e.g., 'branch_doc_summarize' -> 'branch_doc_summarize')
                branch_id = key
                usage = extract_token_usage(value)
                input_tokens = usage['input_tokens']
                output_tokens = usage['output_tokens']
                
                if input_tokens > 0 or output_tokens > 0:  # Only count branches with actual token usage
                    total_input_tokens += input_tokens
                    total_output_tokens += output_tokens
                    
                    aggregation_sources.append({
                        'source': 'parallel_branch',
                        'branch_id': branch_id,
                        'input_tokens': input_tokens,
                        'output_tokens': output_tokens,
                        'total_tokens': input_tokens + output_tokens
                    })
        
        if aggregation_sources:
            logger.info(f"Aggregator found {len(aggregation_sources)} branches in flattened structure")
        else:
            logger.warning(f"Aggregator {node_id}: No parallel branch results found in state")
    
    # 2. ForEach/Map Í≤∞Í≥º Ìï©ÏÇ∞
    foreach_result = state.get('for_each_result') or state.get('map_result')
    if foreach_result and isinstance(foreach_result, list):
        for i, item_result in enumerate(foreach_result):
            if isinstance(item_result, dict):
                usage = extract_token_usage(item_result)
                input_tokens = usage['input_tokens']
                output_tokens = usage['output_tokens']
                
                total_input_tokens += input_tokens
                total_output_tokens += output_tokens
                
                aggregation_sources.append({
                    'source': 'iteration',
                    'iteration': i,
                    'input_tokens': input_tokens,
                    'output_tokens': output_tokens,
                    'total_tokens': input_tokens + output_tokens
                })
    
    # 3. Ï§ëÏ≤© ForEach Í≤∞Í≥º Ìï©ÏÇ∞
    nested_result = state.get('nested_for_each_result')
    if nested_result and isinstance(nested_result, list):
        usage = aggregate_tokens_from_nested(nested_result)
        total_input_tokens += usage['input_tokens']
        total_output_tokens += usage['output_tokens']
        
        # Add aggregation sources for nested results
        for outer_result in nested_result:
            if isinstance(outer_result, dict):
                outer_id = outer_result.get('outer_id', 'unknown')
                inner_results = outer_result.get('inner_results', [])
                
                for inner_result in inner_results:
                    if isinstance(inner_result, dict):
                        inner_usage = extract_token_usage(inner_result)
                        
                        aggregation_sources.append({
                            'source': 'nested_iteration',
                            'outer_id': outer_id,
                            'input_tokens': inner_usage['input_tokens'],
                            'output_tokens': inner_usage['output_tokens'],
                            'total_tokens': inner_usage['total_tokens']
                        })
    
    # 4. ÏßÅÏ†ëÏ†ÅÏù∏ ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ ÌïÑÎìú Ìï©ÏÇ∞ Ï†úÍ±∞ (token_utils.accumulate_tokens_in_stateÎ°ú ÎåÄÏ≤¥)
    # accumulate_tokens_in_stateÍ∞Ä Ïù¥Ï†Ñ ÏÉÅÌÉúÏùò ÌÜ†ÌÅ∞ÏùÑ ÏûêÎèôÏúºÎ°ú ÎàÑÏ†ÅÌïòÎØÄÎ°ú ÏàòÎèô Ìï©ÏÇ∞ Î∂àÌïÑÏöî
    
    total_tokens = total_input_tokens + total_output_tokens
    
    # Í≤∞Í≥º ÏóÖÎç∞Ïù¥Ìä∏
    result = {
        'total_input_tokens': total_input_tokens,
        'total_output_tokens': total_output_tokens,
        'total_tokens': total_tokens,
        'aggregation_sources': aggregation_sources,
        'aggregated_at': node_id
    }
    
    # [Accumulation] Ïù¥Ï†Ñ ÏÉÅÌÉúÏùò ÌÜ†ÌÅ∞ Í∞íÍ≥º ÎàÑÏ†Å
    result = accumulate_tokens_in_state(result, state)
    
    # ÎπÑÏö© Í≥ÑÏÇ∞ Ï∂îÍ∞Ä
    estimated_cost = calculate_cost_usd({
        'input_tokens': result['total_input_tokens'],
        'output_tokens': result['total_output_tokens']
    })
    result['estimated_cost_usd'] = estimated_cost
    
    logger.info(f"Aggregator {node_id}: Aggregated {len(aggregation_sources)} sources, "
                f"total tokens: {result['total_tokens']} "
                f"({result['total_input_tokens']} input + {result['total_output_tokens']} output), "
                f"cost: ${estimated_cost:.6f}")
    
    return result


def operator_runner(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """Runs arbitrary python code (sandboxed)."""
    # üõ°Ô∏è [Guard] Input Validation
    if config is None:
        config = {}
    
    node_id = config.get("id", "operator")
    code = config.get("code") or (config.get("config") or {}).get("code")
    result_updates = {}
    mock_mode_enabled = _is_mock_mode()
    
    # 1. 'sets' shorthand
    sets = config.get("sets")
    if isinstance(sets, dict):
        result_updates.update(sets)

    # 2. Execute code with security restrictions
    if code:
        # Only allow exec in MOCK_MODE to avoid RCE in production paths
        if not mock_mode_enabled:
            raise PermissionError(f"Operator code execution is disabled outside MOCK_MODE (node={node_id})")
        try:
            # Security: Restrict builtins
            # [FIX v3.2] Pre-import safe standard library modules
            import time as _time
            import json as _json
            import re as _re
            import uuid as _uuid
            import math as _math
            import random as _random
            import sys as _sys
            import collections as _collections
            from datetime import datetime as _datetime, timezone as _timezone, timedelta as _timedelta
            from collections import Counter as _Counter
            
            # [FIX v3.2] Whitelisted __import__ wrapper
            SAFE_MODULES = {
                'time': _time,
                'json': _json,
                're': _re,
                'uuid': _uuid,
                'math': _math,
                'random': _random,
                'datetime': __import__('datetime'),
                'collections': _collections,
                'sys': _sys,
            }
            
            def _safe_import(name, globals=None, locals=None, fromlist=(), level=0):
                if name not in SAFE_MODULES:
                    raise ImportError(f"Module '{name}' is not allowed in workflow code. Allowed: {list(SAFE_MODULES.keys())}")
                return SAFE_MODULES[name]
            
            safe_builtins = {
                "print": print,
                "len": len,
                "list": list,
                "dict": dict,
                "range": range,
                "str": str,
                "int": int,
                "float": float,
                "bool": bool,
                "sum": sum,
                "min": min,
                "max": max,
                "abs": abs,
                "round": round,
                "enumerate": enumerate,
                "zip": zip,
                "sorted": sorted,
                "reversed": reversed,
                "all": all,
                "any": any,
                "filter": filter,
                "map": map,
                "isinstance": isinstance,
                "type": type,
                "tuple": tuple,
                "set": set,
                "frozenset": frozenset,
                "getattr": getattr,
                "setattr": setattr,
                "hasattr": hasattr,
                "open": None,
                "eval": None,
                "exec": None,
                "compile": None,
                "Exception": Exception,
                "ValueError": ValueError,
                "TypeError": TypeError,
                "KeyError": KeyError,
                "RuntimeError": RuntimeError,
                "IndexError": IndexError,
                "AttributeError": AttributeError,
                "StopIteration": StopIteration,
                "ImportError": ImportError,
                "True": True,
                "False": False,
                "None": None,
                "time": _time,
                "json": _json,
                "re": _re,
                "uuid": _uuid,
                "math": _math,
                "random": _random,
                "datetime": _datetime,
                "timezone": _timezone,
                "timedelta": _timedelta,
                "Counter": _Counter,
                "collections": _collections,
                "__import__": _safe_import,
            }
            
            # [Guard] Safety check for state
            if state is None:
                logger.warning(f"[Operator] {node_id} received None state! Initializing empty dict.")
                exec_state = {}
            else:
                exec_state = dict(state)
                
            local_vars = {"state": exec_state, "result": None}
            # Execute with restricted builtins
            exec(code, {"__builtins__": safe_builtins}, local_vars)
            code_result = local_vars.get("result")
            
            # Check if state was modified during execution
            if exec_state != state:
                # State was modified, merge changes (but be careful with security)
                for key, value in exec_state.items():
                    if key not in state or state[key] != value:
                        result_updates[key] = value
            
            if code_result is not None:
                if isinstance(code_result, dict): result_updates.update(code_result)
                else: result_updates[f"{node_id}_result"] = code_result
                    
        except Exception as e:
            logger.exception(f"Operator {node_id} failed")
            raise e
    
    # 3. Handle output_key if specified (for backward compatibility)
    output_key = config.get("output_key")
    if output_key and output_key in result_updates:
        # output_key is already handled above
        pass
            
    if not result_updates:
        result_updates = {f"{node_id}_status": "ok"}
        
    return result_updates

def api_call_runner(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    node_id = config.get("id", "api_call")
    # [Fix] Support both flattened and nested config structures
    inner_config = config.get("config") or config
    # Hydrate potential S3 inputs first
    exec_state = _hydrate_state_for_config(state, inner_config)
    
    url_template = inner_config.get("url")
    if not url_template: raise ValueError("api_call requires 'url'")

    def _validate_outbound_url(url: str) -> None:
        parsed = urlparse(url)
        if parsed.scheme not in ("http", "https"):
            raise ValueError("Only http/https schemes are allowed")
        host = parsed.hostname
        if not host:
            raise ValueError("URL must include hostname")
        # Restrict ports to common web ports to avoid hitting infra/admin ports
        if parsed.port not in (None, 80, 443):
            raise ValueError(f"Port {parsed.port} is not allowed (only 80/443)")
        try:
            infos = socket.getaddrinfo(host, parsed.port or 80, proto=socket.IPPROTO_TCP)
        except socket.gaierror:
            raise ValueError("Invalid hostname")
        for info in infos:
            ip_str = info[4][0]
            ip_obj = ipaddress.ip_address(ip_str)
            if ip_obj.is_private or ip_obj.is_loopback or ip_obj.is_link_local or ip_obj.is_reserved or ip_obj.is_multicast:
                raise ValueError(f"Access to internal/private IP is blocked ({ip_str})")
        return None

    url = _render_template(url_template, exec_state)
    method = (_render_template(inner_config.get("method") or "GET", exec_state)).upper()
    headers = _render_template(inner_config.get("headers"), exec_state) or {}
    params = _render_template(inner_config.get("params"), exec_state)
    json_body = _render_template(inner_config.get("json"), exec_state)
    timeout = _render_template(inner_config.get("timeout", 10), exec_state)

    allowed_methods = {"GET", "POST", "PUT", "PATCH", "DELETE"}
    if method not in allowed_methods:
        raise ValueError(f"Method {method} not allowed; allowed: {sorted(allowed_methods)}")

    try:
        _validate_outbound_url(url)
    except ValueError as ve:
        return {f"{node_id}_status": "error", f"{node_id}_error": str(ve)}

    # Clamp timeout to a safe upper bound
    try:
        timeout_val = float(timeout) if timeout is not None else DEFAULT_HTTP_TIMEOUT
    except Exception:
        timeout_val = DEFAULT_HTTP_TIMEOUT
    timeout_val = max(1.0, min(timeout_val, MAX_HTTP_TIMEOUT))

    try:
        import requests
        resp = requests.request(method, url, headers=headers, params=params, json=json_body, timeout=timeout_val, allow_redirects=False)
        try:
            body = resp.json()
        except Exception:
            body = resp.text
        return {f"{node_id}_status": resp.status_code, f"{node_id}_response": body}
    except requests.exceptions.Timeout:
        return {f"{node_id}_status": "error", f"{node_id}_error": f"Request timed out (limit: {timeout_val}s)"}
    except requests.exceptions.ConnectionError:
        return {f"{node_id}_status": "error", f"{node_id}_error": "Connection refused or DNS failure"}
    except Exception as e:
        return {f"{node_id}_status": "error", f"{node_id}_error": str(e)}

def db_query_runner(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Execute a database query.
    
    SECURITY NOTE: This runner is disabled in production environments.
    Only allowed when ALLOW_DB_QUERY=true environment variable is set.
    """
    node_id = config.get("id", "db_query")
    # [Fix] Support both flattened and nested config structures
    inner_config = config.get("config") or config
    
    # Security: Block db_query in production unless explicitly allowed
    allow_db_query = os.getenv("ALLOW_DB_QUERY", "false").lower() in ("true", "1", "yes")
    is_production = os.getenv("APP_ENV", "").lower() == "production"
    
    if is_production and not allow_db_query:
        logger.warning(f"db_query node {node_id} blocked in production environment")
        return {f"{node_id}_error": "db_query is not allowed in production"}
    
    query = inner_config.get("query")
    conn_str = _render_template(inner_config.get("connection_string"), state)
    if not query or not conn_str: raise ValueError("db_query requires 'query' and 'connection_string'")

    try:
        from src.sqlalchemy import create_engine, text
        engine = create_engine(conn_str)
        with engine.connect() as conn:
            result = conn.execute(text(query), inner_config.get("params", {}))
            fetch = inner_config.get("fetch", "all")
            if fetch == "one":
                row = result.fetchone()
                res = dict(row) if row else None
            elif fetch == "all":
                res = [dict(r) for r in result.fetchall()]
            else:
                res = result.rowcount
            return {f"{node_id}_result": res}
    except Exception as e:
        return {f"{node_id}_error": str(e)}


# -----------------------------------------------------------------------------
# Skill Executor Runner - Execute skills from src.active_skills context
# -----------------------------------------------------------------------------

def skill_executor_runner(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Execute a skill's tool from src.the hydrated active_skills context.
    """
    import time
    from datetime import datetime, timezone
    
    node_id = config.get("id", "skill_executor")
    # [Fix] Support both flattened and nested config structures
    inner_config = config.get("config") or config
    skill_ref = inner_config.get("skill_ref")
    tool_call = inner_config.get("tool_call")
    input_mapping = inner_config.get("input_mapping", {})
    output_key = inner_config.get("output_key", f"{node_id}_result")
    error_handling = inner_config.get("error_handling", "fail")
    
    # Hydrate potential inputs
    exec_state = _hydrate_state_for_config(state, inner_config)
    
    # Validate required config
    if not skill_ref:
        raise ValueError(f"skill_executor node '{node_id}' requires 'skill_ref'")
    if not tool_call:
        raise ValueError(f"skill_executor node '{node_id}' requires 'tool_call'")
    
    # Get active skills from src.state
    active_skills = exec_state.get("active_skills", {})
    
    # Look up the skill
    skill = active_skills.get(skill_ref)
    if not skill:
        error_msg = f"Skill '{skill_ref}' not found in active_skills. Available: {list(active_skills.keys())}"
        if error_handling == "skip":
            logger.warning(f"[skill_executor] {error_msg} - Skipping")
            return {output_key: None, f"{node_id}_skipped": True}
        raise ValueError(error_msg)
    
    # Find the tool definition
    tool_definitions = skill.get("tool_definitions", [])
    tool_def = None
    for td in tool_definitions:
        if td.get("name") == tool_call:
            tool_def = td
            break
    
    if not tool_def:
        error_msg = f"Tool '{tool_call}' not found in skill '{skill_ref}'. Available: {[t.get('name') for t in tool_definitions]}"
        if error_handling == "skip":
            logger.warning(f"[skill_executor] {error_msg} - Skipping")
            return {output_key: None, f"{node_id}_skipped": True}
        raise ValueError(error_msg)
    
    # Render input mappings
    rendered_inputs = {}
    for key, template in input_mapping.items():
        rendered_inputs[key] = _render_template(template, exec_state)
    
    # Determine handler type and dispatch
    handler_type = tool_def.get("handler_type", "operator")
    handler_config = tool_def.get("handler_config", {})
    
    # Merge skill system instructions if this is an LLM call
    if handler_type == "llm_chat" and skill.get("system_instructions"):
        handler_config = {**handler_config}
        existing_system = handler_config.get("system_prompt", "")
        skill_instructions = skill.get("system_instructions", "")
        handler_config["system_prompt"] = f"{skill_instructions}\n\n{existing_system}".strip()
    
    # Build execution config
    exec_config = {
        "id": f"{node_id}_{tool_call}",
        **handler_config,
        **rendered_inputs,
        "callbacks": config.get("callbacks", [])
    }
    
    # Track execution time
    start_time = time.time()
    result = {}
    error = None
    
    try:
        # Dispatch to appropriate handler
        handler = NODE_REGISTRY.get(handler_type)
        if not handler:
            raise ValueError(f"Unknown handler_type '{handler_type}' for tool '{tool_call}'")
        
        result = handler(exec_state, exec_config)
        
    except Exception as e:
        error = str(e)
        logger.exception(f"[skill_executor] Tool '{tool_call}' in skill '{skill_ref}' failed")
        
        if error_handling == "skip":
            result = {output_key: None, f"{node_id}_error": error}
        elif error_handling == "retry":
            # For now, just fail on retry - could implement actual retry logic
            raise
        else:  # fail
            raise
    
    execution_time_ms = int((time.time() - start_time) * 1000)
    
    # Build execution log entry
    log_entry = {
        "skill_id": skill_ref,
        "node_id": node_id,
        "tool_name": tool_call,
        "input_params": rendered_inputs,
        "execution_time_ms": execution_time_ms,
        "timestamp": datetime.now(timezone.utc).isoformat(),
    }
    if error:
        log_entry["error"] = error
    
    # Prepare output
    output = {output_key: result}
    
    # Append to skill execution log (uses Annotated accumulator)
    current_log = exec_state.get("skill_execution_log", [])
    output["skill_execution_log"] = current_log + [log_entry]
    
    # Update step history
    current_history = exec_state.get("step_history", [])
    output["step_history"] = current_history + [f"{node_id}:skill_executor:{skill_ref}.{tool_call}"]
    
    # üõ°Ô∏è [Guard] Layer 1: Validate output keys (Reserved key check)
    validated_output = _validate_output_keys(output, node_id)
    # üõ°Ô∏è [Guard] Layer 2: Schema validation (Type safety)
    return validate_state_with_schema(validated_output, node_id)

def for_each_runner(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """Executes sub-node for each item in list concurrently with multi-node support."""
    node_id = config.get("id", "for_each")
    # [Fix] Support both config structures with fallback pattern
    # Flattened (builder): {id, type, input_list_key, ...}
    # Nested (for_each/loop): {id, type, config: {input_list_key, ...}}
    inner_config = config.get("config") or config
    
    # [Cancellation Check]
    execution_arn = state.get("execution_arn") or state.get("ExecutionArn")
    if execution_arn and check_execution_cancelled(execution_arn):
        logger.warning(f"üö® Execution cancelled, aborting for_each: {node_id}")
        return {inner_config.get("output_key", "for_each_results"): []}

    # 1. Resolve Input List & Hydration
    input_list_key = (
        inner_config.get("input_list_key") or 
        inner_config.get("items_path")
    )
    
    if not input_list_key:
        logger.error(f"[ForEach] Missing input_list_key in node {node_id}")
        return {}

    # Proactive S3 hydration - handles multiple offload patterns
    input_val = _get_nested_value(state, input_list_key)
    
    # [S3 Hydration] Detect input_val offload patterns:
    # Note: Top-level state is already hydrated by segment_runner_service.execute_segment()
    # Here we only check if input_val itself is offloaded
    # 1. input_val is an s3:// string
    # 2. input_val is a dict with __s3_offloaded: True (nested offload)
    needs_hydration = (
        (isinstance(input_val, str) and input_val.startswith("s3://")) or
        (isinstance(input_val, dict) and input_val.get("__s3_offloaded"))
    )
    
    if needs_hydration:
        logger.info(f"üíß [ForEach] Hydrating {input_list_key} from S3 (pattern: {'dict' if isinstance(input_val, dict) else 'string'})")
        
        # [Fix] Handle nested offload dict pattern
        if isinstance(input_val, dict) and input_val.get("__s3_offloaded"):
            s3_path = input_val.get("s3_path") or input_val.get("__s3_path")
            if s3_path:
                try:
                    from src.services.state.state_manager import StateManager
                    sm = StateManager()
                    hydrated_data = sm.download_state_from_s3(s3_path)
                    # Extract 'results' key if present (from for_each offload format)
                    input_list = hydrated_data.get("results", hydrated_data) if isinstance(hydrated_data, dict) else hydrated_data
                    if not isinstance(input_list, list):
                        input_list = [input_list] if input_list else []
                    logger.info(f"‚úÖ [ForEach] Hydrated {len(input_list)} items from S3")
                except Exception as hydrate_err:
                    logger.error(f"‚ùå [ForEach] S3 hydration failed: {hydrate_err}")
                    input_list = []
            else:
                logger.warning(f"‚ö†Ô∏è [ForEach] __s3_offloaded=True but no s3_path found")
                input_list = []
        else:
            state = _hydrate_state_for_config(state, {"input_variables": [input_list_key]})
            input_list = _get_nested_value(state, input_list_key, [])
    else:
        input_list = input_val if isinstance(input_val, list) else []

    # 2. Resolve Multi-node Sub-workflow
    sub_nodes = []
    # sub_workflow.nodes -> subgraph_inline.nodes -> sub_node_config ÏàúÏúºÎ°ú ÌÉêÏÉâ
    sw_def = inner_config.get("sub_workflow") or inner_config.get("subgraph_inline")
    if isinstance(sw_def, dict):
        sub_nodes = sw_def.get("nodes", [])
    else:
        sub_processor = inner_config.get("sub_node_config")
        if sub_processor: sub_nodes = [sub_processor]

    if not sub_nodes:
        logger.error(f"[ForEach] No executable nodes found for {node_id}")
        return {}

    item_key = inner_config.get("item_key") or "item"
    output_key = inner_config.get("output_key") or "for_each_results"
    max_iterations = inner_config.get("max_iterations", 20)
    
    if len(input_list) > max_iterations:
        logger.warning(f"for_each truncated {len(input_list)} -> {max_iterations}")
        input_list = input_list[:max_iterations]

    # [Critical Fix] Create immutable snapshot of state for thread-safe parallel execution
    # ChainMap with shared state causes race conditions in ThreadPoolExecutor
    import copy
    state_snapshot = copy.deepcopy(state)  # Deep copy once, used by all workers
    
    def worker(item):
        if execution_arn and check_execution_cancelled(execution_arn):
            return {"error": "cancelled", "_item": item}
        
        # [Fix] Each worker gets isolated state view (not shared reference)
        # Using dict merge instead of ChainMap for true isolation
        it_state = {**state_snapshot, item_key: item}
        
        # [Fix] Hydrate item if it's an S3 offloaded object
        if isinstance(item, dict) and item.get("__s3_offloaded"):
            try:
                from src.services.state.state_manager import StateManager
                sm = StateManager()
                s3_path = item.get("s3_path") or item.get("__s3_path")
                if s3_path:
                    hydrated_item = sm.download_state_from_s3(s3_path)
                    it_state[item_key] = hydrated_item
            except Exception as e:
                logger.warning(f"[ForEach] Failed to hydrate item from S3: {e}")
        
        # Deep copy messages to prevent race conditions
        if "messages" in it_state and isinstance(it_state["messages"], list):
            it_state["messages"] = it_state["messages"].copy()
            
        it_updates = {}
        
        for node_def in sub_nodes:
            # [Fix] Ïù¥Ï†Ñ ÎÖ∏ÎìúÏùò Í≤∞Í≥ºÎ•º Îã§Ïùå ÎÖ∏ÎìúÍ∞Ä Î≥º Ïàò ÏûàÍ≤å Î≥ëÌï©
            current_view = ChainMap(it_updates, it_state)
            node_type = node_def.get("type", "llm_chat")
            handler = NODE_REGISTRY.get(node_type)
            
            if not handler:
                logger.error(f"Unknown node type '{node_type}' in for_each iteration")
                continue
                
            try:
                rendered_node = _render_template(node_def, current_view)
                # Ensure node ID uniqueness
                rendered_node["id"] = f"{node_id}_{node_def.get('id', 'sub')}"
                
                updates = handler(current_view, rendered_node)
                if isinstance(updates, dict):
                    it_updates.update(updates)
            # [DISABLED] AsyncLLMRequiredException handling - Fargate async not implemented
            # except AsyncLLMRequiredException:
            #     # Bubble up to trigger PAUSED_FOR_ASYNC_LLM
            #     raise
            except Exception as e:
                logger.error(f"Iteration node {node_def.get('id')} failed: {e}")
                it_updates["error"] = str(e)
                break
        
        return it_updates

    # 3. Parallel Execution
    if not input_list:
        return {output_key: []}

    cpu_count = os.cpu_count() or 2
    max_workers = min(len(input_list), max(1, cpu_count // 2))
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        results = list(executor.map(worker, input_list))
    
    # 5. Token Aggregation & Metrics
    total_input_tokens = 0
    total_output_tokens = 0
    iteration_token_details = []
    
    for i, result in enumerate(results):
        # Í∞Å iteration Í≤∞Í≥ºÏóêÏÑú ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ Ï∂îÏ∂ú (usage ÎòêÎäî token_usage ÌÇ§ ÏßÄÏõê)
        if isinstance(result, dict):
            usage = extract_token_usage(result)
            input_tokens = usage['input_tokens']
            output_tokens = usage['output_tokens']
            
            total_input_tokens += input_tokens
            total_output_tokens += output_tokens
            
            iteration_token_details.append({
                'iteration': i,
                'input_tokens': input_tokens,
                'output_tokens': output_tokens,
                'total_tokens': input_tokens + output_tokens
            })
    
    # Ìï©ÏÇ∞Îêú ÌÜ†ÌÅ∞ Ï†ïÎ≥¥Î•º Í≤∞Í≥ºÏóê Ï∂îÍ∞Ä
    result_updates = {output_key: results}
    
    # üõ°Ô∏è [v3.9] Stage 6 Forced Offloading (Payload Protection)
    # If results list exceeds Step Functions safe payload limit, offload to S3
    try:
        results_json = json.dumps(results, ensure_ascii=False)
        results_size_kb = len(results_json.encode('utf-8')) / 1024
        
        if results_size_kb > 150:  # 150KB Threshold
            logger.warning(f"üì¶ [ForEach Offload] Results too large ({results_size_kb:.1f}KB), offloading to S3")
            from src.services.state.state_manager import StateManager
            sm = StateManager()
            
            auth_user_id = state.get("ownerId") or "system"
            workflow_id = state.get("workflowId") or "unknown"
            execution_id = state.get("execution_id") or "unknown"
            
            s3_path = sm.upload_state_to_s3(
                bucket=os.environ.get("WORKFLOW_STATE_BUCKET"),
                prefix=f"offloaded-results/{auth_user_id}/{workflow_id}/{execution_id}",
                state={"results": results},
                deterministic_filename=f"{node_id}_results.json"
            )
            
            # Replace actual results with a pointer
            result_updates[output_key] = {
                "__s3_offloaded": True,
                "s3_path": s3_path,
                "size_kb": results_size_kb,
                "node_id": node_id
            }
            logger.info(f"‚úÖ [ForEach Offload] Results offloaded to {s3_path}")
    except Exception as offload_err:
        logger.error(f"‚ùå [ForEach Offload] Failed to check/offload results: {offload_err}")

    result_updates['total_input_tokens'] = total_input_tokens
    result_updates['total_output_tokens'] = total_output_tokens
    result_updates['total_tokens'] = total_input_tokens + total_output_tokens
    result_updates['iteration_token_details'] = iteration_token_details
    
    # [Accumulation] Ïù¥Ï†Ñ ÏÉÅÌÉúÏùò ÌÜ†ÌÅ∞ Í∞íÍ≥º ÎàÑÏ†Å
    temp_result = {
        'total_input_tokens': result_updates['total_input_tokens'],
        'total_output_tokens': result_updates['total_output_tokens'],
        'total_tokens': result_updates['total_tokens']
    }
    accumulated_result = accumulate_tokens_in_state(temp_result, state)
    result_updates.update(accumulated_result)
    
    # üõ°Ô∏è [v3.9] Stage 7 StateBag Integrity Merge
    # Ensure iteration results are merged with recursive integrity
    from src.common.statebag import ensure_state_bag
    final_updates = ensure_state_bag(result_updates)
    
    # ÎπÑÏö© Í≥ÑÏÇ∞ Ï∂îÍ∞Ä
    estimated_cost = calculate_cost_usd({
        'input_tokens': result_updates['total_input_tokens'],
        'output_tokens': result_updates['total_output_tokens']
    })
    final_updates['estimated_cost_usd'] = estimated_cost
    
    # üõ°Ô∏è [Guard] Layer 1: Validate output keys (Reserved key check)
    validated_output = _validate_output_keys(final_updates, node_id)
    # üõ°Ô∏è [Guard] Layer 2: Schema validation (Type safety)
    return validate_state_with_schema(validated_output, node_id)


def loop_runner(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Executes a sequence of nodes repeatedly until a condition is met or max_iterations reached.
    üõ°Ô∏è [v3.8] Stage 6 Support: Convergence & Iterative Intelligence
    """
    node_id = config.get("id", "loop")
    # [Fix] Support both config structures with fallback pattern
    inner_config = config.get("config") or config
    
    # 1. Configuration
    sub_nodes = inner_config.get("nodes", [])
    condition = inner_config.get("condition", "false")
    max_iterations = inner_config.get("max_iterations", 5)
    loop_var = inner_config.get("loop_var", "loop_index")
    
    # Convergence Support (Stage 6)
    convergence_key = inner_config.get("convergence_key")
    target_score = inner_config.get("target_score", 0.9)
    
    logger.info(f"üîÅ [Loop] Starting loop node {node_id} (max_iterations: {max_iterations})")
    
    # [Cancellation]
    execution_arn = state.get("execution_arn") or state.get("ExecutionArn")
    
    # [Thread Safety] Deep copy to prevent mutation of original state across iterations
    # Note: Top-level state S3 hydration is already handled by segment_runner_service.execute_segment()
    import copy
    current_state = copy.deepcopy(state)
    
    all_loop_updates = {}
    
    total_input_tokens = 0
    total_output_tokens = 0
    
    from src.common.statebag import ensure_state_bag
    from src.services.operators.operator_strategies import SafeExpressionEvaluator
    
    # SafeExpressionEvaluator is created per-iteration with current state as context
    
    for i in range(max_iterations):
        if execution_arn and check_execution_cancelled(execution_arn):
            logger.warning(f"üö® [Loop] Execution cancelled, aborting at iteration {i}")
            break
            
        logger.info(f"üîÅ [Loop] Iteration {i+1}/{max_iterations}")
        
        # Update loop index in temporary state
        iter_updates = {loop_var: i}
        current_state.update(iter_updates)
        
        # Execute sub-nodes sequentially
        for node_def in sub_nodes:
            node_type = node_def.get("type", "operator")
            handler = NODE_REGISTRY.get(node_type)
            
            if not handler:
                logger.error(f"[Loop] Unknown node type '{node_type}'")
                continue
                
            try:
                # [Fix] Render node config based on current loop state
                rendered_node = _render_template(node_def, current_state)
                # Ensure unique nested ID
                rendered_node["id"] = f"{node_id}_it{i}_{node_def.get('id', 'sub')}"
                
                updates = handler(current_state, rendered_node)
                if isinstance(updates, dict):
                    # üõ°Ô∏è Recursive merge using StateBag logic behavior (mimicked here via update)
                    current_state.update(updates)
                    all_loop_updates.update(updates)
                    
                    # Accumulate tokens
                    usage = extract_token_usage(updates)
                    total_input_tokens += usage['input_tokens']
                    total_output_tokens += usage['output_tokens']
            except Exception as e:
                logger.error(f"‚ùå [Loop] Node {node_def.get('id')} failed in iteration {i}: {e}")
                all_loop_updates[f"{node_id}_error"] = str(e)
                return all_loop_updates

        # Update step history
        current_history = all_loop_updates.get("step_history", state.get("step_history", []))
        all_loop_updates["step_history"] = current_history + [f"{node_id}:iteration_{i}"]
        current_state["step_history"] = all_loop_updates["step_history"]

        # Check exit condition
        try:
            # 1. Logic-based condition
            # Create evaluator with current state as context
            evaluator = SafeExpressionEvaluator(current_state)
            should_exit = evaluator.evaluate(condition)
            if should_exit:
                logger.info(f"‚úÖ [Loop] Exit condition met at iteration {i}")
                all_loop_updates["loop_exit_reason"] = "condition_met"
                break
                
            # 2. Score-based convergence (Stage 6)
            if convergence_key:
                score = _get_nested_value(current_state, convergence_key)
                # [Fix] ÏàòÎ†¥ Ï†êÏàòÍ∞Ä NoneÏù¥Í±∞ÎÇò Ïà´ÏûêÍ∞Ä ÏïÑÎãå Í≤ΩÏö∞Ïóê ÎåÄÌïú Î∞©Ïñ¥ Î°úÏßÅ Í∞ïÌôî
                if isinstance(score, (int, float)) and score >= target_score:
                    logger.info(f"‚úÖ [Loop] Convergence reached (score: {score} >= {target_score})")
                    all_loop_updates["loop_exit_reason"] = "convergence_reached"
                    break
                elif score is None:
                    logger.debug(f"‚ÑπÔ∏è [Loop] Convergence key {convergence_key} is not yet available in state")
        except Exception as eval_err:
            logger.warning(f"‚ö†Ô∏è [Loop] Condition evaluation failed: {eval_err}")

    else:
        # Loop finished all iterations without exiting
        logger.warning(f"‚ö†Ô∏è [Loop] Max iterations ({max_iterations}) reached")
        all_loop_updates["loop_max_reached"] = True
        all_loop_updates["loop_exit_reason"] = "max_iterations"

    # [Token Aggregation]
    all_loop_updates['total_input_tokens'] = total_input_tokens
    all_loop_updates['total_output_tokens'] = total_output_tokens
    all_loop_updates['total_tokens'] = total_input_tokens + total_output_tokens
    
    # üõ°Ô∏è Result integrity
    final_updates = ensure_state_bag(all_loop_updates)
    
    # ÎπÑÏö© Í≥ÑÏÇ∞
    estimated_cost = calculate_cost_usd({
        'input_tokens': total_input_tokens,
        'output_tokens': total_output_tokens
    })
    final_updates['estimated_cost_usd'] = estimated_cost
    
    # üõ°Ô∏è [Guard] Layer 1: Validate output keys (Reserved key check)
    validated_output = _validate_output_keys(final_updates, node_id)
    # üõ°Ô∏è [Guard] Layer 2: Schema validation (Type safety)
    return validate_state_with_schema(validated_output, node_id)


def nested_for_each_runner(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Nested Map-in-Map Ï≤òÎ¶¨Î•º ÏúÑÌïú Ïû¨Í∑ÄÏ†Å ForEach Runner.
    
    V3 ÌïòÏù¥Ìçº-Ïä§Ìä∏Î†àÏä§ ÏãúÎÇòÎ¶¨Ïò§ ÏßÄÏõê:
    - 10Í∞ú Íµ≠Í∞Ä √ó 5Í∞ú ÏÇ∞ÏóÖÍµ∞ = 50Í∞ú Î≥ëÎ†¨ ÌÉúÏä§ÌÅ¨
    - Ï§ëÏ≤©Îêú Map Íµ¨Ï°∞ ÏûêÎèô Ï≤òÎ¶¨
    
    Config ÏòàÏãú:
    {
        "type": "nested_for_each",
        "input_list_key": "countries",          # Ïô∏Î∂Ä Î¶¨Ïä§Ìä∏
        "nested_config": {
            "input_list_key": "industries",     # ÎÇ¥Î∂Ä Î¶¨Ïä§Ìä∏ (item ÎÇ¥ ÏÜçÏÑ±)
            "sub_node_config": {...}            # ÏµúÏ¢Ö Ï≤òÎ¶¨ ÎÖ∏Îìú
        },
        "output_key": "analysis_results",
        "max_outer_iterations": 10,
        "max_inner_iterations": 5
    }
    """
    node_id = config.get("id", "nested_foreach")
    # [Fix] Support both flattened and nested config structures
    inner_config = config.get("config") or config
    input_list_key = inner_config.get("input_list_key", "")
    # [Fix] None defense: nested_config, metadataÍ∞Ä NoneÏùº Ïàò ÏûàÏùå
    nested_config = inner_config.get("nested_config") or {}
    output_key = inner_config.get("output_key", "nested_results")
    max_outer = inner_config.get("max_outer_iterations", 10)
    max_inner = inner_config.get("max_inner_iterations", 5)
    metadata = inner_config.get("metadata") or {}
    
    logger.info(f"üîÑ Nested ForEach starting: {node_id}")
    
    # [Critical Fix] Create immutable snapshot for thread-safe parallel execution
    import copy
    state_snapshot = copy.deepcopy(state)
    
    # Ïô∏Î∂Ä Î¶¨Ïä§Ìä∏ Í∞ÄÏ†∏Ïò§Í∏∞ - with S3 hydration support
    if "." in input_list_key:
        input_list_key = input_list_key.split(".")[-1]
    
    outer_list = _get_nested_value(state_snapshot, input_list_key, [])
    
    # [Critical Fix] Handle S3 offloaded outer_list
    if isinstance(outer_list, dict) and outer_list.get("__s3_offloaded"):
        logger.info(f"üíß [Nested ForEach] Hydrating outer list from S3")
        try:
            from src.services.state.state_manager import StateManager
            sm = StateManager()
            s3_path = outer_list.get("s3_path") or outer_list.get("__s3_path")
            if s3_path:
                hydrated_data = sm.download_state_from_s3(s3_path)
                outer_list = hydrated_data.get("results", hydrated_data) if isinstance(hydrated_data, dict) else hydrated_data
        except Exception as e:
            logger.error(f"‚ùå [Nested ForEach] S3 hydration failed: {e}")
            outer_list = []
    
    if not isinstance(outer_list, list):
        logger.warning(f"Nested ForEach: {input_list_key} is not a list")
        return {output_key: []}
    
    # Ïô∏Î∂Ä Ï†úÌïú Ï†ÅÏö©
    if len(outer_list) > max_outer:
        logger.warning(f"Nested ForEach: truncating outer {len(outer_list)} -> {max_outer}")
        outer_list = outer_list[:max_outer]
    
    # ÎÇ¥Î∂Ä ÏÑ§Ï†ï Ï∂îÏ∂ú
    inner_list_key = nested_config.get("input_list_key", "")
    # [Fix] None defense: nested_config['sub_node_config']Í∞Ä NoneÏùº Ïàò ÏûàÏùå
    sub_node_config = nested_config.get("sub_node_config") or {}
    
    if not sub_node_config:
        logger.error(f"Nested ForEach: missing sub_node_config")
        return {output_key: []}
    
    sub_node_type = sub_node_config.get("type", "llm")
    sub_node_func = NODE_REGISTRY.get(sub_node_type)
    
    if not sub_node_func:
        if sub_node_type == "llm":
            sub_node_func = llm_chat_runner
        else:
            raise ValueError(f"Unknown nested sub-node: {sub_node_type}")
    
    def process_outer_item(outer_idx: int, outer_item: Any) -> Dict[str, Any]:
        """Ïô∏Î∂Ä ÏïÑÏù¥ÌÖú Ï≤òÎ¶¨ (ÎÇ¥Î∂Ä Î¶¨Ïä§Ìä∏ Ìè¨Ìï®)"""
        outer_item_id = outer_item.get("id", f"outer_{outer_idx}") if isinstance(outer_item, dict) else f"outer_{outer_idx}"
        
        # ÎÇ¥Î∂Ä Î¶¨Ïä§Ìä∏ Ï∂îÏ∂ú
        if isinstance(outer_item, dict) and inner_list_key:
            # inner_list_keyÏóêÏÑú $. Ï†úÍ±∞
            clean_inner_key = inner_list_key.replace("$.", "").replace("$.item.", "")
            inner_list = outer_item.get(clean_inner_key, [])
        else:
            inner_list = []
        
        if not isinstance(inner_list, list):
            inner_list = [inner_list] if inner_list else []
        
        # ÎÇ¥Î∂Ä Ï†úÌïú Ï†ÅÏö©
        if len(inner_list) > max_inner:
            logger.debug(f"Nested ForEach [{outer_item_id}]: truncating inner {len(inner_list)} -> {max_inner}")
            inner_list = inner_list[:max_inner]
        
        inner_results = []
        
        def process_inner_item(inner_item: Any) -> Dict[str, Any]:
            """ÎÇ¥Î∂Ä ÏïÑÏù¥ÌÖú Ï≤òÎ¶¨"""
            # [Critical Fix] Use dict merge for true isolation instead of ChainMap
            # ChainMap with shared state causes race conditions
            item_state = {
                **state_snapshot,
                "outer_item": outer_item,
                "inner_item": inner_item,
                "item": inner_item,  # Í∏∞Ï°¥ Ìò∏ÌôòÏÑ± Ïú†ÏßÄ
                "parent": outer_item
            }
            
            # [Fix] Hydrate inner_item if it's S3 offloaded
            if isinstance(inner_item, dict) and inner_item.get("__s3_offloaded"):
                try:
                    from src.services.state.state_manager import StateManager
                    sm = StateManager()
                    s3_path = inner_item.get("s3_path") or inner_item.get("__s3_path")
                    if s3_path:
                        hydrated_item = sm.download_state_from_s3(s3_path)
                        item_state["inner_item"] = hydrated_item
                        item_state["item"] = hydrated_item
                except Exception as e:
                    logger.warning(f"[Nested ForEach] Failed to hydrate inner_item: {e}")
            
            # Î©îÏãúÏßÄ Î≥µÏÇ¨ (Î†àÏù¥Ïä§ Ïª®ÎîîÏÖò Î∞©ÏßÄ)
            if "messages" in item_state and isinstance(item_state["messages"], list):
                item_state["messages"] = item_state["messages"].copy()
            
            # ÏÑúÎ∏åÎÖ∏Îìú ÏÑ§Ï†ï Î†åÎçîÎßÅ
            c = sub_node_config.copy()
            c["id"] = f"{node_id}_{outer_item_id}_sub"
            rendered_sub = _render_template(c, item_state)
            
            try:
                return sub_node_func(item_state, rendered_sub)
            # [DISABLED] AsyncLLMRequiredException handling - Fargate async not implemented
            # except AsyncLLMRequiredException:
            #     # Bubble up to trigger PAUSED_FOR_ASYNC_LLM  
            #     raise
            except Exception as e:
                logger.error(f"Nested ForEach inner error [{outer_item_id}]: {e}")
                return {"error": str(e), "outer": outer_item_id}
        
        # ÎÇ¥Î∂Ä Î¶¨Ïä§Ìä∏ Î≥ëÎ†¨ Ï≤òÎ¶¨
        if inner_list:
            cpu_count = os.cpu_count() or 2
            inner_workers = min(len(inner_list), max(1, cpu_count // 4))  # Î≥¥ÏàòÏ†Å: 1/4 ÏΩîÏñ¥
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=inner_workers) as inner_executor:
                inner_results = list(inner_executor.map(process_inner_item, inner_list))
        
        return {
            "outer_id": outer_item_id,
            "outer_item": outer_item if isinstance(outer_item, dict) else {"value": outer_item},
            "inner_count": len(inner_list),
            "inner_results": inner_results
        }
    
    # Ïô∏Î∂Ä Î¶¨Ïä§Ìä∏ Î≥ëÎ†¨ Ï≤òÎ¶¨
    all_results = []
    if outer_list:
        cpu_count = os.cpu_count() or 2
        outer_workers = min(len(outer_list), max(1, cpu_count // 2))
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=outer_workers) as outer_executor:
            futures = [
                outer_executor.submit(process_outer_item, idx, item)
                for idx, item in enumerate(outer_list)
            ]
            for future in concurrent.futures.as_completed(futures):
                try:
                    result = future.result()
                    all_results.append(result)
                except Exception as e:
                    logger.error(f"Nested ForEach outer error: {e}")
                    all_results.append({"error": str(e)})
    
    # Í≤∞Í≥º ÏßëÍ≥Ñ
    total_inner_processed = sum(r.get("inner_count", 0) for r in all_results)
    logger.info(f"‚úÖ Nested ForEach complete: {len(all_results)} outer √ó {total_inner_processed} inner tasks")
    
    # [Token Aggregation] Ï§ëÏ≤© for_eachÏùò ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ Ìï©ÏÇ∞ (Ïû¨Í∑ÄÏ†Å Ï≤òÎ¶¨)
    usage = aggregate_tokens_from_nested(all_results)
    total_input_tokens = usage['input_tokens']
    total_output_tokens = usage['output_tokens']
    total_tokens = usage['total_tokens']
    
    # ÏÉÅÏÑ∏ ÌÜ†ÌÅ∞ Ï†ïÎ≥¥ ÏàòÏßë (ÎîîÎ≤ÑÍπÖÏö©, Ìï©ÏÇ∞Í≥º Î≥ÑÎèÑ)
    nested_token_details = []
    for outer_result in all_results:
        if isinstance(outer_result, dict):
            outer_id = outer_result.get('outer_id', 'unknown')
            inner_results = outer_result.get('inner_results', [])
            
            for inner_result in inner_results:
                if isinstance(inner_result, dict):
                    inner_usage = extract_token_usage(inner_result)
                    
                    nested_token_details.append({
                        'outer_id': outer_id,
                        'input_tokens': inner_usage['input_tokens'],
                        'output_tokens': inner_usage['output_tokens'],
                        'total_tokens': inner_usage['total_tokens']
                    })
    
    # Ìï©ÏÇ∞Îêú ÌÜ†ÌÅ∞ Ï†ïÎ≥¥Î•º Í≤∞Í≥ºÏóê Ï∂îÍ∞Ä
    result_updates = {
        output_key: all_results,
        f"{output_key}_summary": {
            "outer_count": len(all_results),
            "total_inner_count": total_inner_processed,
            "node_id": node_id
        }
    }
    result_updates['total_input_tokens'] = total_input_tokens
    result_updates['total_output_tokens'] = total_output_tokens
    result_updates['total_tokens'] = total_tokens
    result_updates['nested_token_details'] = nested_token_details
    
    # [Accumulation] Ïù¥Ï†Ñ ÏÉÅÌÉúÏùò ÌÜ†ÌÅ∞ Í∞íÍ≥º ÎàÑÏ†Å
    temp_result = {
        'total_input_tokens': result_updates['total_input_tokens'],
        'total_output_tokens': result_updates['total_output_tokens'],
        'total_tokens': result_updates['total_tokens']
    }
    accumulated_result = accumulate_tokens_in_state(temp_result, state)
    result_updates.update(accumulated_result)
    
    # ÎπÑÏö© Í≥ÑÏÇ∞ Ï∂îÍ∞Ä
    estimated_cost = calculate_cost_usd({
        'input_tokens': result_updates['total_input_tokens'],
        'output_tokens': result_updates['total_output_tokens']
    })
    result_updates['estimated_cost_usd'] = estimated_cost
    
    logger.info(f"Nested ForEach {node_id}: Processed {len(nested_token_details)} nested iterations, "
                f"total tokens: {result_updates['total_tokens']} "
                f"({total_input_tokens} input + {total_output_tokens} output), "
                f"cost: ${estimated_cost:.6f}")
    
    # üõ°Ô∏è [Guard] Layer 1: Validate output keys (Reserved key check)
    validated_output = _validate_output_keys(result_updates, node_id)
    # üõ°Ô∏è [Guard] Layer 2: Schema validation (Type safety)
    return validate_state_with_schema(validated_output, node_id)


def route_draft_quality(state: Dict[str, Any]) -> str:
    draft = state.get("gemini_draft")
    if not isinstance(draft, dict) or not draft.get("is_complete"):
        return "reviser"
    return "send_email"


def parallel_group_runner(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """Enhanced Parallel Runner supporting sub_workflows in branches."""
    node_id = config.get("id", "parallel")
    # [Fix] Support both config structures with fallback pattern
    inner_config = config.get("config") or config
    branches = inner_config.get("branches", [])
    
    if not branches:
        return {}

    # [Thread Safety] Create immutable snapshot for thread-safe parallel execution
    # Note: Top-level state S3 hydration is already handled by segment_runner_service.execute_segment()
    import copy
    state_snapshot = copy.deepcopy(state)

    def run_branch(branch):
        branch_id = branch.get("branch_id", "sub")
        # [Fix] ÏùºÍ¥ÄÏÑ±ÏùÑ ÏúÑÌï¥ branches ÎÇ¥Î∂ÄÏóêÏÑúÎèÑ nodes Î¶¨Ïä§Ìä∏ÎÇò sub_workflow ÏßÄÏõê
        branch_nodes = branch.get("nodes")
        if not branch_nodes and "sub_workflow" in branch:
            branch_def = branch["sub_workflow"]
            if isinstance(branch_def, dict):
                branch_nodes = branch_def.get("nodes", [])
        
        # [Critical Fix] Use dict merge for true isolation instead of ChainMap
        # ChainMap with shared state causes race conditions in ThreadPoolExecutor
        b_state = {**state_snapshot}
        b_updates = {}
        
        for n_def in (branch_nodes or []):
            # [Fix] Î∏åÎûúÏπò ÎÇ¥ ÎÖ∏Îìú Í∞Ñ ÏÉÅÌÉú Í≥µÏú†Î•º ÏúÑÌï¥ ÏóÖÎç∞Ïù¥Ìä∏ Î≥ëÌï©
            current_view = {**b_state, **b_updates}
            node_type = n_def.get("type", "operator")
            handler = NODE_REGISTRY.get(node_type)
            
            if handler:
                try:
                    # Execute node within branch
                    res = handler(current_view, n_def)
                    if isinstance(res, dict):
                        b_updates.update(res)
                except Exception as e:
                    logger.error(f"Node execution failed in branch {branch_id}: {e}")
                    raise e
                    
        return branch_id, b_updates

    # Execute branches in parallel
    combined_updates = {}
    branch_results = {}  # [Fix] Track branch results explicitly
    
    # Use ThreadPoolExecutor for concurrency
    # Note: Be careful with state conflicts if branches write to same keys
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future_to_branch = {executor.submit(run_branch, b): b for b in branches}
        for future in concurrent.futures.as_completed(future_to_branch):
            branch = future_to_branch[future]
            try:
                branch_id, updates = future.result()
                # [Fix] Store branch results explicitly for verification
                branch_results[branch_id] = updates
                # [Fix] Namespace results to prevent race conditions (Last-Write-Wins)
                # Downstream nodes must access results via branch_id
                combined_updates[branch_id] = updates
                # [Optimized] Flattening removed to prevent conflicts: combined_updates.update(updates)
            except Exception as e:
                logger.error(f"Branch execution failed: {e}")
                raise e
    
    # [Fix] Add explicit branch execution markers for test verification
    for branch_id in branch_results.keys():
        combined_updates[f"{branch_id}_executed"] = True
    
    # [Token Aggregation] Î≥ëÎ†¨ Î∏åÎûúÏπòÎì§Ïùò ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ Ìï©ÏÇ∞
    total_input_tokens = 0
    total_output_tokens = 0
    branch_token_details = []
    
    for branch_id, updates in branch_results.items():
        # Í∞Å Î∏åÎûúÏπòÏùò ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ Ï∂îÏ∂ú (usage ÎòêÎäî token_usage ÌÇ§ ÏßÄÏõê)
        usage = extract_token_usage(updates)
        input_tokens = usage['input_tokens']
        output_tokens = usage['output_tokens']
        
        total_input_tokens += input_tokens
        total_output_tokens += output_tokens
        
        branch_token_details.append({
            'branch_id': branch_id,
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'total_tokens': input_tokens + output_tokens
        })
    
    # Ìï©ÏÇ∞Îêú ÌÜ†ÌÅ∞ Ï†ïÎ≥¥Î•º stateÏóê Í∏∞Î°ù
    combined_updates['total_input_tokens'] = total_input_tokens
    combined_updates['total_output_tokens'] = total_output_tokens
    combined_updates['total_tokens'] = total_input_tokens + total_output_tokens
    combined_updates['branch_token_details'] = branch_token_details
    
    # [Accumulation] Ïù¥Ï†Ñ ÏÉÅÌÉúÏùò ÌÜ†ÌÅ∞ Í∞íÍ≥º ÎàÑÏ†Å
    temp_result = {
        'total_input_tokens': combined_updates['total_input_tokens'],
        'total_output_tokens': combined_updates['total_output_tokens'],
        'total_tokens': combined_updates['total_tokens']
    }
    accumulated_result = accumulate_tokens_in_state(temp_result, state)
    combined_updates.update(accumulated_result)
    
    # ÎπÑÏö© Í≥ÑÏÇ∞ Ï∂îÍ∞Ä
    estimated_cost = calculate_cost_usd({
        'input_tokens': combined_updates['total_input_tokens'],
        'output_tokens': combined_updates['total_output_tokens']
    })
    combined_updates['estimated_cost_usd'] = estimated_cost
    
    logger.info(f"Parallel group {node_id}: Aggregated {len(branch_results)} branches, "
                f"total tokens: {combined_updates['total_tokens']} "
                f"({total_input_tokens} input + {total_output_tokens} output), "
                f"cost: ${estimated_cost:.6f}")
    
    # [Debug] Log the keys being returned
    logger.info(f"Parallel group {node_id} returning keys: {list(combined_updates.keys())}")
    
    # [Payload Optimization] Keep flattened structure to avoid 256KB Step Functions limit
    # Aggregators must query individual branch IDs directly from state
    # üõ°Ô∏è [Guard] Layer 1: Validate output keys (Reserved key check)
    validated_output = _validate_output_keys(combined_updates, node_id)
    # üõ°Ô∏è [Guard] Layer 2: Schema validation (Type safety)
    return validate_state_with_schema(validated_output, node_id)


# -----------------------------------------------------------------------------
# Vision Runner - Gemini Vision Multimodal Analysis
# -----------------------------------------------------------------------------

def vision_runner(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Gemini VisionÏùÑ ÌôúÏö©Ìïú Ïù¥ÎØ∏ÏßÄ/ÎπÑÎîîÏò§ Î∂ÑÏÑù Runner.
    
    ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÎÖ∏ÎìúÏóêÏÑú Î©ÄÌã∞Î™®Îã¨ Î∂ÑÏÑùÏùÑ ÏàòÌñâÌï©ÎãàÎã§:
    - Ïù¥ÎØ∏ÏßÄÏóêÏÑú ÌÖçÏä§Ìä∏/Ïä§Ìéô Ï∂îÏ∂ú (OCR)
    - Ï†úÌíà Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù
    - Ïä§ÌÅ¨Î¶∞ÏÉ∑ Ìï¥ÏÑù
    - Îã§Ïù¥Ïñ¥Í∑∏Îû®/Ï∞®Ìä∏ Î∂ÑÏÑù
    
    Config Options:
        image_inputs: List[str] - Ïù¥ÎØ∏ÏßÄ ÏÜåÏä§ Î¶¨Ïä§Ìä∏ (S3 URI, HTTP URL, state ÌÇ§)
        prompt_content: str - Î∂ÑÏÑù ÌîÑÎ°¨ÌîÑÌä∏ (ÌÖúÌîåÎ¶ø ÏßÄÏõê)
        system_prompt: str - ÏãúÏä§ÌÖú ÏßÄÏπ®
        output_key: str - Í≤∞Í≥º Ï†ÄÏû• ÌÇ§
        max_tokens: int - ÏµúÎåÄ Ï∂úÎ†• ÌÜ†ÌÅ∞
        temperature: float - ÏÉòÌîåÎßÅ Ïò®ÎèÑ
        
    Example Config:
        {
            "type": "vision",
            "config": {
                "image_inputs": ["{{product_image_s3_uri}}", "{{spec_sheet_url}}"],
                "prompt_content": "Ïù¥ Ï†úÌíà Ïù¥ÎØ∏ÏßÄÎì§ÏóêÏÑú Ïä§ÌéôÏùÑ JSONÏúºÎ°ú Ï∂îÏ∂úÌï¥Ï£ºÏÑ∏Ïöî",
                "output_key": "extracted_specs"
            }
        }
    """
    # 1. Node ID and config extraction
    node_id = config.get("id", "vision")
    
    # [Fix] Config Extraction: Support nested 'config' dict (Workflow JSON standard) vs Flat dict (Test/Legacy)
    # Prioritize inner 'config' if present, otherwise fall back to root config
    vision_config = config.get("config", config) if isinstance(config.get("config"), dict) else config
    
    # 2. Hydrate state
    exec_state = _hydrate_state_for_config(state, vision_config)
    media_inputs = []
    
    # process image_inputs
    raw_image_inputs = vision_config.get("image_inputs", [])
    if isinstance(raw_image_inputs, str): raw_image_inputs = [raw_image_inputs]
    
    for img_input in raw_image_inputs:
        resolved = _render_template(img_input, exec_state)
        # Check if state key reference
        if resolved and not resolved.startswith(("s3://", "gs://", "http://", "https://", "data:")):
            state_val = exec_state.get(resolved)
            if state_val: resolved = state_val
        
        if resolved:
            media_inputs.append({"type": "image", "source": resolved})
            
    # process video_inputs
    raw_video_inputs = vision_config.get("video_inputs", [])
    if isinstance(raw_video_inputs, str): raw_video_inputs = [raw_video_inputs]
    
    for vid_input in raw_video_inputs:
        resolved = _render_template(vid_input, exec_state)
        # Check if state key reference
        if resolved and not resolved.startswith(("s3://", "gs://", "http://", "https://")):
             state_val = exec_state.get(resolved)
             if state_val: resolved = state_val
             
        if resolved:
            media_inputs.append({"type": "video", "source": resolved})
    
    if not media_inputs:
        logger.warning(f"No media sources resolved for vision node {node_id}")
        out_key = vision_config.get("output_key", f"{node_id}_output")
        return {out_key: "[Error: No media provided]", "step_history": state.get("step_history", []) + [f"{node_id}:no_media"]}
    
    # 3. Render prompt
    prompt_template = vision_config.get("prompt_content") or vision_config.get("user_prompt_template", "Ïù¥ Ïª®ÌÖêÏ∏†Î•º Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî.")
    prompt = _render_template(prompt_template, exec_state)
    
    system_prompt_tmpl = vision_config.get("system_prompt", "")
    system_prompt = _render_template(system_prompt_tmpl, exec_state) if system_prompt_tmpl else None
    
    # 4. Get model config
    max_tokens = vision_config.get("max_tokens", DEFAULT_MAX_TOKENS)
    temperature = vision_config.get("temperature", DEFAULT_TEMPERATURE)
    
    # 5. Invoke Gemini Vision (Multimodal)
    try:
        from src.services.llm.gemini_service import GeminiService, GeminiConfig, GeminiModel
        
        # Vision ÏßÄÏõê Î™®Îç∏ ÏÇ¨Ïö©
        model_name = vision_config.get("model", "gemini-1.5-flash")
        # Model selection logic... (simplified mapping)
        model_enum = GeminiModel.GEMINI_1_5_FLASH
        if "pro" in model_name: model_enum = GeminiModel.GEMINI_1_5_PRO
        elif "2.0" in model_name: model_enum = GeminiModel.GEMINI_2_0_FLASH
        
        gemini_config = GeminiConfig(
            model=model_enum,
            max_output_tokens=max_tokens,
            temperature=temperature,
            system_instruction=system_prompt
        )
        
        service = GeminiService(config=gemini_config)
        
        logger.info(f"Vision runner invoking Multimodal Gemini with {len(media_inputs)} inputs")
        
        result = service.invoke_multimodal(
            user_prompt=prompt,
            media_inputs=media_inputs,
            system_instruction=system_prompt
        )
        
        # Extract text from response
        text = ""
        if "content" in result and result["content"]:
            # Handle list or dict content
            content_data = result["content"]
            if isinstance(content_data, list) and content_data:
                text = content_data[0].get("text", "")
            elif isinstance(content_data, dict):
                text = content_data.get("text", "")
        
        metadata = result.get("metadata", {})
        
        # Count images and videos separately
        image_count = sum(1 for m in media_inputs if m.get("type") == "image")
        video_count = sum(1 for m in media_inputs if m.get("type") == "video")
        
        # Update history
        current_history = state.get("step_history", [])
        new_history = current_history + [f"{node_id}:vision_analysis"]
        
        out_key = vision_config.get("output_key", f"{node_id}_output")
        
        raw_output = {
            out_key: text,
            f"{node_id}_meta": {
                "model": model_name,
                "image_count": image_count,
                "video_count": video_count,
                "total_media": len(media_inputs),
                "token_usage": metadata.get("token_usage", {}),
                "latency_ms": metadata.get("latency_ms", 0)
            },
            "step_history": new_history
        }
        
        # üõ°Ô∏è [Guard] Layer 1: Validate output keys (Reserved key check)
        validated_output = _validate_output_keys(raw_output, node_id)
        # üõ°Ô∏è [Guard] Layer 2: Schema validation (Type safety)
        return validate_state_with_schema(validated_output, node_id)
        
    except Exception as e:
        logger.exception(f"Vision runner failed for node {node_id}: {e}")
        out_key = vision_config.get("output_key", f"{node_id}_output")
        return {
            out_key: f"[Vision Error: {str(e)}]",
            "step_history": state.get("step_history", []) + [f"{node_id}:error"]
        }


def video_chunker_runner(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Splits video into chunks using VideoChunkerService.
    """
    from src.services.media.video_chunker import VideoChunkerService
    
    node_id = config.get("id", "chunker")
    # [Fix] Support both flattened and nested config structures
    inner_config = config.get("config") or config
    exec_state = _hydrate_state_for_config(state, inner_config)
    
    video_uri = _render_template(inner_config.get("video_uri", ""), exec_state)
    segment_min = inner_config.get("segment_length_min", 5)
    output_key = inner_config.get("output_key", "video_chunks")
    
    if not video_uri:
        raise ValueError("video_chunker requires 'video_uri'")
        
    service = VideoChunkerService()
    chunks = service.chunk_video(video_uri, segment_length_min=segment_min)
    
    return {output_key: chunks, f"{node_id}_status": "done"}


# -----------------------------------------------------------------------------
# 5. Registry & Orchestration
# -----------------------------------------------------------------------------

NODE_REGISTRY: Dict[str, Callable] = {}

def register_node(name: str, func: Callable) -> None:
    NODE_REGISTRY[name] = func

# Register Nodes
register_node("operator", operator_runner)
register_node("operator_custom", operator_runner)  # ÏÇ¨Ïö©Ïûê Ï†ïÏùò ÏΩîÎìú/sets Ï†ÑÏö© (MOCK_MODEÏóêÏÑúÎßå exec ÌóàÏö©)

# -----------------------------------------------------------------------------
# Safe Operator Official Runner - Production-ready built-in transformations
# -----------------------------------------------------------------------------
def operator_official_runner(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Execute safe, built-in transformation strategies without exec().
    
    Production-ready operator that provides 50+ transformation strategies:
    - JSON/Object: json_parse, deep_get, pick_fields, merge_objects, etc.
    - List: list_filter, list_map, list_reduce, list_sort, etc.
    - String: string_template, regex_extract, string_case, etc.
    - Type: to_int, to_date, coerce_type, etc.
    - Control: if_else, switch_case, default_value, etc.
    - Encoding: base64_encode, url_encode, hash_sha256, uuid_generate, etc.
    - Math: math_round, math_clamp, math_expression, etc.
    
    Config schema:
    {
        "type": "operator_official",
        "id": "my_transform",
        "config": {
            "strategy": "list_filter",       # Required: strategy name
            "input": "{{items}}",            # Optional: input template (defaults to state)
            "input_key": "items",            # Alternative: direct state key
            "params": {                       # Strategy-specific parameters
                "condition": "$.active == true"
            },
            "output_key": "filtered_items"   # Optional: output key (defaults to {node_id}_result)
        }
    }
    """
    from src.services.operators.operator_strategies import execute_strategy, get_available_strategies
    
    node_id = config.get("id", "operator_official")
    
    # Extract config (support both flat and nested config with fallback)
    inner_config = config.get("config") or config
    strategy = inner_config.get("strategy")
    
    if not strategy:
        raise ValueError(
            f"operator_official node '{node_id}' requires 'strategy'. "
            f"Available: {get_available_strategies()}"
        )
    
    # Resolve input
    input_template = inner_config.get("input")
    input_key = inner_config.get("input_key")
    
    if input_template:
        # Render template against state
        input_value = _render_template(input_template, state)
    elif input_key:
        # Direct state key access
        input_value = _get_nested_value(state, input_key)
    else:
        # Use entire state as input
        input_value = state
    
    # Get strategy parameters
    params = inner_config.get("params", {})
    
    # Render any templates in params
    if isinstance(params, dict):
        params = _render_template(params, state)
    
    # Execute strategy
    try:
        result = execute_strategy(strategy, input_value, params, state)
    except AssertionError as e:
        # Assert strategy failed - propagate as error
        raise ValueError(f"Assertion failed in node '{node_id}': {e}")
    except Exception as e:
        # Check for fallback handling
        error_handling = inner_config.get("error_handling", "fail")
        fallback = inner_config.get("fallback")
        
        if error_handling == "fallback" and fallback is not None:
            logger.warning(f"[operator_official] {node_id} failed, using fallback: {e}")
            result = _render_template(fallback, state) if isinstance(fallback, str) else fallback
        elif error_handling == "skip":
            logger.warning(f"[operator_official] {node_id} failed, skipping: {e}")
            return {f"{node_id}_skipped": True, f"{node_id}_error": str(e)}
        else:
            raise
    
    # Build output
    output_key = inner_config.get("output_key") or f"{node_id}_result"
    
    output = {output_key: result}
    
    # Update step history
    current_history = state.get("step_history", [])
    output["step_history"] = current_history + [f"{node_id}:operator_official:{strategy}"]
    
    # üõ°Ô∏è [Guard] Layer 1: Validate output keys (Reserved key check)
    validated_output = _validate_output_keys(output, node_id)
    # üõ°Ô∏è [Guard] Layer 2: Schema validation (Type safety)
    return validate_state_with_schema(validated_output, node_id)

register_node("operator_official", operator_official_runner)
register_node("safe_operator", operator_official_runner)  # Alias for operator_official
register_node("llm_chat", llm_chat_runner)
register_node("video_chunker", video_chunker_runner)
register_node("aiModel", llm_chat_runner)  # aiModelÏùÄ llm_chatÍ≥º ÎèôÏùºÌïòÍ≤å Ï≤òÎ¶¨
register_node("api_call", api_call_runner)
register_node("db_query", db_query_runner)
register_node("for_each", for_each_runner)
register_node("loop", loop_runner)  # Convergence support (v3.8)
register_node("route_draft_quality", route_draft_quality)
register_node("parallel_group", parallel_group_runner)
register_node("parallel", parallel_group_runner)  # Alias for backward compat
register_node("aggregator", aggregator_runner)
register_node("skill_executor", skill_executor_runner)  # Skills integration
register_node("nested_for_each", nested_for_each_runner)  # V3 Hyper-Stress: Nested Map-in-Map support
register_node("vision", vision_runner)  # Gemini Vision multimodal analysis
register_node("image_analysis", vision_runner)  # Alias for vision
# Note: 'code' ÌÉÄÏûÖÏùÄ NODE_REGISTRYÏóê Ï∂îÍ∞ÄÌïòÏßÄ ÏïäÏùå
# Pydantic field_validatorÏóêÏÑú 'code' -> 'operator'Î°ú Î≥ÄÌôòÎêòÎØÄÎ°ú Ïó¨Í∏∞ ÎèÑÎã¨ Î∂àÍ∞Ä
# ÎßåÏïΩ 'code' ÌÉÄÏûÖÏù¥ Ïó¨Í∏∞ ÎèÑÎã¨ÌïòÎ©¥ Í≤ÄÏ¶ù Îã®Í≥ÑÎ•º Ïö∞ÌöåÌïú Í≤ÉÏù¥ÎØÄÎ°ú ÏóêÎü¨Í∞Ä ÎßûÏùå

# SubGraph/Group ÎÖ∏Îìú Îü¨ÎÑà - DynamicWorkflowBuilderÏóêÏÑú Ïû¨Í∑ÄÏ†ÅÏúºÎ°ú Ï≤òÎ¶¨
def subgraph_runner(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """
    SubGraph/Group ÎÖ∏Îìú Ïã§Ìñâ Ìï∏Îì§Îü¨.
    
    Ïã§Ï†ú ÏÑúÎ∏åÍ∑∏ÎûòÌîÑ Ïª¥ÌååÏùº Î∞è Ïã§ÌñâÏùÄ DynamicWorkflowBuilderÏóêÏÑú Ï≤òÎ¶¨Îê©ÎãàÎã§.
    Ïù¥ Ìï∏Îì§Îü¨Îäî ÏÑ∏Í∑∏Î®ºÌä∏ Îü¨ÎÑàÏóêÏÑú ÏßÅÏ†ë Ìò∏Ï∂úÎê† ÎïåÎ•º ÏúÑÌïú Ìè¥Î∞±ÏûÖÎãàÎã§.
    
    Config ÏòµÏÖò:
    - subgraph_ref: Ï∞∏Ï°∞Ìï† ÏÑúÎ∏åÍ∑∏ÎûòÌîÑ ID
    - subgraph_inline: Ïù∏ÎùºÏù∏ ÏÑúÎ∏åÍ∑∏ÎûòÌîÑ Ï†ïÏùò
    - skill_ref: Ï∞∏Ï°∞Ìï† Skill ID
    - input_mapping: Î∂ÄÎ™®‚ÜíÏûêÏãù ÏÉÅÌÉú Îß§Ìïë
    - output_mapping: ÏûêÏãù‚ÜíÎ∂ÄÎ™® ÏÉÅÌÉú Îß§Ìïë
    """
    node_id = config.get("id", "subgraph")
    # [Fix] Support both flattened and nested config structures
    inner_config = config.get("config") or config
    logger.info(f"üì¶ SubGraph ÎÖ∏Îìú Ïã§Ìñâ: {node_id}")
    
    try:
        # DynamicWorkflowBuilder import
        from src.services.workflow.builder import DynamicWorkflowBuilder
        
        # ÏÑúÎ∏åÍ∑∏ÎûòÌîÑ Ï†ïÏùò Ìï¥ÏÑù
        subgraph_def = None
        
        if inner_config.get("subgraph_inline"):
            subgraph_def = inner_config["subgraph_inline"]
        elif inner_config.get("subgraph_ref"):
            # subgraph_refÎäî ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ïª®ÌÖçÏä§Ìä∏ÏóêÏÑú Ìï¥ÏÑùÎêòÏñ¥Ïïº Ìï®
            # Ïó¨Í∏∞ÏÑúÎäî stateÏóêÏÑú subgraphsÎ•º Ï∞æÏùå
            subgraphs = state.get("_workflow_subgraphs", {})
            ref = inner_config["subgraph_ref"]
            if ref in subgraphs:
                subgraph_def = subgraphs[ref]
            else:
                logger.warning(f"SubGraph Ï∞∏Ï°∞ '{ref}'Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
                return {"subgraph_error": f"SubGraph ref not found: {ref}"}
        elif inner_config.get("skill_ref"):
            # Skill Í∏∞Î∞ò ÏÑúÎ∏åÍ∑∏ÎûòÌîÑ
            try:
                from src.services.skill_repository import get_skill_repository
                repo = get_skill_repository()
                skill = repo.get_latest_skill(inner_config["skill_ref"])
                if skill and skill.get("skill_type") == "subgraph_based":
                    subgraph_def = skill.get("subgraph_config")
            except ImportError:
                logger.warning("SkillRepositoryÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§.")
        
        if not subgraph_def:
            logger.warning(f"SubGraph Ï†ïÏùòÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {node_id}")
            return {"subgraph_status": "skipped", "reason": "no_definition"}
        
        # ÏûÖÎ†• Îß§Ìïë Ï†ÅÏö©
        input_mapping = config.get("input_mapping", {})
        child_state = {}
        for parent_key, child_key in input_mapping.items():
            if parent_key in state:
                child_state[child_key] = state[parent_key]
        
        # Í∏∞Î≥∏ ÌïÑÎìú ÏÉÅÏÜç
        for key in ["execution_id", "workflow_id", "owner_id"]:
            if key in state and key not in child_state:
                child_state[key] = state[key]
        
        # ÏÑúÎ∏åÍ∑∏ÎûòÌîÑ ÎπåÎìú Î∞è Ïã§Ìñâ
        builder = DynamicWorkflowBuilder(subgraph_def, use_lightweight_state=True)
        compiled = builder.build()
        child_output = compiled.invoke(child_state)
        
        # Ï∂úÎ†• Îß§Ìïë Ï†ÅÏö©
        output_mapping = config.get("output_mapping", {})
        result = {}
        for child_key, parent_key in output_mapping.items():
            if child_key in child_output:
                result[parent_key] = child_output[child_key]
        
        # step_history Î≥ëÌï©
        if "step_history" in child_output:
            current_history = state.get("step_history", [])
            result["step_history"] = current_history + child_output["step_history"]
        
        logger.info(f"‚úÖ SubGraph ÎÖ∏Îìú ÏôÑÎ£å: {node_id}")
        return result
        
    except Exception as e:
        logger.exception(f"‚ùå SubGraph ÎÖ∏Îìú Ïã§Ìñâ Ïã§Ìå®: {node_id}")
        error_handling = config.get("error_handling", "fail")
        if error_handling == "ignore":
            return {"subgraph_status": "error_ignored", "error": str(e)}
        elif error_handling == "fallback":
            return {"subgraph_status": "fallback", "error": str(e)}
        else:
            raise

register_node("group", subgraph_runner)  # SubGraph ÎÖ∏Îìú (group ÌÉÄÏûÖ)
register_node("subgraph", subgraph_runner)  # SubGraph ÎÖ∏Îìú (subgraph ÌÉÄÏûÖ)


def _get_mock_config(mock_behavior: str) -> Dict[str, Any]:
    """Returns test configurations for mock behaviors."""
    if mock_behavior == "E2E_S3_LARGE_DATA":
        return {
            "nodes": [{
                "id": "large_data_generator", "type": "operator",
                "config": { "code": "state['res'] = 'X'*300000", "output_key": "res" }
            }],
            "edges": [], "start_node": "large_data_generator"
        }
    elif mock_behavior == "CONTINUE":
        return {
            "nodes": [
                {"id": "step1", "type": "operator", "config": {"sets": {"step": 1}}},
                {"id": "step2", "type": "operator", "config": {"sets": {"step": 2}}}
            ],
            "edges": [{"source": "step1", "target": "step2"}], "start_node": "step1"
        }
    # Add other mock configs as needed (FAIL, PAUSE, etc.)
    return {"nodes": [], "edges": []} # Default empty


def run_workflow(config_json: str | Dict[str, Any], initial_state: Dict[str, Any] | None = None, 
                 user_api_keys: Dict[str, str] | None = None, 
                 use_cache: bool = True, 
                 conversation_id: str | None = None, 
                 ddb_table_name: str | None = None,
                 run_config: Dict[str, Any] | None = None) -> Dict[str, Any]:
    """
    Main entry point using Dynamic Builder architecture.
    Supports both Real and Mock execution paths.
    """
    initial_state = dict(initial_state) if initial_state else {}
    
    # 1. Check for Mock/Test Request
    mock_behavior = initial_state.get("mock_behavior")
    if mock_behavior:
        logger.info(f"üß™ Mock behavior detected: {mock_behavior}")
        mock_config = _get_mock_config(mock_behavior)
        config_json = json.dumps(mock_config)
        
        # Mock Response Simulation for HITP
        if mock_behavior == "PAUSED_FOR_HITP":
            return {"status": "PAUSED_FOR_HITP", "next_segment_to_run": 1}

    # 2. Config Validation (JSON parse + Pydantic schema)
    if isinstance(config_json, dict):
        raw_config = config_json
    else:
        try:
            raw_config = json.loads(config_json)
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON config: {str(e)}. Please check the config_json format.")
        except Exception as e:
            raise ValueError(f"Failed to parse workflow config: {str(e)}")

    try:
        validated_config = WorkflowConfigModel.model_validate(raw_config)
        # Use validated dict for downstream (keeps types constrained)
        workflow_config = validated_config.model_dump()
    except ValidationError as ve:
        raise ValueError(f"Invalid workflow config: {ve}") from ve

    # 3. Dynamic Build (No S3, No Pickle)
    # Lazy import to avoid circular ref with NODE_REGISTRY
    from src.services.workflow.builder import DynamicWorkflowBuilder
    
    logger.info("üèóÔ∏è Building workflow dynamically...")
    builder = DynamicWorkflowBuilder(workflow_config)
    app = builder.build()

    # 4. Apply Checkpointer if needed
    if ddb_table_name:
        try:
            from langgraph_checkpoint_dynamodb import DynamoDBSaver
            saver = DynamoDBSaver(table_name=ddb_table_name)
            app = app.with_checkpointer(saver)
        except ImportError:
            logger.warning("DynamoDBSaver not found, skipping persistence")

    # 5. Execution
    # [ÏàòÏ†ï] run_configÍ∞Ä ÏóÜÏúºÎ©¥ Îπà ÎîïÏÖîÎÑàÎ¶¨Î°ú Ï¥àÍ∏∞Ìôî
    final_config = run_config.copy() if run_config else {}
    
    # configurableÏù¥ ÏóÜÏúºÎ©¥ ÏÉùÏÑ±
    if "configurable" not in final_config:
        final_config["configurable"] = {}
    
    # thread_id Î∞è conversation_id Î≥¥Ï†ï
    configurable = final_config["configurable"]
    if not configurable.get("thread_id"):
        configurable["thread_id"] = conversation_id or "default_thread"
    if conversation_id and not configurable.get("conversation_id"):
        configurable["conversation_id"] = conversation_id

    # Setup API Keys
    if user_api_keys:
        initial_state.setdefault("user_api_keys", {}).update(user_api_keys)
    initial_state.setdefault("step_history", [])

    # 6. Setup Callbacks (Glass Box)
    from src.langchain_core_custom.callbacks import BaseCallbackHandler
    import uuid
    
    class StateHistoryCallback(BaseCallbackHandler):
        """
        Capture AI thoughts and tool usage with PII masking for Glass-Box UI.
        """
        def __init__(self):
            self.logs = []
            
        def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any) -> None:
            pass
            
        def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
            prompt_safe = mask_pii(prompts[0]) if prompts else ""
            self.logs.append({
                "id": str(uuid.uuid4()),
                "type": "ai_thought",
                "name": serialized.get("name", "LLM Thinking"),
                "node_id": serialized.get("name", "llm_node"),
                "content": "Thinking...",
                "timestamp": int(time.time()),
                "status": "RUNNING",
                "details": {
                    "prompts": [prompt_safe] if prompt_safe else []
                }
            })
            
        def on_llm_end(self, response: Any, **kwargs: Any) -> None:
            if self.logs and self.logs[-1]["type"] == "ai_thought":
                self.logs[-1]["status"] = "COMPLETED"
                
                if getattr(response, "llm_output", None) and "usage" in response.llm_output:
                    self.logs[-1]["usage"] = response.llm_output["usage"]
                
                try:
                    text = response.generations[0][0].text
                    self.logs[-1]["content"] = mask_pii(text)
                except Exception:
                    pass
                    
        def on_llm_error(self, error: BaseException, **kwargs: Any) -> None:
            if self.logs and self.logs[-1]["type"] == "ai_thought":
                self.logs[-1]["status"] = "FAILED"
                self.logs[-1]["error"] = {
                    "message": str(error),
                    "type": type(error).__name__
                }

        def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs: Any) -> None:
            self.logs.append({
                "id": str(uuid.uuid4()),
                "type": "tool_usage",
                "name": serialized.get("name", "Tool"),
                "node_id": serialized.get("name", "tool_node"),
                "status": "RUNNING",
                "timestamp": int(time.time()),
                "input": mask_pii(input_str)
            })

        def on_tool_end(self, output: str, **kwargs: Any) -> None:
            if self.logs and self.logs[-1]["type"] == "tool_usage":
                self.logs[-1]["status"] = "COMPLETED"
                self.logs[-1]["output"] = mask_pii(str(output))

        def on_tool_error(self, error: BaseException, **kwargs: Any) -> None:
            if self.logs and self.logs[-1]["type"] == "tool_usage":
                self.logs[-1]["status"] = "FAILED"
                self.logs[-1]["error"] = str(error)

    history_callback = StateHistoryCallback()
    final_config.setdefault("callbacks", []).append(history_callback)

    # Run!
    logger.info("üöÄ Invoking workflow...")
    try:
        # [ÏàòÏ†ï] config Ï†ÑÏ≤¥Î•º ÎÑòÍ≤®Ïïº metadata Îì±Ïù¥ Ìï®Íªò Ï†ÑÎã¨Îê®
        result = app.invoke(initial_state, config=final_config) 
        
        # [NEW] Attach collected logs to the result (if result is a dict)
        if isinstance(result, dict):
            # Legacy field (kept for backward compatibility)
            result["__new_history_logs"] = history_callback.logs
            # Glass-box logs for UI
            prev_logs = result.get("execution_logs", [])
            result["execution_logs"] = prev_logs + history_callback.logs
        return result
    except AsyncLLMRequiredException:
        # Signal orchestrator to pause for async LLM / HITP handling
        return {"status": "PAUSED_FOR_ASYNC_LLM"}
    except Exception as e:
        logger.exception("Workflow execution failed")
        raise e


# -----------------------------------------------------------------------------
# Partition Workflow Functions (for Lambda compatibility)
# -----------------------------------------------------------------------------

def partition_workflow(config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    ÏõåÌÅ¨ÌîåÎ°úÏö∞Î•º ÏÑ∏Í∑∏Î®ºÌä∏Î°ú Î∂ÑÌï†ÌïòÎäî Ìï®Ïàò.
    partition_workflow_advancedÏùò aliasÎ°ú, Lambda Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌï¥ Ïú†ÏßÄ.
    """
    from src.services.workflow.partition_service import partition_workflow_advanced
    
    # partition_workflow_advancedÎäî {"partition_map": [...], ...} ÌòïÌÉúÎ°ú Î∞òÌôòÌïòÎØÄÎ°ú
    # partition_map Î¶¨Ïä§Ìä∏Îßå Ï∂îÏ∂ú
    result = partition_workflow_advanced(config)
    return result.get("partition_map", [])


def _build_segment_config(segment: Dict[str, Any]) -> Dict[str, Any]:
    """
    ÏÑ∏Í∑∏Î®ºÌä∏ Í∞ùÏ≤¥Î•º Ïã§Ìñâ Í∞ÄÎä•Ìïú ÏõåÌÅ¨ÌîåÎ°úÏö∞ configÎ°ú Î≥ÄÌôò.
    
    ÏÑ∏Í∑∏Î®ºÌä∏Îäî {"id": str, "nodes": [...], "edges": [...], "type": str, "node_ids": [...]} ÌòïÌÉú.
    Ïù¥Î•º run_workflowÏóê Ï†ÑÎã¨Ìï† Ïàò ÏûàÎäî {"nodes": [...], "edges": [...]} ÌòïÌÉúÎ°ú Î≥ÄÌôò.
    """
    return {
        "nodes": segment.get("nodes", []),
        "edges": segment.get("edges", [])
    }


def run_workflow_from_dynamodb(table_name: str, key_name: str, key_value: str, initial_state: Optional[Dict[str, Any]] = None, user_api_keys: Optional[Dict[str, str]] = None) -> Dict[str, Any]:
    """
    DynamoDBÏóêÏÑú ÏõåÌÅ¨ÌîåÎ°úÏö∞ configÎ•º Í∞ÄÏ†∏ÏôÄÏÑú Ïã§Ìñâ.
    
    Args:
        table_name: DynamoDB ÌÖåÏù¥Î∏î Ïù¥Î¶Ñ
        key_name: ÌååÌã∞ÏÖò ÌÇ§ Ïù¥Î¶Ñ
        key_value: ÌååÌã∞ÏÖò ÌÇ§ Í∞í
        initial_state: Ï¥àÍ∏∞ ÏÉÅÌÉú (ÏòµÏÖò)
        user_api_keys: ÏÇ¨Ïö©Ïûê API ÌÇ§ (ÏòµÏÖò)
    
    Returns:
        ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ïã§Ìñâ Í≤∞Í≥º
    """
    # DynamoDBÏóêÏÑú config Í∞ÄÏ†∏Ïò§Í∏∞ - Î¶¨Ï†ÑÏùÑ ÌôòÍ≤ΩÎ≥ÄÏàòÏóêÏÑú Í∞ÄÏ†∏Ïò¥
    region = os.environ.get('AWS_REGION', 'ap-northeast-2')
    dynamodb = boto3.resource('dynamodb', region_name=region)
    table = dynamodb.Table(table_name)
    
    response = table.get_item(Key={key_name: key_value})
    
    if 'Item' not in response:
        raise ValueError(f"Workflow config not found in DynamoDB table {table_name} with key {key_name}={key_value}")
    
    item = response['Item']
    
    # [Hybrid Storage] S3 Hydration Check
    config_s3_ref = item.get('config_s3_ref')
    if config_s3_ref:
        logger.info(f"üîÑ Hybrid Storage: Hydrating config from {config_s3_ref}")
        try:
            # Parse s3://bucket/key
            if config_s3_ref.startswith("s3://"):
                parts = config_s3_ref[5:].split("/", 1)
                bucket = parts[0]
                key = parts[1]
                
                s3_client = boto3.client('s3', region_name=region)
                obj = s3_client.get_object(Bucket=bucket, Key=key)
                config_json = obj['Body'].read().decode('utf-8')
            else:
                logger.warning(f"Invalid S3 ref format: {config_s3_ref}, falling back to item config")
                config_json = item.get('config_json') or item.get('config')
        except Exception as e:
            logger.error(f"‚ùå Failed to hydrate config from S3: {e}")
            raise ValueError(f"Failed to load offloaded config: {e}")
    else:
        # Standard load
        config_json = item.get('config_json') or item.get('config')
    
    if not config_json:
        raise ValueError(f"No config_json found in DynamoDB item")
    
    # JSON ÌååÏã± (ÌïÑÏöîÌïú Í≤ΩÏö∞)
    if isinstance(config_json, str):
        config_json = json.loads(config_json)
    
    # ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ïã§Ìñâ
    return run_workflow(config_json, initial_state, user_api_keys)
