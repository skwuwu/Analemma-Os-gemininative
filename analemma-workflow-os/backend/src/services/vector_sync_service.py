"""
ì§€ëŠ¥í˜• ì§€ì¹¨ ì¦ë¥˜ê¸° - ë²¡í„° ë™ê¸°í™” ì„œë¹„ìŠ¤ (ê³ ê¸‰ ìµœì í™” + ê¸°ìˆ ì  ì •ë°€ íŠœë‹)

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì£¼ì…
2. ì§€ìˆ˜ ë°±ì˜¤í”„(Exponential Backoff) ë„ì…
3. í† í° ì œí•œ ë°©ì–´ (Token Clipping)

ê¸°ìˆ ì  ì •ë°€ íŠœë‹:
4. tiktoken ë ˆì´ì–´ ê´€ë¦¬ (AWS Lambda ìµœì í™”)
5. ì„ë² ë”© í…ìŠ¤íŠ¸ì˜ ì •ê·œí™” (Contextual Formatting)
6. ë²¡í„° DB ì¸ë±ì‹± ì§€ì—° ì²˜ë¦¬ (Refresh Interval)

[í•´ì»¤í†¤ ìˆ˜ìƒ í¬ì¸íŠ¸] Google Native ì„ ì–¸:
- Analemma OSëŠ” Vertex AI text-embedding-004 (768ì°¨ì›)ì„ í‘œì¤€ìœ¼ë¡œ ì‚¬ìš©
- ë™ì¼í•œ ë²¡í„° DB ì¸ë±ìŠ¤ì—ëŠ” ë™ì¼í•œ ì°¨ì›ì˜ ë²¡í„°ë§Œ ì €ì¥
- OpenAI (1536ì°¨ì›) í´ë°±ì€ ì œê±°í•˜ì—¬ ì°¨ì› ë¶ˆì¼ì¹˜ ë°©ì§€
"""

import boto3
import asyncio
import logging
import os
import time
import random
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta, timezone
from src.models.correction_log import (
    CorrectionLog, 
    VectorSyncStatus, 
    VectorSyncManager
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„ë² ë”© ì°¨ì› í‘œì¤€ (Google Native)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Analemma OSëŠ” Google Cloud Native í”„ë¡œì íŠ¸ë¡œì„œ
# Vertex AI text-embedding-004 ëª¨ë¸(768ì°¨ì›)ì„ í‘œì¤€ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
# 
# âš ï¸ ì¤‘ìš”: ë²¡í„° DB ì¸ë±ìŠ¤ëŠ” ë‹¨ì¼ ì°¨ì›ë§Œ ì§€ì›í•©ë‹ˆë‹¤.
# - Vertex AI text-embedding-004: 768ì°¨ì› âœ… (í‘œì¤€)
# - OpenAI text-embedding-ada-002: 1536ì°¨ì› âŒ (ì‚¬ìš© ê¸ˆì§€)
# - OpenAI text-embedding-3-small: 1536ì°¨ì› âŒ (ì‚¬ìš© ê¸ˆì§€)
#
# í˜¼í•© ì‚¬ìš© ì‹œ Dimension Mismatch ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤.
EMBEDDING_DIMENSION = 768  # Vertex AI text-embedding-004 í‘œì¤€ ì°¨ì›
EMBEDDING_MODEL = "text-embedding-004"  # Vertex AI í‘œì¤€ ëª¨ë¸

# ê°œì„ ì‚¬í•­ 4: tiktoken ë ˆì´ì–´ ê´€ë¦¬ (AWS Lambda ìµœì í™”)
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logging.warning("tiktoken not available - using character-based token approximation")

logger = logging.getLogger(__name__)

class VectorSyncService:
    """
    ë²¡í„° DB ë™ê¸°í™” ì„œë¹„ìŠ¤ (ê³ ê¸‰ ìµœì í™” ì ìš©)
    
    [Google Native í‘œì¤€]
    ì„ë² ë”©: Vertex AI text-embedding-004 (768ì°¨ì›)
    ë²¡í„° DB: OpenSearch / pgvector
    """
    
    def __init__(self):
        self.ddb = boto3.resource('dynamodb')
        # ğŸš¨ [Critical Fix] ê¸°ë³¸ê°’ì„ template.yamlê³¼ ì¼ì¹˜ì‹œí‚´
        self.correction_table = self.ddb.Table(
            os.environ.get('CORRECTION_LOGS_TABLE', 'CorrectionLogsTable')
        )
        self.vector_sync_manager = VectorSyncManager()
        
        # ë²¡í„° DB í´ë¼ì´ì–¸íŠ¸ (OpenSearch/pgvector ë“±)
        self.vector_client = self._initialize_vector_client()
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # [DEPRECATED] OpenAI í´ë¼ì´ì–¸íŠ¸ ì œê±°
        # ì´ìœ : Vertex AI (768ì°¨ì›)ì™€ OpenAI (1536ì°¨ì›) í˜¼í•© ì‚¬ìš© ë¶ˆê°€
        # Analemma OSëŠ” Google Nativeë¡œì„œ Vertex AIë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # self.openai_client = None  # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        
        # ê°œì„ ì‚¬í•­ 2: ì§€ìˆ˜ ë°±ì˜¤í”„ ì„¤ì •
        self.base_retry_delay = 1.0  # ê¸°ë³¸ ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        self.max_retry_delay = 300.0  # ìµœëŒ€ ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„ (5ë¶„)
        self.backoff_multiplier = 2.0  # ì§€ìˆ˜ ë°±ì˜¤í”„ ë°°ìˆ˜

    def _initialize_vector_client(self):
        """ë²¡í„° DB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            vector_db_type = os.environ.get('VECTOR_DB_TYPE', 'opensearch')
            
            if vector_db_type == 'opensearch':
                from opensearchpy import OpenSearch, AsyncOpenSearch
                
                # OpenSearch í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
                opensearch_host = os.environ.get('OPENSEARCH_HOST')
                opensearch_port = int(os.environ.get('OPENSEARCH_PORT', '443'))
                opensearch_user = os.environ.get('OPENSEARCH_USER')
                opensearch_password = os.environ.get('OPENSEARCH_PASSWORD')
                
                if opensearch_host:
                    return AsyncOpenSearch(
                        hosts=[{'host': opensearch_host, 'port': opensearch_port}],
                        http_auth=(opensearch_user, opensearch_password) if opensearch_user else None,
                        use_ssl=True,
                        verify_certs=True
                    )
                else:
                    logger.warning("OpenSearch host not configured, vector operations will be no-ops")
                    return None
                    
            elif vector_db_type == 'pgvector':
                # pgvector ì§€ì›
                import asyncpg
                
                pgvector_host = os.environ.get('PGVECTOR_HOST')
                pgvector_port = int(os.environ.get('PGVECTOR_PORT', '5432'))
                pgvector_user = os.environ.get('PGVECTOR_USER', 'postgres')
                pgvector_password = os.environ.get('PGVECTOR_PASSWORD')
                pgvector_database = os.environ.get('PGVECTOR_DATABASE', 'vectors')
                
                if pgvector_host:
                    # ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì—°ê²° ì •ë³´ ì €ì¥
                    self._pgvector_config = {
                        'host': pgvector_host,
                        'port': pgvector_port,
                        'user': pgvector_user,
                        'password': pgvector_password,
                        'database': pgvector_database
                    }
                    logger.info(f"pgvector configured: {pgvector_host}:{pgvector_port}/{pgvector_database}")
                    return 'pgvector'  # ì»¨ë„¥ì…˜ í’€ ë°˜í™˜ ëŒ€ì‹  ë§ˆì»¤ ë°˜í™˜
                else:
                    logger.warning("pgvector host not configured, vector operations will be no-ops")
                    return None
                
            else:
                logger.warning(f"Unknown vector DB type: {vector_db_type}")
                return None
                
        except Exception as e:
            logger.error(f"Failed to initialize vector client: {str(e)}")
            return None
        self.jitter_range = 0.1  # ì§€í„° ë²”ìœ„ (10%)
        
        # ê°œì„ ì‚¬í•­ 3: í† í° ì œí•œ ì„¤ì •
        self.max_embedding_tokens = 3072  # Vertex AI text-embedding-004 ìµœëŒ€ í† í°
        self.token_safety_margin = 100  # ì•ˆì „ ë§ˆì§„
        self.effective_token_limit = self.max_embedding_tokens - self.token_safety_margin
        
        # ê°œì„ ì‚¬í•­ 4: tiktoken ë ˆì´ì–´ ê´€ë¦¬ (AWS Lambda ìµœì í™”)
        self.tokenizer = self._initialize_tokenizer()
        
        # ê°œì„ ì‚¬í•­ 6: ë²¡í„° DB ì¸ë±ì‹± ì§€ì—° ì²˜ë¦¬ ì„¤ì •
        self.vector_db_refresh_delay = float(os.environ.get('VECTOR_DB_REFRESH_DELAY', '2.0'))  # ê¸°ë³¸ 2ì´ˆ
        self.enable_refresh_wait = os.environ.get('ENABLE_VECTOR_DB_REFRESH_WAIT', 'true').lower() == 'true'
    
    def _initialize_tokenizer(self):
        """
        ê°œì„ ì‚¬í•­ 4: tiktoken í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (AWS Lambda ìµœì í™”)
        
        AWS Lambda Layer ì‚¬ìš© ì‹œ ê³ ë ¤ì‚¬í•­:
        1. tiktoken ë°”ì´ë„ˆë¦¬ì™€ ì¸ì½”ë”© íŒŒì¼ì„ Layerì— í¬í•¨
        2. ë„¤íŠ¸ì›Œí¬ ë‹¤ìš´ë¡œë“œ ì˜¤ë²„í—¤ë“œ ì œê±°
        3. ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ graceful fallback
        """
        if not TIKTOKEN_AVAILABLE:
            logger.warning("tiktoken not available, using character-based approximation")
            return None
        
        try:
            # AWS Lambda Layerì—ì„œ tiktoken ì‚¬ìš© ì‹œ ìµœì í™”
            encoding_name = os.environ.get('TIKTOKEN_ENCODING', 'cl100k_base')  # GPT-4/ada-002 í˜¸í™˜
            
            # Lambda Layer ê²½ë¡œ í™•ì¸
            layer_path = os.environ.get('TIKTOKEN_LAYER_PATH')
            if layer_path and os.path.exists(layer_path):
                # Layerì—ì„œ ì¸ì½”ë”© íŒŒì¼ ë¡œë“œ (ë„¤íŠ¸ì›Œí¬ ë‹¤ìš´ë¡œë“œ ë°©ì§€)
                os.environ['TIKTOKEN_CACHE_DIR'] = layer_path
                logger.info(f"Using tiktoken from src.Lambda Layer: {layer_path}")
            
            tokenizer = tiktoken.get_encoding(encoding_name)
            logger.info(f"tiktoken initialized successfully with encoding: {encoding_name}")
            return tokenizer
            
        except Exception as e:
            logger.warning(f"tiktoken initialization failed, using approximation: {e}")
            return None
    
    async def sync_correction_to_vector_db(
        self,
        correction_log: CorrectionLog,
        retry_count: int = 0
    ) -> bool:
        """
        ë‹¨ì¼ ìˆ˜ì • ë¡œê·¸ë¥¼ ë²¡í„° DBì— ë™ê¸°í™” (ê°œì„ ëœ ë²„ì „)
        
        ê°œì„ ì‚¬í•­:
        1. ì§€ìˆ˜ ë°±ì˜¤í”„ ì ìš©
        2. í† í° ì œí•œ ë°©ì–´
        3. í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ ë©”íƒ€ë°ì´í„° í¬í•¨
        """
        try:
            # ê°œì„ ì‚¬í•­ 2: ì§€ìˆ˜ ë°±ì˜¤í”„ ì ìš©
            if retry_count > 0:
                delay = self._calculate_backoff_delay(retry_count)
                logger.info(f"Retrying vector sync after {delay:.2f}s delay (attempt {retry_count + 1})")
                await asyncio.sleep(delay)
            
            # ê°œì„ ì‚¬í•­ 3: í† í° ì œí•œ ë°©ì–´ê°€ ì ìš©ëœ ì„ë² ë”© í…ìŠ¤íŠ¸ ì¤€ë¹„
            embedding_text = self._prepare_embedding_text_with_token_limit(correction_log)
            embedding_vector = await self._generate_embedding(embedding_text)
            
            if not embedding_vector:
                raise Exception("Failed to generate embedding")
            
            # ê°œì„ ì‚¬í•­ 1: í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ë²¡í„° DBì— ì €ì¥
            embedding_id = await self._store_in_vector_db_with_metadata(
                correction_log, 
                embedding_vector,
                embedding_text
            )
            
            if not embedding_id:
                raise Exception("Failed to store in vector DB")
            
            # ê°œì„ ì‚¬í•­ 6: ë²¡í„° DB ì¸ë±ì‹± ì§€ì—° ì²˜ë¦¬
            if self.enable_refresh_wait:
                logger.debug(f"Waiting {self.vector_db_refresh_delay}s for vector DB refresh")
                await asyncio.sleep(self.vector_db_refresh_delay)
            
            # ì„±ê³µ ì²˜ë¦¬
            self.vector_sync_manager.mark_sync_success(correction_log, embedding_id)
            await self._update_correction_sync_status(correction_log)
            
            logger.info(f"Vector sync successful: {correction_log.sk} -> {embedding_id}")
            return True
            
        except Exception as e:
            # ì‹¤íŒ¨ ì²˜ë¦¬ (ì§€ìˆ˜ ë°±ì˜¤í”„ ê³ ë ¤)
            error_message = f"Vector sync failed (attempt {retry_count + 1}): {str(e)}"
            self.vector_sync_manager.mark_sync_failed(correction_log, error_message)
            await self._update_correction_sync_status(correction_log)
            
            logger.error(f"Vector sync failed for {correction_log.sk}: {error_message}")
            
            # ìë™ ì¬ì‹œë„ (ìµœëŒ€ 3íšŒ)
            if retry_count < 3 and self._should_retry_error(str(e)):
                logger.info(f"Scheduling automatic retry for {correction_log.sk}")
                return await self.sync_correction_to_vector_db(correction_log, retry_count + 1)
            
            return False
    
    def _calculate_backoff_delay(self, retry_count: int) -> float:
        """
        ê°œì„ ì‚¬í•­ 2: ì§€ìˆ˜ ë°±ì˜¤í”„ ì§€ì—° ì‹œê°„ ê³„ì‚°
        
        ê³µì‹: base_delay * (multiplier ^ retry_count) + jitter
        """
        # ê¸°ë³¸ ì§€ìˆ˜ ë°±ì˜¤í”„
        exponential_delay = self.base_retry_delay * (self.backoff_multiplier ** retry_count)
        
        # ìµœëŒ€ ì§€ì—° ì‹œê°„ ì œí•œ
        capped_delay = min(exponential_delay, self.max_retry_delay)
        
        # ì§€í„° ì¶”ê°€ (ë™ì‹œ ì¬ì‹œë„ë¡œ ì¸í•œ ë¶€í•˜ ë¶„ì‚°)
        jitter = random.uniform(-self.jitter_range, self.jitter_range) * capped_delay
        final_delay = max(0.1, capped_delay + jitter)  # ìµœì†Œ 0.1ì´ˆ
        
        return final_delay
    
    def _should_retry_error(self, error_message: str) -> bool:
        """ì¬ì‹œë„ ê°€ëŠ¥í•œ ì—ëŸ¬ì¸ì§€ íŒë‹¨"""
        
        # ì¬ì‹œë„ ê°€ëŠ¥í•œ ì—ëŸ¬ íŒ¨í„´ë“¤
        retryable_patterns = [
            "timeout",
            "connection",
            "network",
            "rate limit",
            "throttle",
            "503",  # Service Unavailable
            "502",  # Bad Gateway
            "500",  # Internal Server Error
            "429"   # Too Many Requests
        ]
        
        error_lower = error_message.lower()
        return any(pattern in error_lower for pattern in retryable_patterns)
    
    def _prepare_embedding_text_with_token_limit(self, correction_log: CorrectionLog) -> str:
        """
        ê°œì„ ì‚¬í•­ 3 + 5: í† í° ì œí•œì„ ê³ ë ¤í•œ ì„ë² ë”© í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì •ê·œí™”ëœ ì»¨í…ìŠ¤íŠ¸ í¬ë§·)
        
        ìš°ì„ ìˆœìœ„:
        1. ì‚¬ìš©ì ìˆ˜ì • (ê°€ì¥ ì¤‘ìš”)
        2. ì—ì´ì „íŠ¸ ì¶œë ¥
        3. ë©”íƒ€ë°ì´í„° (Instruction Type ìš°ì„  ë°°ì¹˜)
        4. ì›ë³¸ ì…ë ¥ (ê°€ì¥ ëœ ì¤‘ìš”, í•„ìš”ì‹œ ì˜ë¼ëƒ„)
        
        ê°œì„ ì‚¬í•­ 5: ì„ë² ë”© í…ìŠ¤íŠ¸ì˜ ì •ê·œí™” (Contextual Formatting)
        - êµ¬ë¶„ì(|)ë¡œ ëª…í™•í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ë¶„
        - Instruction Typeì„ ë©”íƒ€ë°ì´í„° ì•ë¶€ë¶„ì— ë°°ì¹˜í•˜ì—¬ ëª¨ë¸ ì£¼ëª©ë„ í–¥ìƒ
        - ë„ë©”ì¸ë³„ ìµœì í™” (SQL, Email ë“±)
        """
        
        # ìš°ì„ ìˆœìœ„ë³„ í…ìŠ¤íŠ¸ êµ¬ì„± ìš”ì†Œ
        components = []
        
        # 1ìˆœìœ„: ì‚¬ìš©ì ìˆ˜ì • (í•„ìˆ˜)
        corrected_text = f"USER_CORRECTION: {correction_log.user_correction}"
        components.append(("corrected", corrected_text, True))  # (name, text, required)
        
        # 2ìˆœìœ„: ì—ì´ì „íŠ¸ ì¶œë ¥ (ì¤‘ìš”)
        agent_text = f"AGENT_OUTPUT: {correction_log.agent_output}"
        components.append(("agent", agent_text, True))
        
        # 3ìˆœìœ„: ë©”íƒ€ë°ì´í„° (ì¤‘ìš”) - Instruction Type ìš°ì„  ë°°ì¹˜
        metadata_parts = self._build_prioritized_metadata(correction_log)
        metadata_text = " | ".join(metadata_parts)
        components.append(("metadata", metadata_text, True))
        
        # 4ìˆœìœ„: ì›ë³¸ ì…ë ¥ (í•„ìš”ì‹œ ì˜ë¼ë‚¼ ìˆ˜ ìˆìŒ)
        original_text = f"ORIGINAL_INPUT: {correction_log.original_input}"
        components.append(("original", original_text, False))
        
        # í† í° ì œí•œ ë‚´ì—ì„œ ìµœì  ì¡°í•© ì°¾ê¸°
        return self._optimize_text_within_token_limit(components)
    
    def _build_prioritized_metadata(self, correction_log: CorrectionLog) -> List[str]:
        """
        ê°œì„ ì‚¬í•­ 5: ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë©”íƒ€ë°ì´í„° êµ¬ì„±
        
        Instruction Typeì„ ì•ì— ë°°ì¹˜í•˜ì—¬ ëª¨ë¸ì˜ ì£¼ëª©ë„(Attention) í–¥ìƒ
        """
        metadata_parts = []
        
        # ìµœìš°ì„ : Instruction Type (ëª¨ë¸ ì£¼ëª©ë„ í–¥ìƒ)
        if correction_log.extracted_metadata and 'instruction_type' in correction_log.extracted_metadata:
            instruction_type = correction_log.extracted_metadata['instruction_type']
            metadata_parts.append(f"INSTRUCTION_TYPE: {instruction_type}")
        
        # í•µì‹¬ ë¶„ë¥˜ ì •ë³´
        metadata_parts.extend([
            f"CATEGORY: {correction_log.task_category.value}",
            f"CONTEXT: {correction_log.context_scope}",
            f"DOMAIN: {correction_log.workflow_domain}"
        ])
        
        # ë„ë©”ì¸ë³„ íŠ¹í™” ë©”íƒ€ë°ì´í„°
        domain_metadata = self._get_domain_specific_metadata(correction_log)
        if domain_metadata:
            metadata_parts.extend(domain_metadata)
        
        # ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„° (ìš°ì„ ìˆœìœ„ ì •ë ¬)
        if correction_log.extracted_metadata:
            prioritized_keys = ['tone', 'style', 'length', 'format', 'language']
            
            for key in prioritized_keys:
                if key in correction_log.extracted_metadata and key != 'instruction_type':
                    value = correction_log.extracted_metadata[key]
                    metadata_parts.append(f"{key.upper()}: {value}")
            
            # ë‚˜ë¨¸ì§€ ë©”íƒ€ë°ì´í„°
            for key, value in correction_log.extracted_metadata.items():
                if key not in prioritized_keys and key != 'instruction_type':
                    metadata_parts.append(f"{key.upper()}: {value}")
        
        return metadata_parts
    
    def _get_domain_specific_metadata(self, correction_log: CorrectionLog) -> List[str]:
        """
        ê°œì„ ì‚¬í•­ 5: ë„ë©”ì¸ë³„ íŠ¹í™” ë©”íƒ€ë°ì´í„°
        
        íŠ¹ì • ë„ë©”ì¸(SQL, Email ë“±)ì—ì„œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        """
        domain_parts = []
        domain = correction_log.workflow_domain.lower()
        
        if domain in ['sql', 'database', 'query']:
            # SQL ë„ë©”ì¸ íŠ¹í™”
            if correction_log.extracted_metadata:
                if 'query_type' in correction_log.extracted_metadata:
                    domain_parts.append(f"SQL_TYPE: {correction_log.extracted_metadata['query_type']}")
                if 'table_names' in correction_log.extracted_metadata:
                    domain_parts.append(f"TABLES: {correction_log.extracted_metadata['table_names']}")
        
        elif domain in ['email', 'communication', 'messaging']:
            # ì´ë©”ì¼ ë„ë©”ì¸ íŠ¹í™”
            if correction_log.extracted_metadata:
                if 'recipient_type' in correction_log.extracted_metadata:
                    domain_parts.append(f"RECIPIENT: {correction_log.extracted_metadata['recipient_type']}")
                if 'urgency' in correction_log.extracted_metadata:
                    domain_parts.append(f"URGENCY: {correction_log.extracted_metadata['urgency']}")
        
        elif domain in ['code', 'programming', 'development']:
            # ì½”ë“œ ë„ë©”ì¸ íŠ¹í™”
            if correction_log.extracted_metadata:
                if 'language' in correction_log.extracted_metadata:
                    domain_parts.append(f"PROG_LANG: {correction_log.extracted_metadata['language']}")
                if 'code_type' in correction_log.extracted_metadata:
                    domain_parts.append(f"CODE_TYPE: {correction_log.extracted_metadata['code_type']}")
        
        return domain_parts
    
    def _optimize_text_within_token_limit(self, components: List[tuple]) -> str:
        """í† í° ì œí•œ ë‚´ì—ì„œ ìµœì ì˜ í…ìŠ¤íŠ¸ ì¡°í•© ìƒì„±"""
        
        # í•„ìˆ˜ êµ¬ì„± ìš”ì†Œë“¤ ë¨¼ì € í¬í•¨
        required_parts = []
        optional_parts = []
        
        for name, text, required in components:
            if required:
                required_parts.append(text)
            else:
                optional_parts.append((name, text))
        
        # í•„ìˆ˜ ë¶€ë¶„ ê²°í•©
        current_text = " | ".join(required_parts)
        current_tokens = self._count_tokens(current_text)
        
        # í† í° ì œí•œ í™•ì¸
        if current_tokens > self.effective_token_limit:
            # í•„ìˆ˜ ë¶€ë¶„ë„ ì œí•œ ì´ˆê³¼ ì‹œ ì›ë³¸ ì…ë ¥ë¶€í„° ì¶•ì†Œ
            logger.warning(f"Required components exceed token limit: {current_tokens} > {self.effective_token_limit}")
            return self._truncate_text_to_token_limit(current_text)
        
        # ì„ íƒì  ë¶€ë¶„ ì¶”ê°€ (í† í° ì œí•œ ë‚´ì—ì„œ)
        for name, text in optional_parts:
            test_text = current_text + " | " + text
            test_tokens = self._count_tokens(test_text)
            
            if test_tokens <= self.effective_token_limit:
                current_text = test_text
                current_tokens = test_tokens
            else:
                # ë¶€ë¶„ì ìœ¼ë¡œë¼ë„ í¬í•¨í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸
                available_tokens = self.effective_token_limit - current_tokens - 3  # " | " ê³ ë ¤
                if available_tokens > 50:  # ìµœì†Œ 50í† í°ì€ ìˆì–´ì•¼ ì˜ë¯¸ ìˆìŒ
                    truncated_text = self._truncate_text_to_tokens(text, available_tokens)
                    if truncated_text:
                        current_text += " | " + truncated_text
                break
        
        logger.debug(f"Optimized embedding text: {current_tokens} tokens")
        return current_text
    
    def _count_tokens(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
        if self.tokenizer:
            try:
                return len(self.tokenizer.encode(text))
            except Exception as e:
                logger.warning(f"Token counting failed, using approximation: {e}")
        
        # í† í¬ë‚˜ì´ì € ì—†ì„ ì‹œ ê·¼ì‚¬ì¹˜ (ì˜ì–´ ê¸°ì¤€ 1í† í° â‰ˆ 4ì)
        return len(text) // 4
    
    def _truncate_text_to_token_limit(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ í† í° ì œí•œì— ë§ê²Œ ìë¥´ê¸°"""
        return self._truncate_text_to_tokens(text, self.effective_token_limit)
    
    def _truncate_text_to_tokens(self, text: str, max_tokens: int) -> str:
        """ì§€ì •ëœ í† í° ìˆ˜ì— ë§ê²Œ í…ìŠ¤íŠ¸ ìë¥´ê¸°"""
        if self._count_tokens(text) <= max_tokens:
            return text
        
        # ì´ì§„ íƒìƒ‰ìœ¼ë¡œ ìµœì  ê¸¸ì´ ì°¾ê¸°
        left, right = 0, len(text)
        best_text = ""
        
        while left <= right:
            mid = (left + right) // 2
            candidate = text[:mid]
            
            if self._count_tokens(candidate) <= max_tokens:
                best_text = candidate
                left = mid + 1
            else:
                right = mid - 1
        
        # ë‹¨ì–´ ê²½ê³„ì—ì„œ ìë¥´ê¸° (ê°€ëŠ¥í•œ ê²½ìš°)
        if best_text and not best_text.endswith(' '):
            last_space = best_text.rfind(' ')
            if last_space > len(best_text) * 0.8:  # 80% ì´ìƒ ìœ ì§€ë˜ëŠ” ê²½ìš°ë§Œ
                best_text = best_text[:last_space]
        
        return best_text + "..." if best_text != text else best_text
    
    async def batch_sync_corrections(
        self,
        user_id: str,
        batch_size: int = 10,
        max_retries: int = 3
    ) -> Dict[str, int]:
        """
        ë°°ì¹˜ë¡œ ìˆ˜ì • ë¡œê·¸ë“¤ì„ ë²¡í„° DBì— ë™ê¸°í™”
        """
        try:
            # ë™ê¸°í™”ê°€ í•„ìš”í•œ ìˆ˜ì • ë¡œê·¸ë“¤ ì¡°íšŒ
            pending_corrections = await self._get_pending_sync_corrections(
                user_id, batch_size * 2  # ì—¬ìœ ë¶„ í™•ë³´
            )
            
            if not pending_corrections:
                logger.info(f"No pending corrections for vector sync: user {user_id}")
                return {"processed": 0, "successful": 0, "failed": 0}
            
            # ì¬ì‹œë„ í•„ìš”í•œ í•­ëª©ë“¤ í•„í„°ë§
            corrections_to_sync = []
            for item in pending_corrections:
                correction = CorrectionLog(**item)
                if self.vector_sync_manager.should_retry_sync(correction):
                    corrections_to_sync.append(correction)
            
            # ë°°ì¹˜ í¬ê¸°ë¡œ ì œí•œ
            corrections_to_sync = corrections_to_sync[:batch_size]
            
            logger.info(f"Starting batch vector sync: {len(corrections_to_sync)} corrections")
            
            # ë™ì‹œ ì²˜ë¦¬ (í•˜ì§€ë§Œ API ì œí•œ ê³ ë ¤)
            semaphore = asyncio.Semaphore(3)  # ìµœëŒ€ 3ê°œ ë™ì‹œ ì²˜ë¦¬
            
            async def sync_with_semaphore(correction):
                async with semaphore:
                    return await self.sync_correction_to_vector_db(
                        correction, 
                        correction.vector_sync_attempts
                    )
            
            # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
            results = await asyncio.gather(
                *[sync_with_semaphore(correction) for correction in corrections_to_sync],
                return_exceptions=True
            )
            
            # ê²°ê³¼ ì§‘ê³„
            successful = sum(1 for result in results if result is True)
            failed = len(results) - successful
            
            logger.info(f"Batch vector sync completed: {successful} successful, {failed} failed")
            
            return {
                "processed": len(corrections_to_sync),
                "successful": successful,
                "failed": failed
            }
            
        except Exception as e:
            logger.error(f"Batch vector sync failed: {str(e)}")
            return {"processed": 0, "successful": 0, "failed": 0}
    
    async def retry_failed_syncs(
        self,
        user_id: str,
        max_age_hours: int = 24
    ) -> Dict[str, int]:
        """
        ì‹¤íŒ¨í•œ ë™ê¸°í™”ë“¤ì„ ì¬ì‹œë„
        """
        try:
            # ì¬ì‹œë„ ëŒ€ìƒ ì¡°íšŒ (ìµœê·¼ 24ì‹œê°„ ë‚´ ì‹¤íŒ¨í•œ ê²ƒë“¤)
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=max_age_hours)
            
            failed_corrections = await self._get_failed_sync_corrections(
                user_id, cutoff_time
            )
            
            if not failed_corrections:
                logger.info(f"No failed corrections to retry: user {user_id}")
                return {"processed": 0, "successful": 0, "failed": 0}
            
            logger.info(f"Retrying failed vector syncs: {len(failed_corrections)} corrections")
            
            # ì¬ì‹œë„ ì‹¤í–‰
            results = await self.batch_sync_corrections(
                user_id, 
                batch_size=len(failed_corrections)
            )
            
            return results
            
        except Exception as e:
            logger.error(f"Failed sync retry failed: {str(e)}")
            return {"processed": 0, "successful": 0, "failed": 0}
    
    async def get_sync_status_summary(
        self,
        user_id: str
    ) -> Dict[str, Any]:
        """
        ì‚¬ìš©ìì˜ ë²¡í„° ë™ê¸°í™” ìƒíƒœ ìš”ì•½
        """
        try:
            # ìƒíƒœë³„ ì¹´ìš´íŠ¸ ì¡°íšŒ
            response = self.correction_table.query(
                KeyConditionExpression='pk = :pk',
                ExpressionAttributeValues={
                    ':pk': f'user#{user_id}'
                }
            )
            
            items = response.get('Items', [])
            
            status_counts = {
                VectorSyncStatus.PENDING: 0,
                VectorSyncStatus.SUCCESS: 0,
                VectorSyncStatus.FAILED: 0,
                VectorSyncStatus.RETRY: 0
            }
            
            total_corrections = len(items)
            recent_failures = 0
            
            for item in items:
                status = item.get('vector_sync_status', VectorSyncStatus.PENDING)
                status_counts[status] += 1
                
                # ìµœê·¼ 1ì‹œê°„ ë‚´ ì‹¤íŒ¨ ì¹´ìš´íŠ¸
                if (status == VectorSyncStatus.FAILED and 
                    item.get('last_vector_sync_attempt')):
                    try:
                        last_attempt = datetime.fromisoformat(
                            item['last_vector_sync_attempt'].replace('Z', '+00:00')
                        )
                        if datetime.now(timezone.utc) - last_attempt < timedelta(hours=1):
                            recent_failures += 1
                    except:
                        pass
            
            sync_rate = (status_counts[VectorSyncStatus.SUCCESS] / total_corrections * 100) if total_corrections > 0 else 0
            
            return {
                "total_corrections": total_corrections,
                "status_counts": status_counts,
                "sync_rate_percent": round(sync_rate, 2),
                "recent_failures": recent_failures,
                "needs_attention": status_counts[VectorSyncStatus.FAILED] > 0 or status_counts[VectorSyncStatus.RETRY] > 0
            }
            
        except Exception as e:
            logger.error(f"Failed to get sync status summary: {str(e)}")
            return {}
    
    def _prepare_embedding_text(self, correction_log: CorrectionLog) -> str:
        """
        ì„ë² ë”©ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì¤€ë¹„ (ë ˆê±°ì‹œ ë©”ì„œë“œ)
        
        ìƒˆë¡œìš´ _prepare_embedding_text_with_token_limit ì‚¬ìš© ê¶Œì¥
        """
        logger.warning("Using legacy _prepare_embedding_text method. Consider using _prepare_embedding_text_with_token_limit")
        return self._prepare_embedding_text_with_token_limit(correction_log)
    
    def get_optimization_metrics(self) -> Dict[str, Any]:
        """ìµœì í™” ê´€ë ¨ ë©”íŠ¸ë¦­ ë°˜í™˜ (ê¸°ìˆ ì  ì •ë°€ íŠœë‹ í¬í•¨)"""
        return {
            "exponential_backoff": {
                "base_retry_delay": self.base_retry_delay,
                "max_retry_delay": self.max_retry_delay,
                "backoff_multiplier": self.backoff_multiplier,
                "jitter_range": self.jitter_range
            },
            "token_limits": {
                "max_embedding_tokens": self.max_embedding_tokens,
                "token_safety_margin": self.token_safety_margin,
                "effective_token_limit": self.effective_token_limit,
                "tokenizer_available": self.tokenizer is not None,
                "tiktoken_available": TIKTOKEN_AVAILABLE
            },
            "hybrid_filtering": {
                "metadata_fields": [
                    "user_id", "task_category", "context_scope", 
                    "node_type", "workflow_domain", "correction_type",
                    "edit_distance", "correction_time_seconds"
                ],
                "filter_optimization_enabled": True
            },
            "technical_refinements": {
                "tiktoken_layer_management": {
                    "enabled": TIKTOKEN_AVAILABLE,
                    "layer_path": os.environ.get('TIKTOKEN_LAYER_PATH'),
                    "encoding": os.environ.get('TIKTOKEN_ENCODING', 'cl100k_base')
                },
                "contextual_formatting": {
                    "enabled": True,
                    "instruction_type_priority": True,
                    "domain_specific_metadata": True,
                    "structured_separators": True
                },
                "vector_db_refresh": {
                    "enabled": self.enable_refresh_wait,
                    "delay_seconds": self.vector_db_refresh_delay,
                    "wait_for_completion": True
                }
            }
        }
    
    def get_lambda_layer_requirements(self) -> Dict[str, Any]:
        """
        ê°œì„ ì‚¬í•­ 4: AWS Lambda Layer ìš”êµ¬ì‚¬í•­ ë°˜í™˜
        
        tiktoken ì‚¬ìš©ì„ ìœ„í•œ Lambda Layer êµ¬ì„± ê°€ì´ë“œ
        """
        return {
            "layer_name": "tiktoken-layer",
            "description": "tiktoken library with encoding files for token counting",
            "required_files": [
                "python/lib/python3.9/site-packages/tiktoken/",
                "python/lib/python3.9/site-packages/tiktoken_ext/",
                "python/tiktoken_cache/"  # ì¸ì½”ë”© íŒŒì¼ ìºì‹œ
            ],
            "environment_variables": {
                "TIKTOKEN_LAYER_PATH": "/opt/python/tiktoken_cache",
                "TIKTOKEN_ENCODING": "cl100k_base",
                "TIKTOKEN_CACHE_DIR": "/opt/python/tiktoken_cache"
            },
            "layer_size_estimate": "~15MB",
            "python_version": "3.9+",
            "deployment_commands": [
                "mkdir -p layer/python/lib/python3.9/site-packages",
                "pip install tiktoken -t layer/python/lib/python3.9/site-packages/",
                "mkdir -p layer/python/tiktoken_cache",
                "python -c \"import tiktoken; tiktoken.get_encoding('cl100k_base')\"",
                "cp -r ~/.cache/tiktoken/* layer/python/tiktoken_cache/",
                "zip -r tiktoken-layer.zip layer/"
            ]
        }
    
    def get_contextual_formatting_examples(self) -> Dict[str, str]:
        """
        ê°œì„ ì‚¬í•­ 5: ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… ì˜ˆì‹œ
        
        ë„ë©”ì¸ë³„ ìµœì í™”ëœ ì„ë² ë”© í…ìŠ¤íŠ¸ êµ¬ì¡° ì˜ˆì‹œ
        """
        return {
            "email_domain": """USER_CORRECTION: Make this more professional | AGENT_OUTPUT: Hi there, hope you're doing well... | INSTRUCTION_TYPE: tone_adjustment | CATEGORY: email | CONTEXT: global | DOMAIN: communication | RECIPIENT: client | URGENCY: high | TONE: professional | STYLE: formal | ORIGINAL_INPUT: Hey, can you send me that report?""",
            
            "sql_domain": """USER_CORRECTION: Add proper JOIN syntax | AGENT_OUTPUT: SELECT * FROM users WHERE id = 1 | INSTRUCTION_TYPE: syntax_correction | CATEGORY: query | CONTEXT: database | DOMAIN: sql | SQL_TYPE: select | TABLES: users, orders | TONE: technical | FORMAT: sql | ORIGINAL_INPUT: get user data""",
            
            "code_domain": """USER_CORRECTION: Use async/await pattern | AGENT_OUTPUT: function getData() { return fetch('/api/data'); } | INSTRUCTION_TYPE: pattern_improvement | CATEGORY: code | CONTEXT: function | DOMAIN: programming | PROG_LANG: javascript | CODE_TYPE: async | STYLE: modern | ORIGINAL_INPUT: fetch data from src.API""",
            
            "general_domain": """USER_CORRECTION: Be more concise | AGENT_OUTPUT: This is a very long explanation that could be shorter... | INSTRUCTION_TYPE: length_adjustment | CATEGORY: text | CONTEXT: global | DOMAIN: general | TONE: neutral | LENGTH: short | STYLE: concise | ORIGINAL_INPUT: Explain the concept"""
        }
    
    def get_vector_db_refresh_strategies(self) -> Dict[str, Any]:
        """
        ê°œì„ ì‚¬í•­ 6: ë²¡í„° DB ì¸ë±ì‹± ì§€ì—° ì²˜ë¦¬ ì „ëµ
        
        ë‹¤ì–‘í•œ ë²¡í„° DBì˜ refresh ì „ëµê³¼ ìµœì í™” ë°©ë²•
        """
        return {
            "opensearch": {
                "refresh_strategies": [
                    {
                        "name": "immediate_refresh",
                        "method": "refresh=True parameter",
                        "pros": "ì¦‰ì‹œ ê²€ìƒ‰ ê°€ëŠ¥",
                        "cons": "ì„±ëŠ¥ ì˜¤ë²„í—¤ë“œ",
                        "use_case": "ì‹¤ì‹œê°„ ê²€ìƒ‰ì´ ì¤‘ìš”í•œ ê²½ìš°"
                    },
                    {
                        "name": "wait_for_refresh",
                        "method": "refresh=wait_for parameter",
                        "pros": "ì¼ê´€ì„± ë³´ì¥",
                        "cons": "ì‘ë‹µ ì‹œê°„ ì¦ê°€",
                        "use_case": "ë°ì´í„° ì¼ê´€ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°"
                    },
                    {
                        "name": "delayed_trigger",
                        "method": "asyncio.sleep + manual refresh",
                        "pros": "ì„±ëŠ¥ê³¼ ì¼ê´€ì„± ê· í˜•",
                        "cons": "ë³µì¡í•œ ë¡œì§",
                        "use_case": "ëŒ€ë¶€ë¶„ì˜ ìš´ì˜ í™˜ê²½"
                    }
                ],
                "recommended_delay": "1-2 seconds",
                "refresh_interval_setting": "index.refresh_interval: 1s"
            },
            "pinecone": {
                "refresh_strategies": [
                    {
                        "name": "eventual_consistency",
                        "method": "ê¸°ë³¸ ë™ì‘",
                        "pros": "ë†’ì€ ì„±ëŠ¥",
                        "cons": "ì•½ê°„ì˜ ì§€ì—°",
                        "use_case": "ëŒ€ë¶€ë¶„ì˜ ê²½ìš°"
                    },
                    {
                        "name": "polling_check",
                        "method": "describe_index_stats í™•ì¸",
                        "pros": "ì •í™•í•œ ìƒíƒœ í™•ì¸",
                        "cons": "ì¶”ê°€ API í˜¸ì¶œ",
                        "use_case": "ì •í™•ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°"
                    }
                ],
                "recommended_delay": "2-5 seconds",
                "consistency_model": "eventual"
            },
            "pgvector": {
                "refresh_strategies": [
                    {
                        "name": "transaction_commit",
                        "method": "COMMIT í›„ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥",
                        "pros": "ì¦‰ì‹œ ì¼ê´€ì„±",
                        "cons": "ì—†ìŒ",
                        "use_case": "ëª¨ë“  ê²½ìš°"
                    }
                ],
                "recommended_delay": "0 seconds",
                "consistency_model": "immediate"
            }
        }
    
    async def _generate_embedding(self, text: str) -> Optional[List[float]]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ì„ë² ë”© ë²¡í„° ìƒì„±
        
        [Google Native í‘œì¤€] Vertex AI text-embedding-004 ëª¨ë¸ ì „ìš© (768ì°¨ì›)
        
        âš ï¸ ì¤‘ìš”: OpenAI fallbackì€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.
        ì´ìœ : OpenAI (1536ì°¨ì›)ì™€ Vertex AI (768ì°¨ì›)ëŠ” ë™ì¼ ì¸ë±ìŠ¤ì— ì €ì¥ ë¶ˆê°€
        ë™ì¼í•œ ë²¡í„° DB ì¸ë±ìŠ¤(corrections)ì—ëŠ” ë‹¨ì¼ ì°¨ì›ì˜ ë²¡í„°ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.
        """
        try:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Vertex AI text-embedding-004 (768ì°¨ì›) - Google Native í‘œì¤€
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            try:
                from vertexai.language_models import TextEmbeddingModel
                
                model = TextEmbeddingModel.from_pretrained(EMBEDDING_MODEL)
                embeddings = model.get_embeddings([text])
                
                if embeddings and len(embeddings) > 0:
                    vector = embeddings[0].values
                    
                    # ì°¨ì› ê²€ì¦ (ë””ë²„ê¹…ìš©)
                    if len(vector) != EMBEDDING_DIMENSION:
                        logger.warning(
                            f"Unexpected embedding dimension: {len(vector)}, "
                            f"expected: {EMBEDDING_DIMENSION}"
                        )
                    
                    return vector
                else:
                    logger.warning("Vertex AI returned empty embeddings")
                    return self._generate_dummy_embedding(text)
                    
            except ImportError:
                logger.error(
                    "Vertex AI SDK not available. "
                    "Install: pip install google-cloud-aiplatform"
                )
                return self._generate_dummy_embedding(text)
                
            except Exception as e:
                logger.error(f"Vertex AI embedding failed: {e}")
                # Vertex AI ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ì„ë² ë”© (OpenAI í´ë°± ì œê±°)
                # ì´ìœ : ì°¨ì› ë¶ˆì¼ì¹˜ ë°©ì§€
                return self._generate_dummy_embedding(text)
            
        except Exception as e:
            logger.error(f"Failed to generate embedding: {str(e)}")
            return self._generate_dummy_embedding(text)
    
    def _generate_dummy_embedding(self, text: str) -> List[float]:
        """
        ë”ë¯¸ ì„ë² ë”© ìƒì„± (Vertex AI API ë¶ˆê°€ ì‹œ í´ë°±)
        
        768ì°¨ì› ë²¡í„° (Vertex AI text-embedding-004 í‘œì¤€ ì°¨ì›)
        """
        import hashlib
        text_hash = hashlib.md5(text.encode()).hexdigest()
        # EMBEDDING_DIMENSION(768)ì°¨ì› ë²¡í„° ìƒì„±
        dummy_vector = [float(int(text_hash[i:i+2], 16)) / 255.0 for i in range(0, min(len(text_hash), 32), 2)]
        dummy_vector.extend([0.0] * (EMBEDDING_DIMENSION - len(dummy_vector)))
        return dummy_vector[:EMBEDDING_DIMENSION]
    
    async def _store_in_vector_db_with_metadata(
        self,
        correction_log: CorrectionLog,
        embedding_vector: List[float],
        embedding_text: str
    ) -> Optional[str]:
        """
        ê°œì„ ì‚¬í•­ 1: í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ë²¡í„° DBì— ì €ì¥
        
        ë²¡í„° DBì— ì €ì¥ ì‹œ ë‹¤ìŒ ë©”íƒ€ë°ì´í„°ë¥¼ ë°˜ë“œì‹œ í¬í•¨:
        - user_id: ì‚¬ìš©ìë³„ í•„í„°ë§
        - task_category: íƒœìŠ¤í¬ ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§  
        - context_scope: ì»¨í…ìŠ¤íŠ¸ ë²”ìœ„ë³„ í•„í„°ë§
        - node_type: ë…¸ë“œ íƒ€ì…ë³„ í•„í„°ë§
        - workflow_domain: ì›Œí¬í”Œë¡œìš° ë„ë©”ì¸ë³„ í•„í„°ë§
        """
        try:
            # í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° êµ¬ì„±
            filter_metadata = {
                # í•„ìˆ˜ í•„í„°ë§ í•„ë“œë“¤
                "user_id": correction_log.user_id,
                "task_category": correction_log.task_category.value,
                "context_scope": correction_log.context_scope,
                "node_type": correction_log.node_type,
                "workflow_domain": correction_log.workflow_domain,
                
                # ì¶”ê°€ í•„í„°ë§ í•„ë“œë“¤
                "correction_type": correction_log.correction_type.value if correction_log.correction_type else None,
                "workflow_id": correction_log.workflow_id,
                "node_id": correction_log.node_id,
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­ (í•„í„°ë§ ë° ë­í‚¹ìš©)
                "edit_distance": correction_log.edit_distance,
                "correction_time_seconds": correction_log.correction_time_seconds,
                "user_confirmed_valuable": correction_log.user_confirmed_valuable,
                
                # íƒ€ì„ìŠ¤íƒ¬í”„ (ì‹œê°„ ê¸°ë°˜ í•„í„°ë§ìš©)
                "created_at": correction_log.created_at.isoformat(),
                "updated_at": correction_log.updated_at.isoformat(),
                
                # ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„° (ì„¸ë°€í•œ í•„í„°ë§ìš©)
                **correction_log.extracted_metadata
            }
            
            # None ê°’ ì œê±°
            filter_metadata = {k: v for k, v in filter_metadata.items() if v is not None}
            
            # ë²¡í„° DB ì €ì¥ ë°ì´í„° êµ¬ì„±
            vector_document = {
                "id": f"correction_{correction_log.sk}",
                "vector": embedding_vector,
                "text": embedding_text,
                "metadata": filter_metadata
            }
            
            # ì‹¤ì œ ë²¡í„° DB ì €ì¥ (OpenSearch/Pinecone/pgvector ë“±)
            embedding_id = await self._store_vector_document(vector_document)
            
            if embedding_id:
                logger.info(f"Vector stored with metadata: {embedding_id}, filters: {list(filter_metadata.keys())}")
            
            return embedding_id
            
        except Exception as e:
            logger.error(f"Failed to store vector with metadata: {str(e)}")
            return None
    
    async def _store_vector_document(self, vector_document: Dict[str, Any]) -> Optional[str]:
        """
        ì‹¤ì œ ë²¡í„° DBì— ë¬¸ì„œ ì €ì¥ (ê°œì„ ì‚¬í•­ 6: ì¸ë±ì‹± ì§€ì—° ì²˜ë¦¬ í¬í•¨)
        
        ë²¡í„° DBë³„ êµ¬í˜„:
        - OpenSearch: ì¸ë±ìŠ¤ì— ë¬¸ì„œ ì €ì¥ + refresh ì˜µì…˜
        - pgvector: í…Œì´ë¸”ì— ë²¡í„°ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥
        """
        try:
            vector_db_type = os.environ.get('VECTOR_DB_TYPE', 'opensearch')
            embedding_id = vector_document["id"]
            
            if vector_db_type == 'opensearch' and self.vector_client:
                # OpenSearch ì €ì¥
                response = await self.vector_client.index(
                    index="corrections",
                    body={
                        "vector": vector_document["vector"],
                        "text": vector_document["text"],
                        **vector_document.get("metadata", {})
                    },
                    id=embedding_id,
                    refresh=True if self.enable_refresh_wait else False
                )
                logger.debug(f"OpenSearch document stored: {embedding_id}")
                return response.get("_id", embedding_id)
                
            elif vector_db_type == 'pgvector' and hasattr(self, '_pgvector_config'):
                # pgvector ì €ì¥
                import asyncpg
                import json as json_lib
                
                conn = await asyncpg.connect(**self._pgvector_config)
                try:
                    # í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ìƒì„± (768ì°¨ì› ê³ ì •)
                    await conn.execute(f'''
                        CREATE TABLE IF NOT EXISTS corrections (
                            id TEXT PRIMARY KEY,
                            vector vector({EMBEDDING_DIMENSION}),
                            text TEXT,
                            metadata JSONB,
                            created_at TIMESTAMP DEFAULT NOW()
                        )
                    ''')
                    
                    # ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± (ì—†ìœ¼ë©´)
                    await conn.execute('''
                        CREATE INDEX IF NOT EXISTS corrections_vector_idx 
                        ON corrections USING ivfflat (vector vector_cosine_ops)
                        WITH (lists = 100)
                    ''')
                    
                    # ë¬¸ì„œ UPSERT
                    await conn.execute('''
                        INSERT INTO corrections (id, vector, text, metadata)
                        VALUES ($1, $2, $3, $4)
                        ON CONFLICT (id) DO UPDATE SET
                            vector = EXCLUDED.vector,
                            text = EXCLUDED.text,
                            metadata = EXCLUDED.metadata
                    ''', 
                        embedding_id,
                        str(vector_document["vector"]),  # pgvector í˜•ì‹
                        vector_document["text"],
                        json_lib.dumps(vector_document.get("metadata", {}))
                    )
                    
                    logger.debug(f"pgvector document stored: {embedding_id}")
                    return embedding_id
                    
                finally:
                    await conn.close()
            
            else:
                # Fallback: ì‹œë®¬ë ˆì´ì…˜
                await asyncio.sleep(0.1)
                logger.debug(f"Vector document simulated: {embedding_id}")
                return embedding_id
            
        except Exception as e:
            logger.error(f"Failed to store vector document: {str(e)}")
            return None
    
    def get_hybrid_search_example(self) -> Dict[str, Any]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ ê²€ìƒ‰ ì˜ˆì‹œ
        
        ë²¡í„° DBì—ì„œ íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ êµ¬ì¡° ì˜ˆì‹œ
        """
        return {
            "opensearch_example": {
                "query": {
                    "bool": {
                        "must": [
                            {
                                "knn": {
                                    "vector": {
                                        "vector": "[query_embedding_vector]",
                                        "k": 10
                                    }
                                }
                            }
                        ],
                        "filter": [
                            {"term": {"user_id": "user123"}},
                            {"term": {"task_category": "email"}},
                            {"term": {"context_scope": "global"}},
                            {"range": {"edit_distance": {"gte": 5}}},
                            {"range": {"created_at": {"gte": "2024-01-01"}}}
                        ]
                    }
                }
            },
            "pinecone_example": {
                "vector": "[query_embedding_vector]",
                "filter": {
                    "user_id": {"$eq": "user123"},
                    "task_category": {"$eq": "email"},
                    "context_scope": {"$eq": "global"},
                    "edit_distance": {"$gte": 5}
                },
                "top_k": 10
            },
            "pgvector_example": """
                SELECT id, text, metadata, 
                       vector <-> %s AS distance
                FROM corrections 
                WHERE metadata->>'user_id' = %s 
                  AND metadata->>'task_category' = %s
                  AND metadata->>'context_scope' = %s
                  AND (metadata->>'edit_distance')::int >= %s
                ORDER BY vector <-> %s 
                LIMIT 10
            """
        }
    
    async def _update_correction_sync_status(self, correction_log: CorrectionLog) -> bool:
        """DynamoDBì—ì„œ ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸"""
        try:
            update_expression = 'SET vector_sync_status = :status, vector_sync_attempts = :attempts, updated_at = :updated'
            expression_values = {
                ':status': correction_log.vector_sync_status.value,
                ':attempts': correction_log.vector_sync_attempts,
                ':updated': datetime.now(timezone.utc).isoformat()
            }
            
            if correction_log.embedding_id:
                update_expression += ', embedding_id = :embedding_id'
                expression_values[':embedding_id'] = correction_log.embedding_id
            
            if correction_log.vector_sync_error:
                update_expression += ', vector_sync_error = :error'
                expression_values[':error'] = correction_log.vector_sync_error
            
            if correction_log.last_vector_sync_attempt:
                update_expression += ', last_vector_sync_attempt = :last_attempt'
                expression_values[':last_attempt'] = correction_log.last_vector_sync_attempt.isoformat()
            
            self.correction_table.update_item(
                Key={
                    'pk': correction_log.pk,
                    'sk': correction_log.sk
                },
                UpdateExpression=update_expression,
                ExpressionAttributeValues=expression_values
            )
            
            return True
            
        except Exception as e:
            logger.error(f"Failed to update correction sync status: {str(e)}")
            return False
    
    async def _get_pending_sync_corrections(
        self,
        user_id: str,
        limit: int
    ) -> List[Dict[str, Any]]:
        """ë™ê¸°í™” ëŒ€ê¸° ì¤‘ì¸ ìˆ˜ì • ë¡œê·¸ë“¤ ì¡°íšŒ"""
        try:
            response = self.correction_table.query(
                KeyConditionExpression='pk = :pk',
                FilterExpression='vector_sync_status IN (:pending, :failed, :retry)',
                ExpressionAttributeValues={
                    ':pk': f'user#{user_id}',
                    ':pending': VectorSyncStatus.PENDING.value,
                    ':failed': VectorSyncStatus.FAILED.value,
                    ':retry': VectorSyncStatus.RETRY.value
                },
                Limit=limit
            )
            
            return response.get('Items', [])
            
        except Exception as e:
            logger.error(f"Failed to get pending sync corrections: {str(e)}")
            return []
    
    async def _get_failed_sync_corrections(
        self,
        user_id: str,
        cutoff_time: datetime
    ) -> List[Dict[str, Any]]:
        """ì‹¤íŒ¨í•œ ë™ê¸°í™” ìˆ˜ì • ë¡œê·¸ë“¤ ì¡°íšŒ"""
        try:
            response = self.correction_table.query(
                KeyConditionExpression='pk = :pk',
                FilterExpression='vector_sync_status = :failed AND last_vector_sync_attempt > :cutoff',
                ExpressionAttributeValues={
                    ':pk': f'user#{user_id}',
                    ':failed': VectorSyncStatus.FAILED.value,
                    ':cutoff': cutoff_time.isoformat()
                }
            )
            
            return response.get('Items', [])
            
        except Exception as e:
            logger.error(f"Failed to get failed sync corrections: {str(e)}")
            return []

    async def store_correction_vector(
        self,
        user_id: str,
        correction_data: Dict[str, Any],
        quality_result: Dict[str, Any]
    ) -> bool:
        """
        ê°€ì¹˜ ìˆëŠ” ìˆ˜ì •ì„ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            user_id: ì‚¬ìš©ì ID
            correction_data: ìˆ˜ì • ë°ì´í„°
            quality_result: í’ˆì§ˆ í‰ê°€ ê²°ê³¼
            
        Returns:
            ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ì„ë² ë”© ìƒì„±
            text_content = self._extract_text_for_embedding(correction_data)
            if not text_content:
                logger.warning("No text content available for embedding")
                return False
                
            embedding = await self._generate_embedding(text_content)
            if not embedding:
                logger.error("Failed to generate embedding")
                return False
            
            # ë²¡í„° ë¬¸ì„œ ìƒì„±
            vector_document = {
                "vector": embedding,
                "text": text_content,
                "metadata": {
                    "user_id": user_id,
                    "correction_sk": correction_data.get("sk"),
                    "task_category": correction_data.get("taskCategory"),
                    "correction_type": correction_data.get("correctionType"),
                    "quality_score": quality_result.get("quality_score", 0),
                    "is_valuable": quality_result.get("is_valuable", False),
                    "created_at": datetime.now(timezone.utc).isoformat()
                }
            }
            
            # ë²¡í„° DBì— ì €ì¥
            success = await self._store_vector_document(vector_document)
            if success:
                logger.info(f"Stored correction vector: {correction_data.get('sk')}")
                
                # DynamoDBì— ë²¡í„° ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸
                try:
                    self.correction_table.update_item(
                        Key={
                            'pk': f'user#{user_id}',
                            'sk': correction_data.get('sk')
                        },
                        UpdateExpression='SET vector_sync_status = :status, updated_at = :updated',
                        ExpressionAttributeValues={
                            ':status': 'SUCCESS',
                            ':updated': datetime.now(timezone.utc).isoformat()
                        }
                    )
                except Exception as e:
                    logger.error(f"Failed to update DynamoDB after vector storage: {str(e)}")
                    # DynamoDB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ (ë²¡í„°ëŠ” ì´ë¯¸ ì €ì¥ë¨)
            
            return success
            
        except Exception as e:
            logger.error(f"Failed to store correction vector: {str(e)}")
            return False

    def _extract_text_for_embedding(self, correction_data: Dict[str, Any]) -> str:
        """ìˆ˜ì • ë°ì´í„°ì—ì„œ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        # instructionê³¼ correctionì„ ê²°í•©
        instruction = correction_data.get("instruction", "")
        correction = correction_data.get("correction", "")
        text = f"{instruction} {correction}".strip()
        
        # í† í° ì œí•œ ê³ ë ¤ (ëŒ€ëµ 1í† í° = 4ë¬¸ì, 8000í† í° ì œí•œ)
        max_chars = 8000 * 4
        if len(text) > max_chars:
            logger.warning(f"Text too long ({len(text)} chars), truncating to {max_chars}")
            text = text[:max_chars]
        
        return text

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [DEPRECATED] ì•„ë˜ ë©”ì„œë“œë“¤ì€ ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    # ìœ„ìª½ì˜ _generate_embedding ë©”ì„œë“œ(Vertex AI ì „ìš©)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # async def _generate_embedding_legacy(self, text: str) -> Optional[List[float]]:
    #     """
    #     [DEPRECATED] OpenAI ì„ë² ë”© - ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    #     
    #     ì´ìœ : OpenAI (1536ì°¨ì›)ì™€ Vertex AI (768ì°¨ì›) í˜¼í•© ì‚¬ìš© ë¶ˆê°€
    #     ë™ì¼í•œ ë²¡í„° DB ì¸ë±ìŠ¤ì—ëŠ” ë‹¨ì¼ ì°¨ì›ì˜ ë²¡í„°ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.
    #     
    #     ëŒ€ì‹  ìœ„ìª½ì˜ _generate_embedding ë©”ì„œë“œ(Vertex AI ì „ìš©)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    #     """
    #     pass

    async def _store_vector_document(self, vector_document: Dict[str, Any]) -> bool:
        """ë²¡í„° ë¬¸ì„œë¥¼ DBì— ì €ì¥"""
        try:
            if not self.vector_client:
                logger.warning("Vector client not initialized, skipping storage")
                return False
                
            # OpenSearch ì¸ë±ìŠ¤ì— ì €ì¥
            index_name = os.environ.get('VECTOR_INDEX_NAME', 'corrections')
            doc_id = vector_document['metadata']['correction_sk']
            
            # OpenSearch ë¬¸ì„œ í˜•ì‹
            document = {
                "vector": vector_document["vector"],
                "text": vector_document["text"],
                **vector_document["metadata"]
            }
            
            response = await self.vector_client.index(
                index=index_name,
                id=doc_id,
                body=document,
                refresh=True  # ì¦‰ì‹œ ê²€ìƒ‰ ê°€ëŠ¥í•˜ë„ë¡
            )
            
            if response.get('result') == 'created' or response.get('result') == 'updated':
                return True
            else:
                logger.error(f"Unexpected OpenSearch response: {response}")
                return False
                
        except Exception as e:
            logger.error(f"Failed to store vector document: {str(e)}")
            return False