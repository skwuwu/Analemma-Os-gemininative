# -*- coding: utf-8 -*-
"""
Plan Briefing Service

ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì „ ë¯¸ë¦¬ë³´ê¸°ë¥¼ ìƒì„±í•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
ì›Œí¬í”Œë¡œìš° ì„¤ì •ê³¼ ì´ˆê¸° ìƒíƒœë¥¼ ë¶„ì„í•˜ì—¬ ì˜ˆìƒ ì‹¤í–‰ ê³„íšê³¼ ê²°ê³¼ë¬¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

import json
import os
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta

# ëª¨ë¸ ì„í¬íŠ¸
try:
    from src.models.plan_briefing import (
        PlanBriefing, PlanStep, DraftResult, RiskLevel
    )
except ImportError:
    from src.models.plan_briefing import (
        PlanBriefing, PlanStep, DraftResult, RiskLevel
    )

logger = logging.getLogger(__name__)

# LLM í´ë¼ì´ì–¸íŠ¸ (OpenAI ë˜ëŠ” ë‹¤ë¥¸ í”„ë¡œë°”ì´ë”)
try:
    from openai import AsyncOpenAI
    HAS_OPENAI = True
except ImportError:
    try:
        import openai
        HAS_OPENAI = True
    except ImportError:
        HAS_OPENAI = False
    logger.warning("OpenAI not available, using mock briefing generation")


class PlanBriefingService:
    """
    ì‹¤í–‰ ì „ ê³„íš ë¸Œë¦¬í•‘ ìƒì„± ì„œë¹„ìŠ¤
    
    ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì§ì „ì— ì‚¬ìš©ìì—ê²Œ ë¯¸ë¦¬ë³´ê¸°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    - ì‹¤í–‰ ìˆœì„œ ë¶„ì„
    - ê° ë‹¨ê³„ ì˜ˆìƒ ë™ì‘
    - ì˜ˆìƒ ì†Œìš” ì‹œê°„
    - ìœ„í—˜ ë¶„ì„
    - ì˜ˆìƒ ê²°ê³¼ë¬¼ ì´ˆì•ˆ
    """
    
    def __init__(self, openai_api_key: Optional[str] = None):
        """
        Args:
            openai_api_key: OpenAI API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        """
        self.api_key = openai_api_key or os.environ.get("OPENAI_API_KEY")
        if HAS_OPENAI and self.api_key:
            try:
                # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© (v1.0+)
                self.client = AsyncOpenAI(api_key=self.api_key)
            except NameError:
                # êµ¬ë²„ì „ í˜¸í™˜ì„± ìœ ì§€
                openai.api_key = self.api_key
                self.client = None
    
    SYSTEM_PROMPT = """You are an expert workflow analyst. 
Your job is to analyze a workflow configuration and predict:
1. The exact execution order of nodes
2. What each node will do with the given inputs
3. The expected outputs/results
4. Potential risks or side effects

Be specific and actionable. The user should understand exactly what will happen.
Always respond in the same language as the user's workflow names and descriptions.
If the workflow is in Korean, respond in Korean. If in English, respond in English."""

    # ë…¸ë“œ íƒ€ì…ë³„ ê¸°ë³¸ ì†Œìš” ì‹œê°„ (ì´ˆ)
    DEFAULT_DURATIONS = {
        "llm": 5,
        "hitp": 30,  # Human-in-the-loopì€ ëŒ€ê¸° ì‹œê°„ì´ ê¹€
        "api_call": 3,
        "email": 2,
        "condition": 1,
        "transform": 1,
        "aggregator": 2,
        "default": 2
    }
    
    # ì™¸ë¶€ ì˜í–¥ì´ ìˆëŠ” ë…¸ë“œ íƒ€ì…
    SIDE_EFFECT_TYPES = {"email", "api_call", "webhook", "notification", "payment"}

    def __init__(self, openai_api_key: Optional[str] = None):
        """
        Args:
            openai_api_key: OpenAI API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        """
        self.api_key = openai_api_key or os.environ.get("OPENAI_API_KEY")
        if HAS_OPENAI and self.api_key:
            openai.api_key = self.api_key

    async def generate_briefing(
        self,
        workflow_config: Dict[str, Any],
        initial_statebag: Dict[str, Any],
        user_context: Optional[Dict[str, Any]] = None,
        use_llm: bool = True
    ) -> PlanBriefing:
        """
        ì›Œí¬í”Œë¡œìš° ì„¤ì •ê³¼ ì´ˆê¸° ìƒíƒœë¥¼ ë¶„ì„í•˜ì—¬ ë¸Œë¦¬í•‘ ìƒì„±
        
        Args:
            workflow_config: ì›Œí¬í”Œë¡œìš° ì„¤ì • (nodes, edges í¬í•¨)
            initial_statebag: ì´ˆê¸° ìƒíƒœ ë°ì´í„°
            user_context: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ (ì„ íƒ)
            use_llm: LLMì„ ì‚¬ìš©í•˜ì—¬ ìƒì„¸ ë¶„ì„í• ì§€ ì—¬ë¶€
            
        Returns:
            PlanBriefing: ìƒì„±ëœ ë¸Œë¦¬í•‘
        """
        nodes = workflow_config.get('nodes', [])
        edges = workflow_config.get('edges', [])
        workflow_id = workflow_config.get('id', 'unknown')
        workflow_name = workflow_config.get('name', 'Unnamed Workflow')
        
        if use_llm and HAS_OPENAI and self.api_key:
            try:
                return await self._generate_with_llm(
                    workflow_config, initial_statebag, user_context
                )
            except Exception as e:
                logger.warning(f"LLM briefing generation failed, falling back to rule-based: {e}")
        
        # ê·œì¹™ ê¸°ë°˜ ë¸Œë¦¬í•‘ ìƒì„± (í´ë°± ë˜ëŠ” LLM ë¹„í™œì„±í™” ì‹œ)
        return self._generate_rule_based(
            workflow_id, workflow_name, nodes, edges, initial_statebag
        )

    async def _generate_with_llm(
        self,
        workflow_config: Dict[str, Any],
        initial_statebag: Dict[str, Any],
        user_context: Optional[Dict[str, Any]]
    ) -> PlanBriefing:
        """LLMì„ ì‚¬ìš©í•œ ìƒì„¸ ë¸Œë¦¬í•‘ ìƒì„±"""
        
        analysis_prompt = self._build_analysis_prompt(
            workflow_config.get('nodes', []),
            workflow_config.get('edges', []),
            initial_statebag,
            user_context
        )
        
        if hasattr(self, 'client') and self.client:
            # AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© (v1.0+)
            response = await self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": self.SYSTEM_PROMPT},
                    {"role": "user", "content": analysis_prompt}
                ],
                response_format={"type": "json_object"},
                max_tokens=2000,
                temperature=0.3  # ì¼ê´€ëœ ë¶„ì„ì„ ìœ„í•´ ë‚®ì€ temperature
            )
        else:
            # êµ¬ë²„ì „ í˜¸í™˜ì„± ìœ ì§€
            response = await openai.ChatCompletion.acreate(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": self.SYSTEM_PROMPT},
                    {"role": "user", "content": analysis_prompt}
                ],
                response_format={"type": "json_object"},
                max_tokens=2000,
                temperature=0.3  # ì¼ê´€ëœ ë¶„ì„ì„ ìœ„í•´ ë‚®ì€ temperature
            )
        
        analysis = json.loads(response.choices[0].message.content)
        
        return self._build_briefing_from_analysis(workflow_config, analysis)

    def _build_analysis_prompt(
        self,
        nodes: List[Dict],
        edges: List[Dict],
        statebag: Dict[str, Any],
        context: Optional[Dict]
    ) -> str:
        """LLM ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        # ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹
        safe_statebag = self._mask_sensitive_data(statebag)
        
        nodes_desc = json.dumps(nodes, indent=2, ensure_ascii=False)
        edges_desc = json.dumps(edges, indent=2, ensure_ascii=False)
        statebag_desc = json.dumps(safe_statebag, indent=2, ensure_ascii=False)
        
        return f"""Analyze this workflow and predict its execution:

## Workflow Nodes
{nodes_desc}

## Workflow Edges (Execution Flow)
{edges_desc}

## Initial State (Input Data)
{statebag_desc}

## Analysis Required (respond in JSON):
{{
  "execution_order": ["node_id1", "node_id2", ...],
  "steps": [
    {{
      "node_id": "string",
      "node_name": "string",
      "node_type": "llm|hitp|api_call|email|condition|transform|aggregator",
      "action_description": "What this node will do in plain language",
      "estimated_duration_seconds": number,
      "risk_level": "low|medium|high",
      "risk_description": "Why this risk level (if medium/high)",
      "expected_input": "Summary of input data",
      "expected_output": "Summary of expected output",
      "external_systems": ["system1", "system2"],
      "has_side_effect": boolean,
      "is_conditional": boolean,
      "condition_description": "Condition description if conditional"
    }}
  ],
  "draft_results": [
    {{
      "result_type": "email|document|data|notification|api_call",
      "title": "Title of the result",
      "content_preview": "Preview of the actual content that will be generated",
      "recipients": ["email1@example.com"],
      "warnings": ["Warning message if any"],
      "requires_review": boolean
    }}
  ],
  "overall_risk": "low|medium|high",
  "warnings": ["Warning message 1", ...],
  "requires_confirmation": boolean,
  "confirmation_message": "Message if confirmation required",
  "summary": "1-2 sentence summary of what this workflow will do",
  "confidence": 0.0-1.0
}}"""

    def _generate_rule_based(
        self,
        workflow_id: str,
        workflow_name: str,
        nodes: List[Dict],
        edges: List[Dict],
        statebag: Dict[str, Any]
    ) -> PlanBriefing:
        """ê·œì¹™ ê¸°ë°˜ ë¸Œë¦¬í•‘ ìƒì„± (LLM ì—†ì´)"""
        
        # ì‹¤í–‰ ìˆœì„œ ê²°ì • (í† í´ë¡œì§€ ì •ë ¬)
        execution_order = self._determine_execution_order(nodes, edges)
        
        # ê° ë‹¨ê³„ ìƒì„±
        steps = []
        total_duration = 0
        max_risk = RiskLevel.LOW
        warnings = []
        has_confirmation_required = False
        
        for i, node_id in enumerate(execution_order):
            node = next((n for n in nodes if n.get('id') == node_id), None)
            if not node:
                continue
            
            node_type = node.get('type', 'default')
            node_name = node.get('data', {}).get('label', node.get('label', node_id))
            
            duration = self.DEFAULT_DURATIONS.get(node_type, self.DEFAULT_DURATIONS['default'])
            has_side_effect = node_type in self.SIDE_EFFECT_TYPES
            
            # ìœ„í—˜ ìˆ˜ì¤€ ê²°ì •
            risk_level = RiskLevel.LOW
            risk_description = None
            if has_side_effect:
                risk_level = RiskLevel.MEDIUM
                risk_description = f"{node_type} ë…¸ë“œê°€ ì™¸ë¶€ ì‹œìŠ¤í…œì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤"
                if node_type in {"payment", "email"}:
                    risk_level = RiskLevel.HIGH
                    has_confirmation_required = True
            
            # ìµœê³  ìœ„í—˜ ìˆ˜ì¤€ ì¶”ì 
            if risk_level == RiskLevel.HIGH:
                max_risk = RiskLevel.HIGH
            elif risk_level == RiskLevel.MEDIUM and max_risk != RiskLevel.HIGH:
                max_risk = RiskLevel.MEDIUM
            
            step = PlanStep(
                step_number=i + 1,
                node_id=node_id,
                node_name=node_name,
                node_type=node_type,
                action_description=self._generate_action_description(node),
                estimated_duration_seconds=duration,
                risk_level=risk_level,
                risk_description=risk_description,
                has_external_side_effect=has_side_effect,
                external_systems=[node_type] if has_side_effect else []
            )
            
            steps.append(step)
            total_duration += duration
        
        # ê²½ê³  ìƒì„±
        if max_risk == RiskLevel.HIGH:
            warnings.append("âš ï¸ ì´ ì›Œí¬í”Œë¡œìš°ëŠ” ë˜ëŒë¦´ ìˆ˜ ì—†ëŠ” ì‘ì—…ì„ í¬í•¨í•©ë‹ˆë‹¤")
        
        # ìš”ì•½ ìƒì„±
        summary = self._generate_summary(workflow_name, len(steps), total_duration, max_risk)
        
        return PlanBriefing(
            workflow_id=workflow_id,
            workflow_name=workflow_name,
            summary=summary,
            total_steps=len(steps),
            estimated_total_duration_seconds=total_duration,
            steps=steps,
            draft_results=[],  # ê·œì¹™ ê¸°ë°˜ì—ì„œëŠ” ê²°ê³¼ë¬¼ ì˜ˆì¸¡ ì œí•œ
            overall_risk_level=max_risk,
            warnings=warnings,
            requires_confirmation=has_confirmation_required,
            confirmation_message="ì™¸ë¶€ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì‘ì—…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ?" if has_confirmation_required else None,
            confidence_score=0.6  # ê·œì¹™ ê¸°ë°˜ì€ ë‚®ì€ ì‹ ë¢°ë„
        )

    def _determine_execution_order(
        self,
        nodes: List[Dict],
        edges: List[Dict]
    ) -> List[str]:
        """í† í´ë¡œì§€ ì •ë ¬ë¡œ ì‹¤í–‰ ìˆœì„œ ê²°ì •"""
        from collections import defaultdict, deque
        
        node_ids = [n.get('id') for n in nodes if n.get('id')]
        
        # ì§„ì… ì°¨ìˆ˜ ê³„ì‚°
        in_degree = defaultdict(int)
        adjacency = defaultdict(list)
        
        for node_id in node_ids:
            in_degree[node_id] = 0
        
        for edge in edges:
            source = edge.get('source')
            target = edge.get('target')
            if source and target:
                adjacency[source].append(target)
                in_degree[target] += 1
        
        # ì§„ì… ì°¨ìˆ˜ê°€ 0ì¸ ë…¸ë“œë¶€í„° ì‹œì‘
        queue = deque([n for n in node_ids if in_degree[n] == 0])
        order = []
        
        while queue:
            node = queue.popleft()
            order.append(node)
            
            for neighbor in adjacency[node]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)
        
        return order

    def _generate_action_description(self, node: Dict) -> str:
        """ë…¸ë“œì— ëŒ€í•œ ë™ì‘ ì„¤ëª… ìƒì„±"""
        node_type = node.get('type', 'default')
        node_data = node.get('data', {})
        label = node_data.get('label', node.get('label', ''))
        
        descriptions = {
            "llm": f"AIê°€ '{label}'ì„(ë¥¼) ì²˜ë¦¬í•©ë‹ˆë‹¤",
            "hitp": f"ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤: {label}",
            "api_call": f"ì™¸ë¶€ APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤: {label}",
            "email": f"ì´ë©”ì¼ì„ ë°œì†¡í•©ë‹ˆë‹¤: {label}",
            "condition": f"ì¡°ê±´ì„ í™•ì¸í•©ë‹ˆë‹¤: {label}",
            "transform": f"ë°ì´í„°ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤: {label}",
            "aggregator": f"ê²°ê³¼ë¥¼ ì§‘ê³„í•©ë‹ˆë‹¤: {label}",
        }
        
        return descriptions.get(node_type, f"{label} ë…¸ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤")

    def _generate_summary(
        self,
        workflow_name: str,
        step_count: int,
        total_duration: int,
        risk_level: RiskLevel
    ) -> str:
        """ë¸Œë¦¬í•‘ ìš”ì•½ ìƒì„±"""
        risk_text = {
            RiskLevel.LOW: "",
            RiskLevel.MEDIUM: " ì¼ë¶€ ì™¸ë¶€ ì—°ë™ì´ í¬í•¨ë©ë‹ˆë‹¤.",
            RiskLevel.HIGH: " âš ï¸ ë˜ëŒë¦´ ìˆ˜ ì—†ëŠ” ì‘ì—…ì´ í¬í•¨ë©ë‹ˆë‹¤."
        }
        
        return f"'{workflow_name}' ì›Œí¬í”Œë¡œìš°ëŠ” {step_count}ë‹¨ê³„ë¡œ êµ¬ì„±ë˜ë©°, ì•½ {total_duration}ì´ˆê°€ ì†Œìš”ë©ë‹ˆë‹¤.{risk_text[risk_level]}"

    def _build_briefing_from_analysis(
        self,
        workflow_config: Dict[str, Any],
        analysis: Dict[str, Any]
    ) -> PlanBriefing:
        """LLM ë¶„ì„ ê²°ê³¼ë¥¼ PlanBriefing ëª¨ë¸ë¡œ ë³€í™˜"""
        
        steps = [
            PlanStep(
                step_number=i + 1,
                node_id=step['node_id'],
                node_name=step['node_name'],
                node_type=step.get('node_type', 'generic'),
                action_description=step['action_description'],
                estimated_duration_seconds=step['estimated_duration_seconds'],
                risk_level=RiskLevel(step['risk_level']),
                risk_description=step.get('risk_description'),
                expected_input_summary=step.get('expected_input'),
                expected_output_summary=step.get('expected_output'),
                has_external_side_effect=step.get('has_side_effect', False),
                external_systems=step.get('external_systems', []),
                is_conditional=step.get('is_conditional', False),
                condition_description=step.get('condition_description')
            )
            for i, step in enumerate(analysis.get('steps', []))
        ]
        
        draft_results = [
            DraftResult(
                result_type=dr['result_type'],
                title=dr['title'],
                content_preview=dr['content_preview'],
                recipients=dr.get('recipients'),
                warnings=dr.get('warnings', []),
                requires_review=dr.get('requires_review', False)
            )
            for dr in analysis.get('draft_results', [])
        ]
        
        total_duration = sum(s.estimated_duration_seconds for s in steps)
        
        return PlanBriefing(
            workflow_id=workflow_config.get('id', 'unknown'),
            workflow_name=workflow_config.get('name', 'Unnamed Workflow'),
            summary=analysis.get('summary', ''),
            total_steps=len(steps),
            estimated_total_duration_seconds=total_duration,
            steps=steps,
            draft_results=draft_results,
            overall_risk_level=RiskLevel(analysis.get('overall_risk', 'low')),
            warnings=analysis.get('warnings', []),
            requires_confirmation=analysis.get('requires_confirmation', False),
            confirmation_message=analysis.get('confirmation_message'),
            confidence_score=analysis.get('confidence', 0.8)
        )

    def _mask_sensitive_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹"""
        sensitive_keys = {'password', 'secret', 'api_key', 'token', 'credential', 'ssn', 'credit_card'}
        
        def mask_recursive(obj):
            if isinstance(obj, dict):
                return {
                    k: '***MASKED***' if any(s in k.lower() for s in sensitive_keys) else mask_recursive(v)
                    for k, v in obj.items()
                }
            elif isinstance(obj, list):
                return [mask_recursive(item) for item in obj]
            return obj
        
        return mask_recursive(data)

    async def validate_confirmation_token(
        self,
        token: str,
        workflow_id: str,
        user_id: str
    ) -> bool:
        """
        ì‹¤í–‰ ìŠ¹ì¸ í† í° ê²€ì¦
        
        Redis/DynamoDBì—ì„œ í† í°ì„ ì¡°íšŒí•˜ì—¬ ê²€ì¦í•©ë‹ˆë‹¤.
        """
        try:
            # í† í° ì €ì¥ì†Œ ì—°ë™ êµ¬í˜„
            token_service = ConfirmationTokenService()
            is_valid = await token_service.validate_token(
                token=token,
                workflow_id=workflow_id,
                user_id=user_id
            )
            
            if is_valid:
                # í† í° ì‚¬ìš© í›„ ë¬´íš¨í™” (ì¼íšŒì„±)
                await token_service.invalidate_token(token)
                logger.info(f"Confirmation token validated and invalidated for workflow {workflow_id}")
                return True
            else:
                logger.warning(f"Invalid confirmation token for workflow {workflow_id} by user {user_id}")
                return False
                
        except Exception as e:
            logger.error(f"Token validation error: {e}")
            return False

    async def generate_confirmation_token(
        self,
        workflow_id: str,
        user_id: str,
        expires_in_minutes: int = 30
    ) -> str:
        """
        ì‹¤í–‰ ìŠ¹ì¸ í† í° ìƒì„±
        
        Args:
            workflow_id: ì›Œí¬í”Œë¡œìš° ID
            user_id: ì‚¬ìš©ì ID
            expires_in_minutes: í† í° ë§Œë£Œ ì‹œê°„ (ë¶„)
            
        Returns:
            ìƒì„±ëœ í† í°
        """
        try:
            token_service = ConfirmationTokenService()
            token = await token_service.generate_token(
                workflow_id=workflow_id,
                user_id=user_id,
                expires_in_minutes=expires_in_minutes
            )
            
            logger.info(f"Confirmation token generated for workflow {workflow_id}")
            return token
            
        except Exception as e:
            logger.error(f"Token generation error: {e}")
            raise


class ConfirmationTokenService:
    """
    ì‹¤í–‰ ìŠ¹ì¸ í† í° ê´€ë¦¬ ì„œë¹„ìŠ¤
    
    Redis ë˜ëŠ” DynamoDBë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°ì„ ì €ì¥í•˜ê³  ê²€ì¦í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.use_redis = os.environ.get('REDIS_URL') is not None
        # ğŸš¨ [Critical Fix] ê¸°ë³¸ê°’ì„ template.yamlê³¼ ì¼ì¹˜ì‹œí‚´
        self.token_table = os.environ.get('CONFIRMATION_TOKENS_TABLE', 'ConfirmationTokensTable')
        self._redis_client = None
        self._dynamodb_table = None
    
    @property
    def redis_client(self):
        """Redis í´ë¼ì´ì–¸íŠ¸ ì§€ì—° ì´ˆê¸°í™”"""
        if self._redis_client is None and self.use_redis:
            try:
                import redis.asyncio as redis
                redis_url = os.environ.get('REDIS_URL')
                self._redis_client = redis.from_url(redis_url)
            except ImportError:
                logger.warning("Redis not available, falling back to DynamoDB")
                self.use_redis = False
        return self._redis_client
    
    @property
    def dynamodb_table(self):
        """DynamoDB í…Œì´ë¸” ì§€ì—° ì´ˆê¸°í™”"""
        if self._dynamodb_table is None and not self.use_redis:
            try:
                import boto3
                dynamodb = boto3.resource('dynamodb')
                self._dynamodb_table = dynamodb.Table(self.token_table)
            except Exception as e:
                logger.error(f"DynamoDB initialization failed: {e}")
        return self._dynamodb_table
    
    async def generate_token(
        self,
        workflow_id: str,
        user_id: str,
        expires_in_minutes: int = 30
    ) -> str:
        """í† í° ìƒì„± ë° ì €ì¥"""
        import secrets
        import time
        
        # ì•ˆì „í•œ ëœë¤ í† í° ìƒì„±
        token = secrets.token_urlsafe(32)
        expires_at = int(time.time()) + (expires_in_minutes * 60)
        
        token_data = {
            'workflow_id': workflow_id,
            'user_id': user_id,
            'created_at': int(time.time()),
            'expires_at': expires_at,
            'is_used': False
        }
        
        if self.use_redis and self.redis_client:
            # Redisì— ì €ì¥ (TTL ìë™ ë§Œë£Œ)
            await self.redis_client.hset(
                f"confirmation_token:{token}",
                mapping=token_data
            )
            await self.redis_client.expire(
                f"confirmation_token:{token}",
                expires_in_minutes * 60
            )
        else:
            # DynamoDBì— ì €ì¥
            if self.dynamodb_table:
                self.dynamodb_table.put_item(
                    Item={
                        'token': token,
                        'workflow_id': workflow_id,
                        'user_id': user_id,
                        'created_at': token_data['created_at'],
                        'expires_at': expires_at,
                        'is_used': False,
                        'ttl': expires_at  # DynamoDB TTL
                    }
                )
        
        return token
    
    async def validate_token(
        self,
        token: str,
        workflow_id: str,
        user_id: str
    ) -> bool:
        """í† í° ê²€ì¦"""
        import time
        
        try:
            if self.use_redis and self.redis_client:
                # Redisì—ì„œ ì¡°íšŒ
                token_data = await self.redis_client.hgetall(f"confirmation_token:{token}")
                if not token_data:
                    return False
                
                # ë°”ì´íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (Redis íŠ¹ì„±)
                token_data = {k.decode(): v.decode() for k, v in token_data.items()}
                
            else:
                # DynamoDBì—ì„œ ì¡°íšŒ
                if not self.dynamodb_table:
                    return False
                
                response = self.dynamodb_table.get_item(Key={'token': token})
                token_data = response.get('Item')
                if not token_data:
                    return False
            
            # í† í° ê²€ì¦
            current_time = int(time.time())
            
            return (
                token_data.get('workflow_id') == workflow_id and
                token_data.get('user_id') == user_id and
                int(token_data.get('expires_at', 0)) > current_time and
                not token_data.get('is_used', False)
            )
            
        except Exception as e:
            logger.error(f"Token validation error: {e}")
            return False
    
    async def invalidate_token(self, token: str) -> bool:
        """í† í° ë¬´íš¨í™” (ì‚¬ìš© í›„)"""
        try:
            if self.use_redis and self.redis_client:
                # Redisì—ì„œ ì‚­ì œ
                await self.redis_client.delete(f"confirmation_token:{token}")
            else:
                # DynamoDBì—ì„œ is_used í”Œë˜ê·¸ ì„¤ì •
                if self.dynamodb_table:
                    self.dynamodb_table.update_item(
                        Key={'token': token},
                        UpdateExpression='SET is_used = :used',
                        ExpressionAttributeValues={':used': True}
                    )
            
            return True
            
        except Exception as e:
            logger.error(f"Token invalidation error: {e}")
            return False
