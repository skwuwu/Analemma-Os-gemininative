# -*- coding: utf-8 -*-
"""
Task Service

Business logic service for Task Manager UI.
Converts technical execution logs into business-friendly Task information.

[v2.0] Improvements:
- Efficient task_id lookup based on GSI (removed FilterExpression)
- Critical event DynamoDB persistence (prevents State Drift)
- S3 Presigned URL support (large artifacts)
- Enhanced timestamp validation
"""

import os
import logging
import json
import asyncio
import time
import uuid
from functools import partial
from typing import Dict, Any, List, Optional, Callable
from datetime import datetime, timezone

import boto3
from botocore.exceptions import ClientError

from src.common.aws_clients import get_dynamodb_resource, get_s3_client
from src.common.constants import DynamoDBConfig
from src.models.task_context import (
    TaskContext,
    TaskStatus,
    ArtifactType,
    ArtifactPreview,
    AgentThought,
    convert_technical_status,
    get_friendly_error_message,
)

# Import business metrics calculation module
try:
    from backend.services.business_metrics_calculator import (
        calculate_all_business_metrics,
    )
except ImportError:
    try:
        from src.services.business_metrics_calculator import (
            calculate_all_business_metrics,
        )
    except ImportError:
        # Fallback: Return empty dictionary if module not found
        def calculate_all_business_metrics(*args, **kwargs):
            return {}

logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìƒìˆ˜ ì •ì˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# S3 Presigned URL ë§Œë£Œ ì‹œê°„ (1ì‹œê°„)
PRESIGNED_URL_EXPIRY_SECONDS = int(os.environ.get('PRESIGNED_URL_EXPIRY_SECONDS', '3600'))
# ì´ë²¤íŠ¸ ìŠ¤í† ì–´ìš© í…Œì´ë¸”
TASK_EVENTS_TABLE = os.environ.get('TASK_EVENTS_TABLE', 'TaskEventsTable')
# S3 ë²„í‚·
WORKFLOW_STATE_BUCKET = os.environ.get('WORKFLOW_STATE_BUCKET', '')


class TaskService:
    """
    Task Manager ì„œë¹„ìŠ¤
    
    - ê¸°ìˆ ì  ì‹¤í–‰ ë¡œê·¸ë¥¼ Task ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    - Task ëª©ë¡ ì¡°íšŒ (í•„í„°ë§ ì§€ì›)
    - Task ìƒì„¸ ì •ë³´ ì¡°íšŒ
    - ì‹¤ì‹œê°„ Task ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
    """
    
    def __init__(
        self,
        execution_table: Optional[str] = None,
        notification_table: Optional[str] = None,
        task_events_table: Optional[str] = None,
    ):
        """
        Args:
            execution_table: ì‹¤í–‰ ìƒíƒœ í…Œì´ë¸” ì´ë¦„
            notification_table: ì•Œë¦¼ í…Œì´ë¸” ì´ë¦„
            task_events_table: íƒœìŠ¤í¬ ì´ë²¤íŠ¸ ìŠ¤í† ì–´ í…Œì´ë¸” ì´ë¦„ (ì¤‘ìš” ì´ë²¤íŠ¸ ì˜ì†í™”)
        """
        self.execution_table_name = execution_table or os.environ.get(
            'EXECUTIONS_TABLE', DynamoDBConfig.EXECUTIONS_TABLE if hasattr(DynamoDBConfig, 'EXECUTIONS_TABLE') else 'ExecutionsTableV3'
        )
        self.notification_table_name = notification_table or os.environ.get(
            'PENDING_NOTIFICATIONS_TABLE', DynamoDBConfig.PENDING_NOTIFICATIONS_TABLE if hasattr(DynamoDBConfig, 'PENDING_NOTIFICATIONS_TABLE') else 'PendingNotificationsTableV3'
        )
        self.task_events_table_name = task_events_table or TASK_EVENTS_TABLE
        self._execution_table = None
        self._notification_table = None
        self._task_events_table = None
        self._s3_client = None
    
    @property
    def execution_table(self):
        """ì§€ì—° ì´ˆê¸°í™”ëœ ì‹¤í–‰ í…Œì´ë¸”"""
        if self._execution_table is None:
            dynamodb_resource = get_dynamodb_resource()
            self._execution_table = dynamodb_resource.Table(self.execution_table_name)
        return self._execution_table

    @property
    def notification_table(self):
        """ì§€ì—° ì´ˆê¸°í™”ëœ ì•Œë¦¼ í…Œì´ë¸”"""
        if self._notification_table is None:
            dynamodb_resource = get_dynamodb_resource()
            self._notification_table = dynamodb_resource.Table(self.notification_table_name)
        return self._notification_table

    @property
    def task_events_table(self):
        """ì§€ì—° ì´ˆê¸°í™”ëœ íƒœìŠ¤í¬ ì´ë²¤íŠ¸ ìŠ¤í† ì–´ í…Œì´ë¸”"""
        if self._task_events_table is None:
            dynamodb_resource = get_dynamodb_resource()
            self._task_events_table = dynamodb_resource.Table(self.task_events_table_name)
        return self._task_events_table

    @property
    def s3_client(self):
        """ì§€ì—° ì´ˆê¸°í™”ëœ S3 í´ë¼ì´ì–¸íŠ¸"""
        if self._s3_client is None:
            self._s3_client = get_s3_client()
        return self._s3_client

    async def get_tasks(
        self,
        owner_id: str,
        status_filter: Optional[str] = None,
        limit: int = 50,
        include_completed: bool = True,
    ) -> List[Dict[str, Any]]:
        """
        Task ëª©ë¡ ì¡°íšŒ
        
        [v2.1] Executions í…Œì´ë¸”ì—ì„œ ì§ì ‘ ì¡°íšŒë¡œ ë³€ê²½
        - ì™„ë£Œëœ ì›Œí¬í”Œë¡œìš°ë„ íˆìŠ¤í† ë¦¬ì— í‘œì‹œ
        - OwnerIdStartDateIndex GSI ì‚¬ìš© (ìµœì‹ ìˆœ ì •ë ¬)
        
        Args:
            owner_id: ì‚¬ìš©ì ID
            status_filter: ìƒíƒœ í•„í„° (pending_approval, in_progress, completed ë“±)
            limit: ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜
            include_completed: ì™„ë£Œëœ Task í¬í•¨ ì—¬ë¶€
            
        Returns:
            Task ìš”ì•½ ì •ë³´ ëª©ë¡
        """
        try:
            # [v2.1] Executions í…Œì´ë¸”ì—ì„œ ì§ì ‘ ì¡°íšŒ
            owner_index = os.environ.get('OWNER_INDEX', 'OwnerIdStartDateIndex')
            
            query_func = partial(
                self.execution_table.query,
                IndexName=owner_index,
                KeyConditionExpression="ownerId = :oid",
                ExpressionAttributeValues={
                    ":oid": owner_id
                },
                ScanIndexForward=False,  # ìµœì‹ ìˆœ
                Limit=limit * 2  # í•„í„°ë§ í›„ ì¶©ë¶„í•œ ê²°ê³¼ë¥¼ ìœ„í•´ 2ë°° ì¡°íšŒ
            )
            response = await asyncio.get_event_loop().run_in_executor(None, query_func)
            
            items = response.get('Items', [])
            tasks = []
            
            for item in items:
                task = self._convert_execution_to_task(item)
                if task:
                    # í•„í„° ì ìš©
                    if status_filter:
                        if task.get('status') != status_filter:
                            continue
                    if not include_completed and task.get('status') == 'completed':
                        continue
                    
                    tasks.append(task)
                    
                    # limit ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
                    if len(tasks) >= limit:
                        break
            
            return tasks
            
        except ClientError as e:
            logger.error(f"Failed to get tasks from executions table: {e}")
            # Fallback: notification í…Œì´ë¸”ì—ì„œ ì¡°íšŒ (ì§„í–‰ ì¤‘ì¸ ì‘ì—…ë§Œ)
            try:
                query_func = partial(
                    self.notification_table.query,
                    KeyConditionExpression="ownerId = :oid",
                    ExpressionAttributeValues={
                        ":oid": owner_id
                    },
                    ScanIndexForward=False,
                    Limit=limit
                )
                response = await asyncio.get_event_loop().run_in_executor(None, query_func)
                items = response.get('Items', [])
                tasks = []
                
                for item in items:
                    task = self._convert_notification_to_task(item)
                    if task:
                        if status_filter and task.get('status') != status_filter:
                            continue
                        if not include_completed and task.get('status') == 'completed':
                            continue
                        tasks.append(task)
                
                return tasks[:limit]
            except Exception as fallback_error:
                logger.error(f"Fallback query also failed: {fallback_error}")
                return []

    async def get_task_detail(
        self,
        task_id: str,
        owner_id: str,
        include_technical_logs: bool = False,
    ) -> Optional[Dict[str, Any]]:
        """
        Task ìƒì„¸ ì •ë³´ ì¡°íšŒ
        
        [v2.0] GSI ê¸°ë°˜ íš¨ìœ¨ì ì¸ ì¡°íšŒë¡œ ê°œì„ 
        - ê¸°ì¡´: FilterExpressionìœ¼ë¡œ ì „ì²´ ìŠ¤ìº” í›„ í•„í„°ë§ (RCU ë‚­ë¹„)
        - ê°œì„ : ExecutionIdIndex GSIë¡œ ì§ì ‘ ì¡°íšŒ (O(1) ì¡°íšŒ)
        
        Args:
            task_id: Task ID (execution_id)
            owner_id: ì‚¬ìš©ì ID
            include_technical_logs: ê¸°ìˆ  ë¡œê·¸ í¬í•¨ ì—¬ë¶€ (ê¶Œí•œì— ë”°ë¼)
            
        Returns:
            Task ìƒì„¸ ì •ë³´
        """
        try:
            # [v2.0] GSIë¥¼ ì‚¬ìš©í•œ íš¨ìœ¨ì ì¸ ì¡°íšŒ
            # ExecutionIdIndex: ownerId (PK), execution_id (SK)
            execution_id_index = os.environ.get(
                'EXECUTION_ID_INDEX', 
                DynamoDBConfig.EXECUTION_ID_INDEX if hasattr(DynamoDBConfig, 'EXECUTION_ID_INDEX') else 'ExecutionIdIndex'
            )
            
            query_func = partial(
                self.notification_table.query,
                IndexName=execution_id_index,
                KeyConditionExpression="ownerId = :oid AND execution_id = :eid",
                ExpressionAttributeValues={
                    ":oid": owner_id,
                    ":eid": task_id
                },
                ScanIndexForward=False,  # ìµœì‹ ìˆœ
                Limit=1
            )
            
            try:
                response = await asyncio.get_event_loop().run_in_executor(None, query_func)
            except ClientError as gsi_error:
                # GSIê°€ ì—†ëŠ” ê²½ìš° í´ë°± (ë‹¨, ê²½ê³  ë¡œê·¸ ì¶œë ¥)
                if 'ValidationException' in str(gsi_error) or 'ResourceNotFoundException' in str(gsi_error):
                    logger.warning(
                        f"ExecutionIdIndex GSI not found, falling back to scan. "
                        f"Consider adding GSI for better performance. Error: {gsi_error}"
                    )
                    return await self._get_task_detail_fallback(task_id, owner_id, include_technical_logs)
                raise
            
            items = response.get('Items', [])
            if not items:
                # [FIX] Notification í…Œì´ë¸”ì— ì—†ìœ¼ë©´ Executions í…Œì´ë¸”ì—ì„œ ì§ì ‘ ì¡°íšŒ
                logger.info(f"Task {task_id[:8]}... not found in notifications, checking executions table")
                execution_task = await self._get_task_from_executions_table(task_id, owner_id, include_technical_logs)
                if execution_task:
                    return execution_task
                
                # ë§ˆì§€ë§‰ìœ¼ë¡œ ì´ë²¤íŠ¸ ìŠ¤í† ì–´ì—ì„œ ì¡°íšŒ ì‹œë„
                return await self._get_task_from_event_store(task_id, owner_id, include_technical_logs)
            
            # ê°€ì¥ ìµœì‹  í•­ëª© ì‚¬ìš©
            latest = items[0]
            task = self._convert_notification_to_task(latest, detailed=True)
            
            return task
            
        except ClientError as e:
            logger.error(f"Failed to get task detail: {e}")
            return None

    async def get_task_outcomes(self, task_id: str, owner_id: str) -> Optional[Dict[str, Any]]:
        """
        Task ê²°ê³¼ë¬¼(Outcomes) ì¡°íšŒ
        
        ì™„ì„±ëœ ì•„í‹°íŒ©íŠ¸ ëª©ë¡ê³¼ ì¶•ì•½ëœ íˆìŠ¤í† ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        task_detail = await self.get_task_detail(task_id, owner_id)
        if not task_detail:
            return None
            
        artifacts = task_detail.get('artifacts', [])
        
        # ì¶•ì•½ íˆìŠ¤í† ë¦¬ ìƒì„±
        thought_history = task_detail.get('thought_history', [])
        summary = f"{len(thought_history)}ê°œì˜ ì‚¬ê³  ê³¼ì •ì„ ê±°ì³ {task_detail.get('status')} ë˜ì—ˆìŠµë‹ˆë‹¤."
        
        collapsed_history = {
            'summary': summary,
            'node_count': len(set(t.get('node_id') for t in thought_history if t.get('node_id'))),
            'llm_call_count': len([t for t in thought_history if t.get('thought_type') == 'thinking']),
            'total_duration_seconds': None,
            'key_decisions': [t.get('message') for t in thought_history if t.get('thought_type') == 'decision'][:3],
            'full_trace_available': True
        }
        
        return {
            'task_id': task_id,
            'task_title': task_detail.get('task_summary'),
            'status': task_detail.get('status'),
            'outcomes': artifacts,
            'collapsed_history': collapsed_history,
            'correction_applied': False,
            'last_updated': task_detail.get('updated_at')
        }

    async def get_task_metrics(self, task_id: str, owner_id: str) -> Optional[Dict[str, Any]]:
        """
        Task ë©”íŠ¸ë¦­ ì¡°íšŒ (Bento Gridìš©)
        """
        task_detail = await self.get_task_detail(task_id, owner_id)
        if not task_detail:
            return None
            
        progress = task_detail.get('progress_percentage', 0)
        status = task_detail.get('status', 'unknown')
        
        # Confidence Score
        confidence_score = task_detail.get('confidence_score', 0)
        if confidence_score == 0 and status == 'completed':
            confidence_score = 95
            
        # Autonomy Rate
        autonomy_rate = task_detail.get('autonomy_rate', 100)
        
        # Intervention History
        intervention_history = task_detail.get('intervention_history', {
            'count': 0, 
            'summary': 'ì •ìƒ ì‹¤í–‰ë¨',
            'positive_count': 0,
            'negative_count': 0,
            'history': []
        })
        
        return {
            'display': {
                'title': task_detail.get('task_summary'),
                'status_color': self._get_status_color(status),
                'eta_text': 'Completed' if status == 'completed' else 'Calculating...',
                'status': status,
                'status_label': task_detail.get('current_step_name', status)
            },
            'grid_items': {
                'progress': {
                    'value': progress,
                    'label': 'Overall Progress',
                    'sub_text': f"{progress}% complete"
                },
                'confidence': {
                    'value': confidence_score,
                    'level': 'High' if confidence_score > 80 else 'Medium' if confidence_score > 50 else 'Low',
                    'breakdown': {
                        'reflection': 90,
                        'schema': 85,
                        'alignment': 95
                    }
                },
                'autonomy': {
                    'value': autonomy_rate,
                    'display': f"{autonomy_rate}% Autonomous"
                },
                'intervention': intervention_history
            },
            'last_updated': task_detail.get('updated_at')
        }

    def _get_status_color(self, status: str) -> str:
        colors = {
            'completed': 'green',
            'in_progress': 'blue',
            'failed': 'red',
            'pending_approval': 'yellow',
            'queued': 'gray'
        }
        return colors.get(status, 'gray')

    async def _get_task_detail_fallback(
        self,
        task_id: str,
        owner_id: str,
        include_technical_logs: bool = False,
    ) -> Optional[Dict[str, Any]]:
        """
        GSIê°€ ì—†ëŠ” ê²½ìš°ì˜ í´ë°± ì¡°íšŒ (ë ˆê±°ì‹œ í˜¸í™˜)
        
        âš ï¸ ê²½ê³ : ì´ ë°©ì‹ì€ RCUë¥¼ ë§ì´ ì†Œëª¨í•©ë‹ˆë‹¤.
        í”„ë¡œë•ì…˜ì—ì„œëŠ” ë°˜ë“œì‹œ ExecutionIdIndex GSIë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
        """
        logger.warning(
            f"Using fallback query for task_id={task_id}. "
            "This is inefficient - add ExecutionIdIndex GSI!"
        )
        
        query_func = partial(
            self.notification_table.query,
            KeyConditionExpression="ownerId = :oid",
            FilterExpression="contains(notification, :tid)",
            ExpressionAttributeValues={
                ":oid": owner_id,
                ":tid": task_id
            },
            ScanIndexForward=False,
            Limit=50  # ë” ë§ì´ ì½ì–´ì„œ í•„í„°ë§
        )
        response = await asyncio.get_event_loop().run_in_executor(None, query_func)
        
        items = response.get('Items', [])
        if not items:
            return None
        
        latest = items[0]
        task = self._convert_notification_to_task(latest, detailed=True)
        
        if include_technical_logs and task:
            task['technical_logs'] = self._extract_technical_logs(latest)
        
        return task

    async def _get_task_from_event_store(
        self,
        task_id: str,
        owner_id: str,
        include_technical_logs: bool = False,
    ) -> Optional[Dict[str, Any]]:
        """
        ì´ë²¤íŠ¸ ìŠ¤í† ì–´ì—ì„œ íƒœìŠ¤í¬ ì •ë³´ ì¡°íšŒ
        
        notification í…Œì´ë¸”ì— ì—†ëŠ” ê²½ìš° ì´ë²¤íŠ¸ ìŠ¤í† ì–´ì—ì„œ ì¬êµ¬ì„±
        """
        try:
            query_func = partial(
                self.task_events_table.query,
                KeyConditionExpression="task_id = :tid",
                ExpressionAttributeValues={
                    ":tid": task_id
                },
                ScanIndexForward=True,  # ì‹œê°„ìˆœ
            )
            response = await asyncio.get_event_loop().run_in_executor(None, query_func)
            
            events = response.get('Items', [])
            if not events:
                return None
            
            # ì´ë²¤íŠ¸ë“¤ë¡œë¶€í„° íƒœìŠ¤í¬ ìƒíƒœ ì¬êµ¬ì„±
            return self._reconstruct_task_from_events(events, include_technical_logs)
            
        except ClientError as e:
            # í…Œì´ë¸”ì´ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ
            if 'ResourceNotFoundException' in str(e):
                return None
            logger.error(f"Failed to get task from event store: {e}")
            return None

    async def _get_task_from_executions_table(
        self,
        task_id: str,
        owner_id: str,
        include_technical_logs: bool = False,
    ) -> Optional[Dict[str, Any]]:
        """
        Executions í…Œì´ë¸”ì—ì„œ ì§ì ‘ íƒœìŠ¤í¬ ì •ë³´ ì¡°íšŒ
        
        ì§„í–‰ ì¤‘ì¸ ì‘ì—…ì´ notification í…Œì´ë¸”ì— ì—†ëŠ” ê²½ìš° ì‚¬ìš©
        ë©€í‹° í…Œë„ŒíŠ¸ ê²©ë¦¬ë¥¼ ìœ„í•´ ìì‹ ì˜ owner_idë¡œë§Œ ì¡°íšŒ
        """
        try:
            logger.info(f"[ExecutionsTable] Querying for task_id={task_id[:8]}..., owner_id={owner_id[:8]}...")
            
            # Executions í…Œì´ë¸”ì—ì„œ ì¡°íšŒ (PK: ownerId, SK: executionArn)
            # executionArn êµ¬ì¡°: arn:aws:states:region:account:execution:stateMachineName:executionName
            # executionNameì´ task_idì™€ ì¼ì¹˜í•´ì•¼ í•¨
            
            # ğŸ”’ ë³´ì•ˆ: ë©€í‹° í…Œë„ŒíŠ¸ ê²©ë¦¬ë¥¼ ìœ„í•´ ìì‹ ì˜ owner_idë¡œë§Œ ì¡°íšŒ
            # ì‹œìŠ¤í…œ ì‹¤í–‰ì€ ë³„ë„ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ (ê²©ë¦¬ ìœ„ë°˜ ë°©ì§€)
            get_func = partial(
                self.execution_table.query,
                KeyConditionExpression="ownerId = :oid",
                FilterExpression="executionArn LIKE :arn_pattern",
                ExpressionAttributeValues={
                    ":oid": owner_id,
                    ":arn_pattern": f"%:{task_id}"  # executionArn ëì´ :task_idë¡œ ëë‚˜ëŠ” íŒ¨í„´
                },
                Limit=10  # ì—¬ëŸ¬ ê°œ ê°€ì ¸ì™€ì„œ í™•ì¸
            )
            
            response = await asyncio.get_event_loop().run_in_executor(None, get_func)
            items = response.get('Items', [])
            
            logger.info(f"[ExecutionsTable] Found {len(items)} items for owner_id={owner_id[:8]}...")
            
            if not items:
                logger.warning(f"Task {task_id[:8]}... not found in executions table for owner {owner_id[:8]}...")
                return None
            
            # ì •í™•í•œ ë§¤ì¹­ í™•ì¸ (executionArnì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì´ task_idì™€ ì¼ì¹˜í•˜ëŠ”ì§€)
            matching_items = []
            for item in items:
                execution_arn = item.get('executionArn', '')
                if execution_arn.endswith(f':{task_id}'):
                    execution_name = execution_arn.split(':')[-1]
                    if execution_name == task_id:
                        matching_items.append(item)
            
            if not matching_items:
                logger.warning(f"No exact executionArn match found for task_id={task_id[:8]}...")
                return None
            
            # ê°€ì¥ ìµœì‹  í•­ëª© ì‚¬ìš© (startDate ê¸°ì¤€)
            execution = max(matching_items, key=lambda x: x.get('startDate', ''))
            logger.info(f"Found task {task_id[:8]}... in executions table, status={execution.get('status')}")
            
            # Executionì„ Task í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            task = self._convert_execution_to_task(execution, detailed=True)
            
            if include_technical_logs and task:
                task['technical_logs'] = self._extract_technical_logs_from_execution(execution)
            
            return task
            
        except ClientError as e:
            logger.error(f"Failed to get task from executions table: {e}")
            return None

    def _convert_execution_to_task(
        self,
        execution: Dict[str, Any],
        detailed: bool = False
    ) -> Optional[Dict[str, Any]]:
        """
        Execution ë°ì´í„°ë¥¼ Task í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        [v2.1] Executions í…Œì´ë¸” í•­ëª©ì„ Task Manager UI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            execution: DynamoDB execution í•­ëª©
            detailed: ìƒì„¸ ì •ë³´ í¬í•¨ ì—¬ë¶€
            
        Returns:
            Task ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            execution_arn = execution.get('executionArn', '')
            execution_id = execution_arn.split(':')[-1] if execution_arn else execution.get('execution_id', '')
            
            technical_status = execution.get('status', 'UNKNOWN')
            task_status = convert_technical_status(technical_status)
            
            # step_function_stateì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
            sfn_state = execution.get('step_function_state', {})
            
            # ì§„í–‰ë¥  ê³„ì‚°
            progress = 100 if task_status == TaskStatus.COMPLETED else 0
            if task_status == TaskStatus.IN_PROGRESS:
                # initial_inputì—ì„œ ì´ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ ì¶”ì¶œ ì‹œë„
                initial_input = execution.get('initial_input', {})
                if isinstance(initial_input, str):
                    try:
                        initial_input = json.loads(initial_input)
                    except:
                        initial_input = {}
                
                current_segment = sfn_state.get('current_segment', 0)
                total_segments = initial_input.get('total_segments', 1)
                progress = int((current_segment / max(total_segments, 1)) * 100)
            
            # ê¸°ë³¸ Task ì •ë³´
            task = {
                "task_id": execution_id,
                "task_summary": execution.get('name', '') or self._generate_task_summary_from_execution(execution),
                "agent_name": "AI Assistant",
                "status": task_status.value,
                "progress_percentage": progress,
                "current_step_name": sfn_state.get('current_step', ''),
                "current_thought": self._generate_thought_from_status(task_status, technical_status),
                "is_interruption": technical_status in ['PAUSED', 'PAUSED_FOR_HITP'],
                "started_at": self._format_timestamp(execution.get('startDate') or execution.get('created_at')),
                "updated_at": self._format_timestamp(execution.get('stopDate') or execution.get('updated_at')),
                "workflow_name": execution.get('workflow_name', ''),
                "workflow_id": execution.get('workflowId', ''),
            }
            
            # ì—ëŸ¬ ì •ë³´
            if task_status == TaskStatus.FAILED:
                error = execution.get('error', '')
                if error:
                    message, suggestion = get_friendly_error_message(str(error))
                    task['error_message'] = message
                    task['error_suggestion'] = suggestion
            
            # ìƒì„¸ ì •ë³´
            if detailed:
                task['artifacts'] = []
                task['thought_history'] = []
                
                # final_resultì—ì„œ ì•„í‹°íŒ©íŠ¸ ì¶”ì¶œ
                final_result = execution.get('final_result', {})
                if isinstance(final_result, str):
                    try:
                        final_result = json.loads(final_result)
                    except:
                        pass
                
                if isinstance(final_result, dict):
                    task['artifacts'] = self._extract_artifacts(final_result)
            
            return task
            
        except Exception as e:
            logger.error(f"Failed to convert execution to task: {e}")
            return None

    def _generate_task_summary_from_execution(self, execution: Dict[str, Any]) -> str:
        """Execution ë°ì´í„°ë¡œë¶€í„° Task ìš”ì•½ ìƒì„±"""
        workflow_id = execution.get('workflowId', '')
        if workflow_id:
            return f"ì›Œí¬í”Œë¡œìš° {workflow_id[:8]}... ì‹¤í–‰"
        
        execution_arn = execution.get('executionArn', '')
        if execution_arn:
            exec_id = execution_arn.split(':')[-1]
            return f"ì‹¤í–‰ {exec_id[:8]}..."
        
        return "AI ì‘ì—…"

    def _generate_thought_from_status(self, task_status: TaskStatus, technical_status: str) -> str:
        """ìƒíƒœì— ë”°ë¥¸ ì—ì´ì „íŠ¸ ì‚¬ê³  ë©”ì‹œì§€ ìƒì„±"""
        if technical_status in ['PAUSED', 'PAUSED_FOR_HITP']:
            return "ì‚¬ìš©ìì˜ ìŠ¹ì¸ì„ ê¸°ë‹¤ë¦¬ê³  ìˆìŠµë‹ˆë‹¤."
        
        if task_status == TaskStatus.IN_PROGRESS:
            return "ì‘ì—…ì„ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
        
        if task_status == TaskStatus.COMPLETED:
            return "ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
        
        if task_status == TaskStatus.FAILED:
            return "ì‘ì—… ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        if task_status == TaskStatus.QUEUED:
            return "ì‘ì—…ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
        
        return "ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤."

    def _convert_notification_to_task(
        self,
        notification: Dict[str, Any],
        detailed: bool = False
    ) -> Optional[Dict[str, Any]]:
        """
        Notification ë°ì´í„°ë¥¼ Task í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            notification: DynamoDB notification í•­ëª©
            detailed: ìƒì„¸ ì •ë³´ í¬í•¨ ì—¬ë¶€
            
        Returns:
            Task ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            payload = notification.get('notification', {}).get('payload', {})
            if not payload:
                return None
            
            execution_id = payload.get('execution_id', '')
            technical_status = payload.get('status', 'UNKNOWN')
            task_status = convert_technical_status(technical_status)
            
            # ì§„í–‰ë¥  ê³„ì‚°
            current_segment = payload.get('current_segment', 0)
            total_segments = payload.get('total_segments', 1)
            progress = int((current_segment / max(total_segments, 1)) * 100)
            
            # ê¸°ë³¸ Task ì •ë³´
            task = {
                "task_id": execution_id,
                "task_summary": self._generate_task_summary(payload),
                "agent_name": "AI Assistant",
                "status": task_status.value,
                "progress_percentage": progress,
                "current_step_name": payload.get('current_step_label', ''),
                "current_thought": self._generate_current_thought(payload),
                "is_interruption": technical_status == 'PAUSED_FOR_HITP',
                "started_at": self._format_timestamp(payload.get('start_time')),
                "updated_at": self._format_timestamp(notification.get('timestamp')),
                "workflow_name": payload.get('workflow_name', ''),
                "workflow_id": payload.get('workflowId', ''),
            }
            
            # ì—ëŸ¬ ì •ë³´ (ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ ë³€í™˜)
            if task_status == TaskStatus.FAILED:
                error = payload.get('error', '')
                if error:
                    message, suggestion = get_friendly_error_message(str(error))
                    task['error_message'] = message
                    task['error_suggestion'] = suggestion
            
            # ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ ì§€í‘œ ì¶”ê°€ (ìƒˆë¡œìš´ ê³„ì‚° ë¡œì§)
            business_metrics = calculate_all_business_metrics(
                payload=payload,
                execution_id=execution_id,
                progress=progress,
                status=technical_status
            )
            task.update(business_metrics)
            
            # ìƒì„¸ ì •ë³´
            if detailed:
                task['pending_decision'] = None
                if technical_status == 'PAUSED_FOR_HITP':
                    task['pending_decision'] = {
                        "question": "ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
                        "context": payload.get('message', ''),
                        "pre_hitp_output": payload.get('pre_hitp_output', {}),
                    }
                
                task['artifacts'] = self._extract_artifacts(payload)
                task['thought_history'] = self._extract_thought_history(payload)
                
                # ë¹„ìš© ì •ë³´
                step_function_state = payload.get('step_function_state', {})
                if step_function_state:
                    task['token_usage'] = step_function_state.get('token_usage', {})
            
            return task
            
        except Exception as e:
            logger.error(f"Failed to convert notification to task: {e}")
            return None

    def _generate_task_summary(self, payload: Dict[str, Any]) -> str:
        """ì›Œí¬í”Œë¡œìš° ì •ë³´ë¡œë¶€í„° Task ìš”ì•½ ìƒì„±"""
        workflow_name = payload.get('workflow_name', '')
        if workflow_name:
            return f"{workflow_name} ì‹¤í–‰"
        
        workflow_id = payload.get('workflowId', '')
        if workflow_id:
            return f"ì›Œí¬í”Œë¡œìš° {workflow_id[:8]}... ì‹¤í–‰"
        
        return "AI ì‘ì—… ì§„í–‰ ì¤‘"

    def _generate_current_thought(self, payload: Dict[str, Any]) -> str:
        """í˜„ì¬ ìƒíƒœì— ë”°ë¥¸ ì—ì´ì „íŠ¸ ì‚¬ê³  ë©”ì‹œì§€ ìƒì„±"""
        status = payload.get('status', '')
        current_step = payload.get('current_step_label', '')
        
        if status == 'PAUSED_FOR_HITP':
            return "ì‚¬ìš©ìì˜ ìŠ¹ì¸ì„ ê¸°ë‹¤ë¦¬ê³  ìˆìŠµë‹ˆë‹¤."
        
        if status == 'RUNNING':
            if current_step:
                return f"{current_step} ë‹¨ê³„ë¥¼ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
            return "ì‘ì—…ì„ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
        
        if status == 'COMPLETE' or status == 'COMPLETED':
            return "ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
        
        if status == 'FAILED':
            return "ì‘ì—… ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        return "ì‘ì—…ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."

    def _format_timestamp(self, ts: Any) -> Optional[str]:
        """
        íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        [v2.0] ê²€ì¦ ê°•í™”: ì˜ëª»ëœ ì…ë ¥ì— ëŒ€í•´ None ë°˜í™˜ (í”„ë¡ íŠ¸ì—”ë“œ íŒŒì‹± ì—ëŸ¬ ë°©ì§€)
        """
        if ts is None:
            return None
        
        try:
            # ìˆ«ìí˜• íƒ€ì„ìŠ¤íƒ¬í”„ ì²˜ë¦¬
            if isinstance(ts, (int, float)):
                # ìŒìˆ˜ë‚˜ ë¹„ì •ìƒì ìœ¼ë¡œ ì‘ì€ ê°’ ê±°ë¶€
                if ts <= 0:
                    return None
                
                # ë°€ë¦¬ì´ˆ/ì´ˆ íŒë‹¨ (13ìë¦¬ ì´ìƒì´ë©´ ë°€ë¦¬ì´ˆ)
                if ts > 1e12:
                    ts_seconds = ts / 1000
                else:
                    ts_seconds = ts
                
                # í•©ë¦¬ì ì¸ ë²”ìœ„ ê²€ì¦ (1970ë…„ ~ 2100ë…„)
                if ts_seconds < 0 or ts_seconds > 4102444800:  # 2100-01-01
                    return None
                
                return datetime.fromtimestamp(ts_seconds, tz=timezone.utc).isoformat()
            
            # ë¬¸ìì—´ íƒ€ì„ìŠ¤íƒ¬í”„ ì²˜ë¦¬
            if isinstance(ts, str):
                # ì´ë¯¸ ISO í˜•ì‹ì¸ ê²½ìš° ê²€ì¦ í›„ ë°˜í™˜
                try:
                    # ISO í˜•ì‹ íŒŒì‹± ì‹œë„
                    parsed = datetime.fromisoformat(ts.replace('Z', '+00:00'))
                    return parsed.isoformat()
                except ValueError:
                    pass
                
                # ìˆ«ì ë¬¸ìì—´ì¸ ê²½ìš° ì¬ê·€ ì²˜ë¦¬
                try:
                    numeric_ts = float(ts)
                    return self._format_timestamp(numeric_ts)
                except ValueError:
                    pass
                
                # íŒŒì‹± ë¶ˆê°€ëŠ¥í•œ ë¬¸ìì—´ì€ None ë°˜í™˜
                logger.warning(f"Unable to parse timestamp: {ts}")
                return None
            
            # ê¸°íƒ€ íƒ€ì…ì€ None ë°˜í™˜
            return None
            
        except (ValueError, OSError, OverflowError) as e:
            logger.warning(f"Timestamp parsing error for {ts}: {e}")
            return None

    def _extract_artifacts(self, payload: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ì‹¤í–‰ ê²°ê³¼ë¬¼ ì¶”ì¶œ
        
        [v2.0] S3 Presigned URL ì§€ì›
        - ëŒ€ìš©ëŸ‰ JSON/ì´ë¯¸ì§€ íŒŒì¼ì€ S3ì—ì„œ Presigned URL ìƒì„±
        - ì‘ì€ ë°ì´í„°ëŠ” ì§ì ‘ í¬í•¨
        """
        artifacts = []
        
        # step_function_stateì—ì„œ output ì¶”ì¶œ
        step_state = payload.get('step_function_state', {})
        final_output = step_state.get('final_state', {}) or step_state.get('output', {})
        
        if isinstance(final_output, dict):
            for key, value in final_output.items():
                if key.startswith('_'):  # ë‚´ë¶€ í•„ë“œ ì œì™¸
                    continue
                
                artifact = self._create_artifact(key, value)
                if artifact:
                    artifacts.append(artifact)
        
        return artifacts

    def _create_artifact(self, key: str, value: Any) -> Optional[Dict[str, Any]]:
        """
        ë‹¨ì¼ ì•„í‹°íŒ©íŠ¸ ìƒì„±
        
        S3 ì°¸ì¡°ì¸ ê²½ìš° Presigned URL ìƒì„±
        """
        if value is None:
            return None
        
        artifact = {
            "artifact_id": f"output_{key}",
            "title": key,
        }
        
        # S3 ì°¸ì¡° ê°ì§€ (s3:// URL ë˜ëŠ” S3 ë©”íƒ€ë°ì´í„° ê°ì²´)
        if isinstance(value, str) and value.startswith('s3://'):
            artifact.update(self._handle_s3_artifact(value))
        elif isinstance(value, dict) and 's3_bucket' in value and 's3_key' in value:
            s3_uri = f"s3://{value['s3_bucket']}/{value['s3_key']}"
            artifact.update(self._handle_s3_artifact(s3_uri, value))
        else:
            # ì¼ë°˜ ë°ì´í„° ì²˜ë¦¬
            artifact.update(self._handle_inline_artifact(key, value))
        
        return artifact

    def _handle_s3_artifact(
        self, 
        s3_uri: str, 
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        S3 ì•„í‹°íŒ©íŠ¸ ì²˜ë¦¬: Presigned URL ìƒì„±
        """
        try:
            # s3://bucket/key íŒŒì‹±
            if s3_uri.startswith('s3://'):
                parts = s3_uri[5:].split('/', 1)
                bucket = parts[0]
                key = parts[1] if len(parts) > 1 else ''
            else:
                return {"artifact_type": "error", "preview_content": "Invalid S3 URI"}
            
            # íŒŒì¼ í™•ì¥ìë¡œ íƒ€ì… ì¶”ë¡ 
            extension = key.rsplit('.', 1)[-1].lower() if '.' in key else ''
            artifact_type = self._infer_artifact_type(extension)
            
            # Presigned URL ìƒì„±
            try:
                presigned_url = self.s3_client.generate_presigned_url(
                    'get_object',
                    Params={'Bucket': bucket, 'Key': key},
                    ExpiresIn=PRESIGNED_URL_EXPIRY_SECONDS
                )
            except ClientError as e:
                logger.error(f"Failed to generate presigned URL for {s3_uri}: {e}")
                presigned_url = None
            
            result = {
                "artifact_type": artifact_type,
                "s3_uri": s3_uri,
                "download_url": presigned_url,
                "preview_content": f"[S3 íŒŒì¼] {key.rsplit('/', 1)[-1]}",
            }
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            if metadata:
                result["file_size"] = metadata.get('size', metadata.get('file_size'))
                result["content_type"] = metadata.get('content_type')
            
            return result
            
        except Exception as e:
            logger.error(f"S3 artifact handling error: {e}")
            return {"artifact_type": "error", "preview_content": str(e)}

    def _handle_inline_artifact(self, key: str, value: Any) -> Dict[str, Any]:
        """
        ì¸ë¼ì¸ ì•„í‹°íŒ©íŠ¸ ì²˜ë¦¬ (ì‘ì€ ë°ì´í„°)
        """
        # ë°ì´í„° í¬ê¸° í™•ì¸
        str_value = str(value) if not isinstance(value, str) else value
        
        if len(str_value) > 10000:  # 10KB ì´ìƒì€ ì˜ë¼ì„œ í”„ë¦¬ë·°ë§Œ
            return {
                "artifact_type": "data",
                "preview_content": str_value[:500] + "... [truncated]",
                "is_truncated": True,
                "full_size": len(str_value),
            }
        
        # íƒ€ì… ì¶”ë¡ 
        artifact_type = "data"
        if isinstance(value, dict):
            artifact_type = "json"
        elif isinstance(value, list):
            artifact_type = "array"
        elif key.endswith('_html') or '<html' in str_value.lower()[:100]:
            artifact_type = "html"
        elif key.endswith('_markdown') or key.endswith('_md'):
            artifact_type = "markdown"
        
        return {
            "artifact_type": artifact_type,
            "preview_content": str_value[:500] if len(str_value) > 500 else str_value,
            "full_content": value if len(str_value) <= 10000 else None,
        }

    def _infer_artifact_type(self, extension: str) -> str:
        """íŒŒì¼ í™•ì¥ìë¡œ ì•„í‹°íŒ©íŠ¸ íƒ€ì… ì¶”ë¡ """
        type_map = {
            # ì´ë¯¸ì§€
            'jpg': 'image', 'jpeg': 'image', 'png': 'image', 
            'gif': 'image', 'webp': 'image', 'svg': 'image',
            # ë¬¸ì„œ
            'pdf': 'document', 'doc': 'document', 'docx': 'document',
            'xls': 'spreadsheet', 'xlsx': 'spreadsheet', 'csv': 'spreadsheet',
            # ì½”ë“œ/ë°ì´í„°
            'json': 'json', 'xml': 'data', 'yaml': 'data', 'yml': 'data',
            'html': 'html', 'md': 'markdown',
            # ë¯¸ë””ì–´
            'mp3': 'audio', 'wav': 'audio', 'mp4': 'video', 'webm': 'video',
            # ì••ì¶•
            'zip': 'archive', 'tar': 'archive', 'gz': 'archive',
        }
        return type_map.get(extension, 'file')

    def _reconstruct_task_from_events(
        self, 
        events: List[Dict[str, Any]],
        include_technical_logs: bool = False
    ) -> Optional[Dict[str, Any]]:
        """
        ì´ë²¤íŠ¸ë“¤ë¡œë¶€í„° íƒœìŠ¤í¬ ìƒíƒœ ì¬êµ¬ì„±
        
        WebSocket ë©”ì‹œì§€ë¥¼ ë†“ì¹œ ê²½ìš° ì´ë²¤íŠ¸ ìŠ¤í† ì–´ì—ì„œ ë³µì›
        """
        if not events:
            return None
        
        # ìµœì‹  ì´ë²¤íŠ¸ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        latest = events[-1]
        first = events[0]
        
        task = {
            "task_id": latest.get('task_id'),
            "task_summary": latest.get('task_summary', ''),
            "agent_name": "AI Assistant",
            "status": latest.get('status', 'unknown'),
            "progress_percentage": latest.get('progress', 0),
            "current_step_name": latest.get('current_step', ''),
            "current_thought": latest.get('thought', ''),
            "started_at": self._format_timestamp(first.get('timestamp')),
            "updated_at": self._format_timestamp(latest.get('timestamp')),
            "workflow_name": latest.get('workflow_name', ''),
            "workflow_id": latest.get('workflow_id', ''),
        }
        
        # ì‚¬ê³  íˆìŠ¤í† ë¦¬ ì¬êµ¬ì„±
        task['thought_history'] = [
            {
                "thought_id": e.get('event_id', str(i)),
                "timestamp": self._format_timestamp(e.get('timestamp')),
                "thought_type": e.get('thought_type', 'progress'),
                "message": e.get('thought', ''),
            }
            for i, e in enumerate(events)
            if e.get('thought')
        ]
        
        return task

    def _extract_thought_history(self, payload: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì‚¬ê³  ê³¼ì • íˆìŠ¤í† ë¦¬ ì¶”ì¶œ"""
        from src.models.task_context import THOUGHT_HISTORY_MAX_LENGTH
        
        thoughts = []
        
        # state_historyì—ì„œ ì¶”ì¶œ
        state_history = payload.get('state_history', [])
        if not state_history:
            step_state = payload.get('step_function_state', {})
            state_history = step_state.get('state_history', [])
        
        for entry in state_history[-THOUGHT_HISTORY_MAX_LENGTH:]:  # ìƒìˆ˜ ì‚¬ìš©
            thought = {
                "thought_id": entry.get('id', str(hash(str(entry)))),
                "timestamp": entry.get('entered_at', ''),
                "thought_type": "progress",
                "message": self._state_to_thought_message(entry),
                "node_id": entry.get('node_id', entry.get('state_name', '')),
            }
            thoughts.append(thought)
        
        return thoughts

    def _state_to_thought_message(self, state_entry: Dict[str, Any]) -> str:
        """ìƒíƒœ ì—”íŠ¸ë¦¬ë¥¼ ì‚¬ê³  ë©”ì‹œì§€ë¡œ ë³€í™˜"""
        state_name = state_entry.get('state_name', state_entry.get('name', ''))
        status = state_entry.get('status', '')
        
        if status == 'COMPLETED':
            return f"'{state_name}' ë‹¨ê³„ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤."
        elif status == 'RUNNING':
            return f"'{state_name}' ë‹¨ê³„ë¥¼ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
        elif status == 'FAILED':
            return f"'{state_name}' ë‹¨ê³„ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        return f"'{state_name}' ë‹¨ê³„ ì§„í–‰ ì¤‘"

    def _extract_technical_logs(self, notification: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ê¸°ìˆ  ë¡œê·¸ ì¶”ì¶œ (ê°œë°œì ëª¨ë“œìš©)"""
        logs = []
        
        payload = notification.get('notification', {}).get('payload', {})
        step_state = payload.get('step_function_state', {})
        state_history = step_state.get('state_history', [])
        
        for entry in state_history:
            log = {
                "timestamp": entry.get('entered_at'),
                "node_id": entry.get('state_name', entry.get('node_id', '')),
                "input": entry.get('input'),
                "output": entry.get('output'),
                "duration": entry.get('duration', 0),
                "error": entry.get('error'),
            }
            logs.append(log)
        
        return logs

    def update_task_context(
        self,
        execution_id: str,
        thought: str,
        progress: Optional[int] = None,
        current_step: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Task ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ë…¸ë“œ ì‹¤í–‰ ì¤‘ í˜¸ì¶œ)
        
        ì‹¤ì‹œê°„ìœ¼ë¡œ Taskì˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        WebSocketê³¼ DynamoDBì— ë™ì‹œì— ë°˜ì˜ë©ë‹ˆë‹¤.
        
        Args:
            execution_id: ì‹¤í–‰ ID
            thought: ì—ì´ì „íŠ¸ ì‚¬ê³  ë©”ì‹œì§€
            progress: ì§„í–‰ë¥  (0-100)
            current_step: í˜„ì¬ ë‹¨ê³„ ì´ë¦„
            
        Returns:
            WebSocket í˜ì´ë¡œë“œ
        """
        ws_payload = {
            "type": "task_update",
            "task_id": execution_id,
            "thought": thought,
            "updated_at": datetime.now(timezone.utc).isoformat(),
        }
        
        if progress is not None:
            ws_payload["progress"] = progress
        if current_step:
            ws_payload["current_step"] = current_step
        
        return ws_payload


class ContextAwareLogger:
    """
    ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë¡œê±°
    
    ê¸°ì¡´ logging.Loggerë¥¼ í™•ì¥í•˜ì—¬ Task ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ë¥¼ í•¨ê»˜ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    [v2.0] ê°œì„ ì‚¬í•­:
    - ì¤‘ìš” ì´ë²¤íŠ¸(is_important=True)ëŠ” DynamoDBì—ë„ ì˜ì†í™”
    - WebSocket ì—°ê²° ëŠê¹€ ì‹œì—ë„ ì´ë²¤íŠ¸ ìŠ¤í† ì–´ì—ì„œ ë³µêµ¬ ê°€ëŠ¥
    - State Drift ë°©ì§€
    """
    
    # ì¤‘ìš” ì´ë²¤íŠ¸ íƒ€ì… (DynamoDBì— ì˜ì†í™”)
    IMPORTANT_THOUGHT_TYPES = {'decision', 'success', 'error', 'warning'}
    
    def __init__(
        self,
        execution_id: str,
        owner_id: Optional[str] = None,
        task_service: Optional[TaskService] = None,
        websocket_callback: Optional[Callable] = None,
        persist_important_events: bool = True,
    ):
        """
        Args:
            execution_id: ì‹¤í–‰ ID
            owner_id: ì†Œìœ ì ID (ì´ë²¤íŠ¸ ì˜ì†í™”ì— í•„ìš”)
            task_service: TaskService ì¸ìŠ¤í„´ìŠ¤
            websocket_callback: WebSocket ì „ì†¡ ì½œë°± í•¨ìˆ˜
            persist_important_events: ì¤‘ìš” ì´ë²¤íŠ¸ë¥¼ DynamoDBì— ì €ì¥í• ì§€ ì—¬ë¶€
        """
        self.execution_id = execution_id
        self.owner_id = owner_id
        self.task_service = task_service or TaskService()
        self.websocket_callback = websocket_callback
        self.persist_important_events = persist_important_events
        self._logger = logging.getLogger(f"task.{execution_id[:8]}")
        self._progress = 0
        self._current_step = ""
        self._workflow_name = ""
        self._workflow_id = ""

    def set_workflow_context(self, workflow_name: str, workflow_id: str) -> None:
        """ì›Œí¬í”Œë¡œìš° ì»¨í…ìŠ¤íŠ¸ ì„¤ì •"""
        self._workflow_name = workflow_name
        self._workflow_id = workflow_id

    def report_thought(
        self,
        message: str,
        thought_type: str = "progress",
        progress: Optional[int] = None,
        current_step: Optional[str] = None,
        is_important: Optional[bool] = None,
    ) -> None:
        """
        ì—ì´ì „íŠ¸ ì‚¬ê³  ë³´ê³ 
        
        ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë©”ì‹œì§€ë¥¼ ê¸°ë¡í•˜ê³  WebSocketìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
        
        [v2.0] ì¤‘ìš” ì´ë²¤íŠ¸ëŠ” DynamoDBì—ë„ ì €ì¥í•˜ì—¬ State Drift ë°©ì§€
        
        Args:
            message: ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë©”ì‹œì§€
            thought_type: ì‚¬ê³  ìœ í˜• (progress, decision, warning, success, error)
            progress: ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            current_step: í˜„ì¬ ë‹¨ê³„ ì—…ë°ì´íŠ¸
            is_important: ì¤‘ìš” ì´ë²¤íŠ¸ ì—¬ë¶€ (Noneì´ë©´ thought_typeìœ¼ë¡œ ìë™ íŒë‹¨)
        """
        if progress is not None:
            self._progress = progress
        if current_step:
            self._current_step = current_step
        
        # ê¸°ìˆ  ë¡œê·¸ë„ ê¸°ë¡
        self._logger.info(f"[{thought_type.upper()}] {message}")
        
        # WebSocket í˜ì´ë¡œë“œ ìƒì„±
        payload = self.task_service.update_task_context(
            self.execution_id,
            thought=message,
            progress=self._progress,
            current_step=self._current_step,
        )
        payload["thought_type"] = thought_type
        
        # WebSocket ì „ì†¡
        if self.websocket_callback:
            try:
                self.websocket_callback(payload)
            except Exception as e:
                self._logger.error(f"Failed to send WebSocket message: {e}")
        
        # [v2.0] ì¤‘ìš” ì´ë²¤íŠ¸ëŠ” DynamoDBì— ì˜ì†í™”
        should_persist = is_important if is_important is not None else (
            thought_type in self.IMPORTANT_THOUGHT_TYPES
        )
        
        if should_persist and self.persist_important_events:
            self._persist_event(payload, thought_type, message)

    def _persist_event(
        self, 
        payload: Dict[str, Any], 
        thought_type: str,
        message: str
    ) -> None:
        """
        ì¤‘ìš” ì´ë²¤íŠ¸ë¥¼ DynamoDB ì´ë²¤íŠ¸ ìŠ¤í† ì–´ì— ì €ì¥
        
        WebSocket ì—°ê²°ì´ ëŠê²¨ë„ ì´ ë°ì´í„°ë¡œ ìƒíƒœ ë³µêµ¬ ê°€ëŠ¥
        """
        if not self.owner_id:
            self._logger.debug("owner_id not set, skipping event persistence")
            return
        
        try:
            event_item = {
                'task_id': self.execution_id,
                'event_id': f"{int(time.time() * 1000)}_{uuid.uuid4().hex[:8]}",
                'owner_id': self.owner_id,
                'timestamp': int(time.time() * 1000),
                'thought_type': thought_type,
                'thought': message,
                'progress': self._progress,
                'current_step': self._current_step,
                'workflow_name': self._workflow_name,
                'workflow_id': self._workflow_id,
                'status': payload.get('status', 'in_progress'),
                # TTL: 7ì¼ í›„ ìë™ ì‚­ì œ
                'ttl': int(time.time()) + (7 * 24 * 60 * 60),
            }
            
            # ë¹„ë™ê¸°ë¡œ ì €ì¥ (ë©”ì¸ í”Œë¡œìš° ë¸”ë¡œí‚¹ ë°©ì§€)
            try:
                self.task_service.task_events_table.put_item(Item=event_item)
            except ClientError as e:
                # í…Œì´ë¸”ì´ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ (ì„ íƒì  ê¸°ëŠ¥)
                if 'ResourceNotFoundException' not in str(e):
                    raise
                    
        except Exception as e:
            # ì˜ì†í™” ì‹¤íŒ¨í•´ë„ ë©”ì¸ í”Œë¡œìš°ëŠ” ê³„ì† ì§„í–‰
            self._logger.warning(f"Failed to persist event: {e}")

    def report_progress(self, percentage: int, step_name: str = "") -> None:
        """ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        self.report_thought(
            f"{step_name} ì²˜ë¦¬ ì¤‘..." if step_name else "ì‘ì—… ì§„í–‰ ì¤‘...",
            thought_type="progress",
            progress=percentage,
            current_step=step_name,
            is_important=False,  # ì§„í–‰ë¥ ì€ ì¤‘ìš” ì´ë²¤íŠ¸ê°€ ì•„ë‹˜
        )

    def report_decision(self, question: str, context: str = "") -> None:
        """ì˜ì‚¬ê²°ì • ìš”ì²­ ë³´ê³ """
        message = f"ê²°ì •ì´ í•„ìš”í•©ë‹ˆë‹¤: {question}"
        if context:
            message += f" ({context})"
        self.report_thought(message, thought_type="decision", is_important=True)

    def report_success(self, message: str) -> None:
        """ì„±ê³µ ë³´ê³ """
        self.report_thought(message, thought_type="success", progress=100, is_important=True)

    def report_error(self, error: str) -> None:
        """ì—ëŸ¬ ë³´ê³  (ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ ë³€í™˜)"""
        from src.models.task_context import get_friendly_error_message
        friendly_message, _ = get_friendly_error_message(error)
        self.report_thought(friendly_message, thought_type="error", is_important=True)
        self._logger.error(f"Original error: {error}")

    def report_warning(self, message: str) -> None:
        """ê²½ê³  ë³´ê³ """
        self.report_thought(message, thought_type="warning", is_important=True)

    async def generate_execution_summary(
        self,
        task_id: str,
        owner_id: str,
        summary_type: str = "business"
    ) -> Dict[str, Any]:
        """
        Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰ ë¡œê·¸ë¥¼ ìš”ì•½
        
        [v3.0] LLM ê¸°ë°˜ ë¡œê·¸ ìš”ì•½ ê¸°ëŠ¥
        - Context Cachingìœ¼ë¡œ ë¹„ìš© ìµœì í™”
        - DynamoDB ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
        
        Args:
            task_id: ì‹¤í–‰ ID
            owner_id: ì‚¬ìš©ì ID
            summary_type: ìš”ì•½ íƒ€ì… (business | technical | full)
        
        Returns:
            {
                "summary": "ìš”ì•½ í…ìŠ¤íŠ¸",
                "key_insights": ["ì¸ì‚¬ì´íŠ¸1", "ì¸ì‚¬ì´íŠ¸2"],
                "recommendations": ["ê¶Œì¥ì‚¬í•­1", "ê¶Œì¥ì‚¬í•­2"],
                "token_usage": {...},
                "generation_time_ms": 1234,
                "cached": false
            }
        """
        # 1. ìºì‹œëœ ìš”ì•½ í™•ì¸
        cache_key = f"summary#{owner_id}#{task_id}#{summary_type}"
        cached_summary = await self._get_cached_summary(cache_key)
        if cached_summary:
            logger.info(f"Returning cached summary for {task_id[:8]}... (type={summary_type})")
            cached_summary["cached"] = True
            return cached_summary
        
        # 2. Task ë°ì´í„° ë¡œë“œ
        task_detail = await self.get_task_detail(task_id, owner_id, include_technical_logs=False)
        if not task_detail:
            raise ValueError(f"Task not found: {task_id}")
        
        # state_history ì¶”ì¶œ
        state_history = []
        payload = task_detail.get('payload', {})
        if payload:
            state_history = payload.get('state_history', [])
            if not state_history:
                step_state = payload.get('step_function_state', {})
                state_history = step_state.get('state_history', [])
        
        if not state_history:
            return {
                "summary": "ì‹¤í–‰ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.",
                "key_insights": [],
                "recommendations": [],
                "cached": False
            }
        
        # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_summary_prompt(state_history, summary_type, task_detail)
        
        # 4. Gemini í˜¸ì¶œ
        from src.services.llm.gemini_service import GeminiService, GeminiConfig, GeminiModel
        
        gemini = GeminiService(config=GeminiConfig(
            model=GeminiModel.GEMINI_2_0_FLASH,
            temperature=0.3,
            max_output_tokens=2048
        ))
        
        start_time = time.time()
        
        # Context Caching: state_historyë¥¼ ìºì‹±í•˜ì—¬ ë¹„ìš© ì ˆê°
        context_to_cache = None
        if len(state_history) > 30:  # ì¶©ë¶„íˆ í° ê²½ìš°ë§Œ ìºì‹±
            context_to_cache = self._serialize_history_for_cache(state_history)
        
        response = gemini.invoke_model(
            user_prompt=prompt,
            system_instruction="ë‹¹ì‹ ì€ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.",
            response_schema={
                "type": "object",
                "properties": {
                    "summary": {
                        "type": "string",
                        "description": "ì „ì²´ ì‹¤í–‰ ìš”ì•½ (3-5ë¬¸ì¥)"
                    },
                    "key_insights": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ì£¼ìš” ì¸ì‚¬ì´íŠ¸ (3-5ê°œ)"
                    },
                    "recommendations": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ê°œì„  ê¶Œì¥ì‚¬í•­ (0-3ê°œ, ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´)"
                    }
                },
                "required": ["summary", "key_insights", "recommendations"]
            },
            context_to_cache=context_to_cache
        )
        
        generation_time_ms = int((time.time() - start_time) * 1000)
        
        # 5. ì‘ë‹µ íŒŒì‹±
        result = response.get("parsed", {})
        result["token_usage"] = response.get("metadata", {}).get("token_usage", {})
        result["generation_time_ms"] = generation_time_ms
        result["model_used"] = "gemini-2.0-flash"
        result["cached"] = False
        
        # 6. ê²°ê³¼ ìºì‹± (24ì‹œê°„ TTL)
        await self._cache_summary(cache_key, result, ttl_hours=24)
        
        logger.info(
            f"Generated summary for {task_id[:8]}... (type={summary_type}, "
            f"tokens={result['token_usage'].get('total_tokens', 0)}, "
            f"time={generation_time_ms}ms)"
        )
        
        return result

    def _build_summary_prompt(
        self,
        state_history: List[Dict[str, Any]],
        summary_type: str,
        task_detail: Dict[str, Any]
    ) -> str:
        """ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # state_history ìƒ˜í”Œë§ (ë„ˆë¬´ ê¸¸ë©´ ìµœê·¼ Nê°œë§Œ)
        MAX_HISTORY_ENTRIES = 50
        sampled_history = state_history[-MAX_HISTORY_ENTRIES:] if len(state_history) > MAX_HISTORY_ENTRIES else state_history
        
        history_text = "\n".join([
            f"[{i+1}] {entry.get('state_name', 'unknown')} - {entry.get('status', 'N/A')} "
            f"(duration: {entry.get('duration', 0)}ms)"
            for i, entry in enumerate(sampled_history)
        ])
        
        if summary_type == "business":
            prompt = f"""
ë‹¤ìŒì€ '{task_detail.get('task_summary', 'AI ì‘ì—…')}'ì˜ ì‹¤í–‰ ë¡œê·¸ì…ë‹ˆë‹¤.

**í˜„ì¬ ìƒíƒœ:** {task_detail.get('status')}
**ì§„í–‰ë¥ :** {task_detail.get('progress_percentage')}%
**ì‹¤í–‰ ë‹¨ê³„:** (ìµœê·¼ {len(sampled_history)}ê°œ)
{history_text}

**ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ ë‹¤ìŒì„ ë¶„ì„í•˜ì„¸ìš”:**
1. ì´ ì‘ì—…ì´ ë¬´ì—‡ì„ ë‹¬ì„±í–ˆëŠ”ì§€ ìš”ì•½ (ê¸°ìˆ  ìš©ì–´ ìµœì†Œí™”)
2. í•µì‹¬ ì„±ê³¼ ë° ì¸ì‚¬ì´íŠ¸ (3-5ê°œ)
3. ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ì™€ ê°œì„  ê¶Œì¥ì‚¬í•­ (ìˆìœ¼ë©´ 1-3ê°œ, ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´)
"""
        
        elif summary_type == "technical":
            token_usage = task_detail.get('token_usage', {})
            prompt = f"""
ë‹¤ìŒì€ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ì˜ ê¸°ìˆ  ë¡œê·¸ì…ë‹ˆë‹¤.

**Execution ID:** {task_detail.get('task_id')}
**Status:** {task_detail.get('status')}
**Workflow:** {task_detail.get('workflow_name')}
**State History:** (ìµœê·¼ {len(sampled_history)}ê°œ)
{history_text}

**Token Usage:** {json.dumps(token_usage) if token_usage else 'N/A'}

**ê¸°ìˆ  ê´€ì ì—ì„œ ë‹¤ìŒì„ ë¶„ì„í•˜ì„¸ìš”:**
1. ì‹¤í–‰ íë¦„ ìš”ì•½ (ë³‘ëª© êµ¬ê°„, ì¬ì‹œë„, ì—ëŸ¬ í¬í•¨)
2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¸ì‚¬ì´íŠ¸ (latency, token usage, ë¹„ìš© ì¶”ì •)
3. ìµœì í™” ê°€ëŠ¥ ì˜ì—­ (ìˆìœ¼ë©´ 1-3ê°œ, ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´)
"""
        
        else:  # full
            prompt = f"""
ë‹¤ìŒì€ ì›Œí¬í”Œë¡œìš° '{task_detail.get('workflow_name')}'ì˜ ì „ì²´ ì‹¤í–‰ ë¡œê·¸ì…ë‹ˆë‹¤.

**ê¸°ë³¸ ì •ë³´:**
- Execution ID: {task_detail.get('task_id')}
- Status: {task_detail.get('status')}
- Progress: {task_detail.get('progress_percentage')}%
- Started: {task_detail.get('started_at')}
- Updated: {task_detail.get('updated_at')}

**ì‹¤í–‰ íˆìŠ¤í† ë¦¬:** (ìµœê·¼ {len(sampled_history)}ê°œ)
{history_text}

**ì¢…í•© ë¶„ì„:**
1. ì „ì²´ ì‹¤í–‰ íë¦„ ìš”ì•½ (ë¹„ì¦ˆë‹ˆìŠ¤ + ê¸°ìˆ )
2. í•µì‹¬ ì¸ì‚¬ì´íŠ¸ (3-5ê°œ)
3. ì¢…í•© ê°œì„  ì œì•ˆ (ìˆìœ¼ë©´ 1-3ê°œ, ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´)
"""
        
        return prompt

    def _serialize_history_for_cache(self, state_history: List[Dict[str, Any]]) -> str:
        """state_historyë¥¼ ìºì‹±ìš© ë¬¸ìì—´ë¡œ ì§ë ¬í™”"""
        # ì¤‘ìš” í•„ë“œë§Œ ì¶”ì¶œí•˜ì—¬ í¬ê¸° ìµœì†Œí™”
        simplified = [
            {
                "name": entry.get('state_name', 'unknown'),
                "status": entry.get('status', 'N/A'),
                "duration": entry.get('duration', 0)
            }
            for entry in state_history
        ]
        return json.dumps(simplified, ensure_ascii=False)

    async def _get_cached_summary(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """DynamoDBì—ì„œ ìºì‹œëœ ìš”ì•½ ì¡°íšŒ"""
        try:
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                partial(
                    self.task_events_table.get_item,
                    Key={'task_id': cache_key, 'event_id': 'SUMMARY_CACHE'}
                )
            )
            
            item = response.get('Item')
            if not item:
                return None
            
            # TTL í™•ì¸
            ttl = item.get('ttl', 0)
            if ttl < int(time.time()):
                return None
            
            # ìºì‹œëœ ë°ì´í„° ë°˜í™˜
            cached_data = item.get('cached_summary')
            if cached_data:
                return json.loads(cached_data) if isinstance(cached_data, str) else cached_data
            
            return None
            
        except ClientError as e:
            if 'ResourceNotFoundException' in str(e):
                # í…Œì´ë¸” ì—†ìœ¼ë©´ ë¬´ì‹œ
                return None
            logger.warning(f"Failed to get cached summary: {e}")
            return None
        except Exception as e:
            logger.warning(f"Cache retrieval error: {e}")
            return None

    async def _cache_summary(self, cache_key: str, summary_data: Dict[str, Any], ttl_hours: int = 24) -> None:
        """DynamoDBì— ìš”ì•½ ê²°ê³¼ ìºì‹±"""
        try:
            cache_item = {
                'task_id': cache_key,
                'event_id': 'SUMMARY_CACHE',
                'cached_summary': json.dumps(summary_data, ensure_ascii=False),
                'ttl': int(time.time()) + (ttl_hours * 60 * 60),
                'cached_at': int(time.time() * 1000)
            }
            
            await asyncio.get_event_loop().run_in_executor(
                None,
                partial(self.task_events_table.put_item, Item=cache_item)
            )
            
            logger.debug(f"Cached summary: {cache_key} (TTL: {ttl_hours}h)")
            
        except ClientError as e:
            if 'ResourceNotFoundException' not in str(e):
                logger.warning(f"Failed to cache summary: {e}")
        except Exception as e:
            logger.warning(f"Cache write error: {e}")
