# -*- coding: utf-8 -*-
import logging
import os
import time
import json
import random
from typing import Dict, Any, Optional, List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

# [v2.1] Centralized Retry Utility
try:
    from src.common.retry_utils import retry_call, retry_stepfunctions, retry_s3
    RETRY_UTILS_AVAILABLE = True
except ImportError:
    RETRY_UTILS_AVAILABLE = False

# [Guard] [v2.2] Ring Protection: Prompt Security Guard
try:
    from src.services.recovery.prompt_security_guard import (
        PromptSecurityGuard,
        get_security_guard,
        RingLevel,
        SecurityViolation,
    )
    RING_PROTECTION_AVAILABLE = True
except ImportError:
    RING_PROTECTION_AVAILABLE = False
    get_security_guard = None
    RingLevel = None

# [Guard] [v2.3] 4ë‹¨ê³„ ì•„í‚¤í…ì²˜: Concurrency Controller
try:
    from src.services.quality_kernel.concurrency_controller import (
        ConcurrencyControllerV2,
        get_concurrency_controller,
        LoadLevel
    )
    CONCURRENCY_CONTROLLER_AVAILABLE = True
except ImportError:
    CONCURRENCY_CONTROLLER_AVAILABLE = False
    get_concurrency_controller = None
    ConcurrencyControllerV2 = None

# Services
from src.services.state.state_manager import StateManager, mask_pii_in_state
from src.services.recovery.self_healing_service import SelfHealingService
# [v3.11] Unified State Hydration
from src.common.state_hydrator import StateHydrator, SmartStateBag
# Legacy Imports (for now, until further refactoring)
from src.services.workflow.repository import WorkflowRepository
# Using generic imports from main handler file as source of truth
from src.handlers.core.main import run_workflow, partition_workflow as _partition_workflow_dynamically, _build_segment_config
from src.common.statebag import normalize_inplace

# [P0 Refactoring] Smart StateBag Architecture
from src.common.state_hydrator import (
    check_inter_segment_edges,
    is_hitp_edge,
    is_loop_exit_edge,
    prepare_response_with_offload
)


logger = logging.getLogger(__name__)

# [v3.13] Kernel Protocol - The Great Seal Pattern
# ëª¨ë“  Lambda â†” ASL í†µì‹ ì„ í‘œì¤€í™”
try:
    from src.common.kernel_protocol import seal_state_bag, open_state_bag, get_from_bag
    KERNEL_PROTOCOL_AVAILABLE = True
except ImportError:
    try:
        from common.kernel_protocol import seal_state_bag, open_state_bag, get_from_bag
        KERNEL_PROTOCOL_AVAILABLE = True
    except ImportError:
        KERNEL_PROTOCOL_AVAILABLE = False
        logger.warning("âš ï¸ kernel_protocol not available - falling back to legacy mode")

# [v3.12] Shared Kernel Library: StateBag as Single Source of Truth
# ExecuteSegment now returns StateBag format directly using universal_sync_core
try:
    from src.handlers.utils.universal_sync_core import universal_sync_core
    UNIVERSAL_SYNC_CORE_AVAILABLE = True
except ImportError:
    UNIVERSAL_SYNC_CORE_AVAILABLE = False
    logger.warning("âš ï¸ universal_sync_core not available - falling back to legacy mode")

# ============================================================================
# ğŸ” [v3.5] None Reference Tracing: Environment-controlled debug logging
# ============================================================================
# Set NONE_TRACE_ENABLED=1 to enable verbose None tracing in logs
NONE_TRACE_ENABLED = os.environ.get("NONE_TRACE_ENABLED", "0") == "1"


def _trace_none_access(
    key: str,
    source: str,
    actual_value: Any,
    context: Dict[str, Any] = None,
    caller: str = None
) -> None:
    """
    ğŸ” [v3.5] Trace None value access for debugging NoneType errors
    
    This utility logs when a None value is accessed from state,
    helping identify the source of NoneType errors in production.
    
    Usage:
        val = state.get('input_list')
        _trace_none_access('input_list', 'state', val, caller='for_each_runner:line2850')
        if val is None:
            # handle None case
    
    Args:
        key: The key that was accessed
        source: Where the value came from (e.g., 'state', 'event', 'config')
        actual_value: The value that was retrieved (will log if None)
        context: Optional dict with additional context (will sample keys for log)
        caller: Optional caller identifier for tracing
    """
    if not NONE_TRACE_ENABLED:
        return
    
    if actual_value is None:
        ctx_keys = list(context.keys())[:10] if isinstance(context, dict) else "N/A"
        logger.warning(
            f"ğŸ” [None Trace] key='{key}' is None from {source}. "
            f"Caller: {caller or 'unknown'}. "
            f"Context keys (sample): {ctx_keys}"
        )


# ============================================================================
# [Guard] [Kernel] Dynamic Scheduling Constants
# ============================================================================
# Memory Safety Margin (Trigger split at 80% usage)
MEMORY_SAFETY_THRESHOLD = 0.8
# Minimum Node Count for Segment Splitting
MIN_NODES_PER_SUB_SEGMENT = 2
# Maximum Split Depth (Prevent infinite splitting)
MAX_SPLIT_DEPTH = 3
# Segment Status Values
SEGMENT_STATUS_PENDING = "PENDING"
SEGMENT_STATUS_RUNNING = "RUNNING"
SEGMENT_STATUS_COMPLETED = "COMPLETED"
SEGMENT_STATUS_SKIPPED = "SKIPPED"
SEGMENT_STATUS_FAILED = "FAILED"

# ============================================================================
# [Guard] [Kernel] Aggressive Retry & Partial Success Constants
# ============================================================================
# Kernel Internal Retry Count (Attempt before Step Functions level retry)
KERNEL_MAX_RETRIES = 3
# Retry Interval (Exponential backoff base)
KERNEL_RETRY_BASE_DELAY = 1.0
# Retryable Error Patterns
RETRYABLE_ERROR_PATTERNS = [
    'ThrottlingException',
    'ServiceUnavailable',
    'TooManyRequestsException',
    'ProvisionedThroughputExceeded',
    'InternalServerError',
    'ConnectionError',
    'TimeoutError',
    'ReadTimeoutError',
    'ConnectTimeoutError',
    'BrokenPipeError',
    'ResourceNotFoundException',  # S3 eventual consistency
]
# Enable Partial Success (Continue workflow even if segment fails)
ENABLE_PARTIAL_SUCCESS = True

# ============================================================================
# [Parallel] [Kernel] Parallel Scheduler Constants
# ============================================================================
# Default Concurrency Limit (Lambda account level)
DEFAULT_MAX_CONCURRENT_MEMORY_MB = 3072  # 3GB (Assuming 3 Lambda concurrent executions)
DEFAULT_MAX_CONCURRENT_TOKENS = 100000   # Tokens per minute limit
DEFAULT_MAX_CONCURRENT_BRANCHES = 10     # Maximum concurrent branches

# Scheduling Strategy
STRATEGY_SPEED_OPTIMIZED = "SPEED_OPTIMIZED"      # Maximize parallel execution
STRATEGY_RESOURCE_OPTIMIZED = "RESOURCE_OPTIMIZED" # Prioritize resource efficiency
STRATEGY_COST_OPTIMIZED = "COST_OPTIMIZED"        # Minimize cost

# Default estimated resources per branch
DEFAULT_BRANCH_MEMORY_MB = 256
DEFAULT_BRANCH_TOKENS = 5000

# Account level hard limit (checked even in SPEED_OPTIMIZED)
ACCOUNT_LAMBDA_CONCURRENCY_LIMIT = 100  # AWS default concurrency limit
ACCOUNT_MEMORY_HARD_LIMIT_MB = 10240    # 10GB hard limit

# State merge policy
MERGE_POLICY_OVERWRITE = "OVERWRITE"      # Later values overwrite (default)
MERGE_POLICY_APPEND_LIST = "APPEND_LIST"  # Lists are merged
MERGE_POLICY_KEEP_FIRST = "KEEP_FIRST"    # Keep first value
MERGE_POLICY_CONFLICT_ERROR = "ERROR"     # Error on conflict

# Key patterns requiring list merge
LIST_MERGE_KEY_PATTERNS = [
    '__new_history_logs',
    '__kernel_actions', 
    '_results',
    '_items',
    '_outputs',
    'collected_',
    'aggregated_'
]


def _safe_get_from_bag(
    event: Dict[str, Any], 
    key: str, 
    default: Any = None,
    caller: str = None,
    log_on_default: bool = False
) -> Any:
    """
    ğŸ›¡ï¸ [v3.13] Kernel Protocol ê¸°ë°˜ Bag ë°ì´í„° ì¶”ì¶œ
    
    kernel_protocol.get_from_bagì„ ì‚¬ìš©í•˜ì—¬ í‘œì¤€í™”ëœ ê²½ë¡œë¡œ ë°ì´í„° ì¶”ì¶œ.
    Kernel Protocolì´ ì—†ìœ¼ë©´ ë ˆê±°ì‹œ ë¡œì§ ì‚¬ìš©.
    
    Args:
        event: Lambda ì´ë²¤íŠ¸
        key: ì¶”ì¶œí•  í‚¤
        default: ê¸°ë³¸ê°’
        caller: (Optional) í˜¸ì¶œì ì‹ë³„ì
        log_on_default: (Optional) ê¸°ë³¸ê°’ ë°˜í™˜ ì‹œ ë¡œê¹…
    
    Returns:
        ì°¾ì€ ê°’ ë˜ëŠ” default
    """
    # [v3.13] Kernel Protocol ì‚¬ìš© (ê¶Œì¥)
    if KERNEL_PROTOCOL_AVAILABLE:
        val = get_from_bag(event, key, default)
        if val == default and log_on_default:
            logger.warning(f"ğŸ” [Kernel Protocol] key='{key}' returned default. Caller: {caller}")
        return val
    
    # Legacy fallback
    if not isinstance(event, dict):
        return default
    
    state_data = event.get('state_data') or {}
    
    if isinstance(state_data, dict):
        bag = state_data.get('bag')
        if isinstance(bag, dict):
            val = bag.get(key)
            if val is not None:
                return val
        
        val = state_data.get(key)
        if val is not None:
            return val
    
    val = event.get(key)
    if val is not None:
        return val
    
    if log_on_default:
        logger.warning(f"ğŸ” [Legacy] key='{key}' returned default. Caller: {caller}")
    
    return default


def _safe_get_total_segments(event: Dict[str, Any]) -> int:
    """
    [Guard] [Fix] total_segmentsë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
    
    ë¬¸ì œì : event.get('total_segments')ê°€ None, "", 0 ë“± ë‹¤ì–‘í•œ ê°’ì¼ ìˆ˜ ìˆìŒ
    - None: Step Functionsì—ì„œ nullë¡œ ì „ë‹¬
    - "": ë¹ˆ ë¬¸ìì—´
    - 0: ìœ íš¨í•˜ì§€ë§Œ int(0)ì€ falsy
    
    Returns:
        int: total_segments (ìµœì†Œ 1 ë³´ì¥)
    """
    raw_value = event.get('total_segments')
    
    # Noneì´ë©´ State Bagì—ì„œ partition_map ì¶”ì¶œí•˜ì—¬ ê³„ì‚°
    if raw_value is None:
        # ğŸ›¡ï¸ [v3.4] _safe_get_from_bag ì‚¬ìš©
        partition_map = _safe_get_from_bag(event, 'partition_map')
        
        if partition_map and isinstance(partition_map, list):
            return max(1, len(partition_map))
        return 1
    
    # ìˆ«ì íƒ€ì…ì´ë©´ ì§ì ‘ ì‚¬ìš©
    if isinstance(raw_value, (int, float)):
        return max(1, int(raw_value))
    
    # ë¬¸ìì—´ì´ë©´ íŒŒì‹± ì‹œë„
    if isinstance(raw_value, str):
        raw_value = raw_value.strip()
        if raw_value and raw_value.isdigit():
            return max(1, int(raw_value))
        # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ íŒŒì‹± ë¶ˆê°€ëŠ¥í•˜ë©´ ê¸°ë³¸ê°’
        return 1
    
    # ê·¸ ì™¸ íƒ€ì…ì€ ê¸°ë³¸ê°’
    return 1


def _normalize_node_config(node: Dict[str, Any]) -> Dict[str, Any]:
    """
    [Option A] ë…¸ë“œ configì—ì„œ None ê°’ì„ ë¹ˆ dict/listë¡œ ì •ê·œí™”

    ë¬¸ì œì : í”„ë¡ íŠ¸ì—”ë“œë‚˜ DBì—ì„œ optional í•„ë“œê°€ nullë¡œ ì €ì¥ë˜ë©´
    Pythonì—ì„œ .get('key', {}).get('nested') íŒ¨í„´ì´ ì‹¤íŒ¨í•¨

    í•´ê²°ì±…: ë…¸ë“œ ì‹¤í–‰ ì „ì— None â†’ {} ì •ê·œí™”
    """
    if not isinstance(node, dict):
        return node or {}

    # ì •ê·œí™”í•  í•„ë“œ ëª©ë¡ (None â†’ {} or [])
    DICT_FIELDS = [
        'config', 'llm_config', 'sub_node_config', 'sub_workflow', 'nested_config',
        'retry_config', 'metadata', 'resource_policy', 'callbacks_config'
    ]
    LIST_FIELDS = [
        'nodes', 'edges', 'branches', 'callbacks', 'input_variables'
    ]

    for field in DICT_FIELDS:
        if field in node and node[field] is None:
            node[field] = {}

    for field in LIST_FIELDS:
        if field in node and node[field] is None:
            node[field] = []

    # ì¬ê·€ì ìœ¼ë¡œ nested configë„ ì •ê·œí™”
    if 'config' in node and isinstance(node['config'], dict):
        _normalize_node_config(node['config'])
    if 'sub_node_config' in node and isinstance(node['sub_node_config'], dict):
        _normalize_node_config(node['sub_node_config'])
    if 'sub_workflow' in node and isinstance(node['sub_workflow'], dict):
        _normalize_node_config(node['sub_workflow'])
    if 'nested_config' in node and isinstance(node['nested_config'], dict):
        _normalize_node_config(node['nested_config'])

    # nodes ë°°ì—´ ë‚´ë¶€ë„ ì •ê·œí™”
    if 'nodes' in node and isinstance(node['nodes'], list):
        for child_node in node['nodes']:
            if isinstance(child_node, dict):
                _normalize_node_config(child_node)

    return node


def _normalize_segment_config(segment_config: Dict[str, Any]) -> Dict[str, Any]:
    """
    [Option A] ì„¸ê·¸ë¨¼íŠ¸ config ì „ì²´ë¥¼ ì •ê·œí™”
    """
    if not isinstance(segment_config, dict):
        return segment_config or {}

    # ì„¸ê·¸ë¨¼íŠ¸ ë ˆë²¨ í•„ë“œ ì •ê·œí™”
    if segment_config.get('nodes') is None:
        segment_config['nodes'] = []
    if segment_config.get('edges') is None:
        segment_config['edges'] = []
    if segment_config.get('branches') is None:
        segment_config['branches'] = []

    # ê° ë…¸ë“œ ì •ê·œí™”
    for node in segment_config.get('nodes', []):
        if isinstance(node, dict):
            _normalize_node_config(node)
            # ğŸ›¡ï¸ [P0 Fix] config ë‚´ë¶€ì˜ sub_workflowë„ ì •ê·œí™” (for_each ë…¸ë“œìš©)
            node_config = node.get('config')
            if isinstance(node_config, dict):
                sub_workflow = node_config.get('sub_workflow')
                if isinstance(sub_workflow, dict):
                    _normalize_node_config(sub_workflow)
                    for sub_node in sub_workflow.get('nodes', []):
                        if isinstance(sub_node, dict):
                            _normalize_node_config(sub_node)

    # ë¸Œëœì¹˜ ë‚´ ë…¸ë“œë„ ì •ê·œí™”
    for branch in segment_config.get('branches', []):
        if isinstance(branch, dict):
            for node in branch.get('nodes', []):
                if isinstance(node, dict):
                    _normalize_node_config(node)
                    # ğŸ›¡ï¸ [P0 Fix] ë¸Œëœì¹˜ ë‚´ for_each configì˜ sub_workflowë„ ì •ê·œí™”
                    node_config = node.get('config')
                    if isinstance(node_config, dict):
                        sub_workflow = node_config.get('sub_workflow')
                        if isinstance(sub_workflow, dict):
                            _normalize_node_config(sub_workflow)

    return segment_config


class SegmentRunnerService:
    def __init__(self, s3_bucket: Optional[str] = None):
        self.state_manager = StateManager()
        self.healer = SelfHealingService()
        self.repo = WorkflowRepository()
        
        # [Perf Optimization] Safe threshold - 180KB for Step Functions with wrapper overhead buffer
        # 256KB SF limit - ~15KB AWS wrapper overhead = ~175KB safe, using 180KB
        SF_SAFE_THRESHOLD = 180000
        
        threshold_str = os.environ.get("STATE_SIZE_THRESHOLD", "")
        if threshold_str and threshold_str.strip():
            try:
                self.threshold = int(threshold_str.strip())
                # Warn if threshold is too high
                if self.threshold > SF_SAFE_THRESHOLD:
                    logger.warning(f"[Warning] STATE_SIZE_THRESHOLD={self.threshold} exceeds safe limit {SF_SAFE_THRESHOLD}")
            except ValueError:
                logger.warning(f"[Warning] Invalid STATE_SIZE_THRESHOLD='{threshold_str}', using default {SF_SAFE_THRESHOLD}")
                self.threshold = SF_SAFE_THRESHOLD
        else:
            self.threshold = SF_SAFE_THRESHOLD
            
        # [v3.11] Unified Bucket Resolution
        # Prioritize S3_BUCKET (standard), fallback to env vars if explicit arg missing
        if s3_bucket:
             self.state_bucket = s3_bucket
        else:
             self.state_bucket = (
                 os.environ.get('WORKFLOW_STATE_BUCKET') or 
                 os.environ.get('S3_BUCKET') or 
                 os.environ.get('SKELETON_S3_BUCKET')
             )
        
        # [v3.11] Initialize StateHydrator once (Reuse connection)
        self.hydrator = StateHydrator(bucket_name=self.state_bucket)
        
        if not self.state_bucket:
            logger.warning("[Warning] [SegmentRunnerService] S3 bucket not configured - large payloads may fail")
        else:
            logger.info(f"[Success] [SegmentRunnerService] S3 bucket: {self.state_bucket}, threshold: {self.threshold}")
        
        # [Guard] [Kernel] S3 Client (Lazy Initialization)
        self._s3_client = None
        
        # [Guard] [v2.2] Ring Protection Security Guard
        self._security_guard = None
        
        # [Guard] [v2.3] 4ë‹¨ê³„ ì•„í‚¤í…ì²˜: Concurrency Controller
        self._concurrency_controller = None
    
    @property
    def security_guard(self):
        """Lazy Security Guard initialization"""
        if self._security_guard is None and RING_PROTECTION_AVAILABLE:
            self._security_guard = get_security_guard()
        return self._security_guard
    
    @property
    def concurrency_controller(self):
        """Lazy Concurrency Controller initialization"""
        if self._concurrency_controller is None and CONCURRENCY_CONTROLLER_AVAILABLE:
            # Reserved Concurrency 200 (template.yamlì—ì„œ ì„¤ì •)
            reserved = int(os.environ.get('RESERVED_CONCURRENCY', 200))
            max_budget = float(os.environ.get('MAX_BUDGET_USD', 10.0))
            self._concurrency_controller = get_concurrency_controller(
                workflow_id="segment_runner",
                reserved_concurrency=reserved,
                max_budget_usd=max_budget,
                enable_batching=True,
                enable_throttling=True
            )
        return self._concurrency_controller
    
    def _safe_json_load(self, content: str) -> Dict[str, Any]:
        """
        ğŸ›¡ï¸ [Critical] Safe JSON loading to prevent UnboundLocalError
        
        Problem: Using 'json' as variable name shadows the module reference,
                 causing UnboundLocalError in nested scopes (ThreadPoolExecutor)
        
        Solution: Explicit import with alias to avoid shadowing
        
        Args:
            content: JSON string to parse
            
        Returns:
            Parsed JSON object (dict) or empty dict on error
        """
        import json as _json_module  # ğŸ›¡ï¸ Alias prevents variable shadowing
        try:
            return _json_module.loads(content)
        except Exception as e:
            logger.error(f"[S3 Recovery] JSON parsing failed: {e}")
            return {}  # ğŸ›¡ï¸ Return empty dict to prevent AttributeError cascade
    
    @property
    def s3_client(self):
        """Lazy S3 client initialization"""
        if self._s3_client is None:
            import boto3
            self._s3_client = boto3.client('s3')
        return self._s3_client

    # ========================================================================
    #  [Utility] State Merge: ë¬´ê²°ì„± ë³´ì¥ ìƒíƒœ ë³‘í•©
    # ========================================================================
    def _should_merge_as_list(self, key: str) -> bool:
        """
        Check if this key is a list merge target
        """
        for pattern in LIST_MERGE_KEY_PATTERNS:
            if pattern in key or key.startswith(pattern):
                return True
        return False

    def _merge_states(
        self,
        base_state: Dict[str, Any],
        new_state: Dict[str, Any],
        merge_policy: str = MERGE_POLICY_APPEND_LIST
    ) -> Dict[str, Any]:
        """
        [System] Integrity-guaranteed state merging
        
        Policy:
        - OVERWRITE: Simple overwrite (existing behavior)
        - APPEND_LIST: Merge list keys, overwrite others
        - KEEP_FIRST: Keep existing keys
        - ERROR: Raise exception on key conflict
        
        Special handling:
        - __new_history_logs, __kernel_actions, etc. always merge as lists
        - Keys starting with _ are treated specially
        """
        # ğŸ›¡ï¸ [v3.6] Immortal Kernel: ë³‘í•© ì‹œì‘ ì „ ì´ì¤‘ ì•ˆì „ì¥ì¹˜ (Dual StateBag)
        from src.common.statebag import ensure_state_bag
        base_state = ensure_state_bag(base_state)
        new_state = ensure_state_bag(new_state) # Noneì´ë©´ ë¹ˆ StateBag({})ì´ ë¨, iteration ì•ˆì „ ë³´ì¥
        
        if merge_policy == MERGE_POLICY_OVERWRITE:
            result = base_state.copy()
            result.update(new_state) # Safe now
            return result

        result = base_state.copy()
        conflicts = []
        
        for key, new_value in new_state.items():
            if key not in result:
                # New key: just add
                result[key] = new_value
                continue
            
            existing_value = result[key]
            
            # Check if key is list merge target
            if self._should_merge_as_list(key):
                # ğŸ›¡ï¸ [P0 Fix] ë¦¬ìŠ¤íŠ¸ ë³‘í•© ìˆœì„œ: existing + new (ì‹œê°„ìˆœ ìœ ì§€)
                # ë¡œê·¸ëŠ” ì‹œê°„ìˆœìœ¼ë¡œ ë’¤ì— ë¶™ëŠ” ê²ƒì´ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.
                if isinstance(existing_value, list) and isinstance(new_value, list):
                    result[key] = existing_value + new_value  # ê¸°ì¡´ ë’¤ì— ìƒˆ ê°’ ì¶”ê°€
                elif isinstance(new_value, list):
                    result[key] = ([existing_value] if existing_value else []) + new_value
                elif isinstance(existing_value, list):
                    result[key] = existing_value + ([new_value] if new_value else [])
                else:
                    result[key] = [existing_value, new_value] if existing_value and new_value else [existing_value or new_value]
                continue
            
            # Handle according to policy
            if merge_policy == MERGE_POLICY_KEEP_FIRST:
                # Keep existing value
                continue
            elif merge_policy == MERGE_POLICY_CONFLICT_ERROR:
                if existing_value != new_value:
                    conflicts.append(key)
            else:
                # APPEND_LIST default: overwrite if not list
                result[key] = new_value
        
        if conflicts:
            logger.warning(f"[Merge] State conflicts detected on keys: {conflicts}")
            if merge_policy == MERGE_POLICY_CONFLICT_ERROR:
                raise ValueError(f"State merge conflict on keys: {conflicts}")
        
        return result

    def _cleanup_branch_intermediate_s3(
        self,
        parallel_results: List[Dict[str, Any]],
        workflow_id: str,
        segment_id: int
    ) -> None:
        """
        [Critical] S3 ì¤‘ê°„ ë¸Œëœì¹˜ ê²°ê³¼ íŒŒì¼ ì •ë¦¬ (Garbage Collection)
        
        ğŸ›¡ï¸ [P0 ê°•í™”] ë©±ë“±ì„± ë³´ì¥ + ì¬ì‹œë„ ë¡œì§
        
        Aggregation ì™„ë£Œ í›„ ê° ë¸Œëœì¹˜ê°€ ìƒì„±í•œ ì„ì‹œ S3 íŒŒì¼ë“¤ì„ ì‚­ì œí•˜ì—¬
        S3 ë¹„ìš© ì ˆê° ë° ê´€ë¦¬ ë¶€í•˜ ê°ì†Œ
        
        âš ï¸ ì‹¤ìš´ì˜ ê¶Œì¥ì‚¬í•­:
        - S3 Lifecycle Policy ì„¤ì • í•„ìˆ˜ (24ì‹œê°„ í›„ ìë™ ì‚­ì œ)
        - ì‚­ì œ ì‹¤íŒ¨ ì‹œ 'ìœ ë ¹ ë°ì´í„°' ë°©ì§€
        
        Args:
            parallel_results: ë¸Œëœì¹˜ ì‹¤í–‰ ê²°ê³¼ ëª©ë¡
            workflow_id: ì›Œí¬í”Œë¡œìš° ID
            segment_id: í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ID
        """
        if not parallel_results:
            return
        
        s3_paths_to_delete = []
        
        # ë¸Œëœì¹˜ ê²°ê³¼ì—ì„œ S3 path ìˆ˜ì§‘
        for result in parallel_results:
            if not result or not isinstance(result, dict):
                continue
            
            s3_path = result.get('final_state_s3_path') or result.get('state_s3_path')
            if s3_path:
                s3_paths_to_delete.append(s3_path)
        
        if not s3_paths_to_delete:
            logger.debug(f"[Aggregator] No S3 intermediate files to cleanup")
            return
        
        logger.info(f"[Aggregator] Cleaning up {len(s3_paths_to_delete)} S3 intermediate files")
        
        # ğŸ›¡ï¸ [P0] ì¬ì‹œë„ ë¡œì§ ì¶”ê°€ (max 2íšŒ)
        MAX_RETRIES = 2
        failed_paths = []
        
        def delete_s3_object_with_retry(s3_path: str) -> Tuple[bool, str]:
            """S3 ê°ì²´ ì‚­ì œ (ì¬ì‹œë„ í¬í•¨)"""
            for attempt in range(MAX_RETRIES + 1):
                try:
                    bucket = s3_path.replace("s3://", "").split("/")[0]
                    key = "/".join(s3_path.replace("s3://", "").split("/")[1:])
                    self.state_manager.s3_client.delete_object(Bucket=bucket, Key=key)
                    return True, s3_path
                except Exception as e:
                    if attempt < MAX_RETRIES:
                        time.sleep(0.1 * (attempt + 1))  # ë°±ì˜¤í”„
                        continue
                    logger.warning(
                        f"[Aggregator] âš ï¸ Failed to delete {s3_path} after {MAX_RETRIES + 1} attempts: {e}. "
                        f"This file may become 'ghost data'. Consider S3 Lifecycle Policy."
                    )
                    return False, s3_path
            return False, s3_path
        
        # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì‚­ì œ (ë¹ ë¥´ê²Œ ì²˜ë¦¬)
        deleted_count = 0
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = {executor.submit(delete_s3_object_with_retry, path): path for path in s3_paths_to_delete}
            
            for future in as_completed(futures, timeout=30):
                try:
                    success, path = future.result(timeout=5)
                    if success:
                        deleted_count += 1
                    else:
                        failed_paths.append(path)
                except Exception as e:
                    logger.warning(f"[Aggregator] Cleanup future failed: {e}")
        
        # ê²°ê³¼ ë¡œê¹…
        if failed_paths:
            logger.warning(
                f"[Aggregator] âš ï¸ Cleanup incomplete: {deleted_count}/{len(s3_paths_to_delete)} deleted, "
                f"{len(failed_paths)} failed. Ghost data paths: {failed_paths[:3]}{'...' if len(failed_paths) > 3 else ''}"
            )
        else:
            logger.info(f"[Aggregator] Cleanup complete: {deleted_count}/{len(s3_paths_to_delete)} files deleted")

    # ========================================================================
    # [Pattern 1] Segment-Level Self-Healing: Segment Auto-Splitting
    # ========================================================================
    def _estimate_segment_memory(self, segment_config: Dict[str, Any], state: Dict[str, Any]) -> int:
        """Memory estimation."""
        base_memory = 50  # base overhead
        
        nodes = segment_config.get('nodes', [])
        if not nodes:
            return base_memory
        
        node_memory = len(nodes) * 10  # 10MB per node
        
        llm_memory = 0
        foreach_memory = 0
        
        for node in nodes:
            # ğŸ›¡ï¸ [v3.8] None defense in nodes iteration
            if node is None or not isinstance(node, dict):
                continue
                
            node_type = node.get('type', '')
            
            # [Debug] [KERNEL DEBUG] ë…¸ë“œ íƒ€ì… ë³€ì§ˆ ì¶”ì  - code íƒ€ì…ì´ ë°œê²¬bë˜ë©´ ë¡œê·¸
            if node_type == 'code':
                logger.warning(
                    f"[KERNEL DEBUG] Detected 'code' type node! "
                    f"Node ID: {node.get('id')}, Config keys: {list(node.get('config', {}).keys())}. "
                    f"This should have been aliased to 'operator' by Pydantic validator."
                )
            
            if node_type in ('llm_chat', 'aiModel'):
                llm_memory += 50  # LLM nodes get additional 50MB
            elif node_type == 'for_each':
                config = node.get('config', {})
                items_key = config.get('input_list_key', '')
                if items_key and items_key in state:
                    items = state.get(items_key, [])
                    if isinstance(items, list):
                        foreach_memory += len(items) * 5
        
        # [Optimization] State size estimation based on metadata (avoid json.dumps)
        state_size_mb = self._estimate_state_size_lightweight(state)
        
        total = base_memory + node_memory + llm_memory + foreach_memory + int(state_size_mb)
        
        logger.debug(f"[Kernel] Memory estimate: base={base_memory}, nodes={node_memory}, "
                    f"llm={llm_memory}, foreach={foreach_memory}, state={state_size_mb:.1f}MB, total={total}MB")
        
        return total

    def _estimate_state_size_lightweight(self, state: Dict[str, Any], max_sample_keys: int = 20) -> float:
        """
        [Optimization] Lightweight estimation of state size without json.dumps
        
        Strategy:
        1. Sample only top N keys to calculate average size
        2. Estimate lists as length * average item size
        3. Use len() for strings
        4. Estimate nested dicts by key count
        
        Returns:
            Estimated size (MB)
        """
        if not state or not isinstance(state, dict):
            return 0.1  # minimum 100KB
        
        total_bytes = 0
        keys = list(state.keys())[:max_sample_keys]
        
        for key in keys:
            value = state.get(key)
            total_bytes += self._estimate_value_size(value)
        
        # Estimate total size based on sampling ratio
        if len(state) > max_sample_keys:
            sample_ratio = len(state) / max_sample_keys
            total_bytes = int(total_bytes * sample_ratio)
        
        return total_bytes / (1024 * 1024)  # bytes â†’ MB

    def _estimate_value_size(self, value: Any, depth: int = 0) -> int:
        """
        Heuristically estimate value size (bytes)
        
        Prevent infinite loops with recursion depth limit
        """
        if depth > 3:  # depth limit
            return 100  # approximate estimate
        
        if value is None:
            return 4
        elif isinstance(value, bool):
            return 4
        elif isinstance(value, (int, float)):
            return 8
        elif isinstance(value, str):
            return len(value.encode('utf-8', errors='ignore'))
        elif isinstance(value, bytes):
            return len(value)
        elif isinstance(value, list):
            if not value:
                return 2
            # Sample only first 3 items to calculate average
            sample = value[:3]
            avg_size = sum(self._estimate_value_size(v, depth + 1) for v in sample) / len(sample)
            return int(avg_size * len(value))
        elif isinstance(value, dict):
            if not value:
                return 2
            # Sample only first 5 keys
            sample_keys = list(value.keys())[:5]
            sample_size = sum(
                len(str(k)) + self._estimate_value_size(value[k], depth + 1) 
                for k in sample_keys
            )
            if len(value) > 5:
                return int(sample_size * len(value) / 5)
            return sample_size
        else:
            # Other types: approximate estimate
            return 100

    def _split_segment(self, segment_config: Dict[str, Any], split_depth: int = 0) -> List[Dict[str, Any]]:
        """
        Split segment into smaller sub-segments
        
        Splitting strategy:
        1. Split node list in half
        2. Maintain dependencies: preserve edge connections
        3. ìµœì†Œ ë…¸ë“œ ìˆ˜ ë³´ì¥
        """
        if split_depth >= MAX_SPLIT_DEPTH:
            logger.warning(f"[Kernel] Max split depth ({MAX_SPLIT_DEPTH}) reached, returning original segment")
            return [segment_config]
        
        nodes = segment_config.get('nodes', [])
        edges = segment_config.get('edges', [])
        
        if len(nodes) < MIN_NODES_PER_SUB_SEGMENT * 2:
            logger.info(f"[Kernel] Segment too small to split ({len(nodes)} nodes)")
            return [segment_config]
        
        # ë…¸ë“œë¥¼ ë°˜ìœ¼ë¡œ ë¶„í• 
        mid = len(nodes) // 2
        first_nodes = nodes[:mid]
        second_nodes = nodes[mid:]
        
        # [Critical Guard] None í•„í„°ë§ (nodes ë°°ì—´ì— Noneì´ ì„ì—¬ìˆì„ ìˆ˜ ìˆìŒ)
        first_nodes = [n for n in first_nodes if n is not None]
        second_nodes = [n for n in second_nodes if n is not None]
        
        if not first_nodes or not second_nodes:
            logger.warning(f"[Kernel] Node filtering resulted in empty segment, returning original")
            return [segment_config]
        
        first_node_ids = {n.get('id') for n in first_nodes if isinstance(n, dict)}
        second_node_ids = {n.get('id') for n in second_nodes if isinstance(n, dict)}
        
        # ì—£ì§€ ë¶„ë¦¬: ê° ì„œë¸Œ ì„¸ê·¸ë¨¼íŠ¸ ë‚´ë¶€ ì—£ì§€ë§Œ ìœ ì§€
        # [Critical Guard] edgesë„ Noneì¼ ìˆ˜ ìˆê³ , ê° edgeë„ Noneì´ê±°ë‚˜ dictê°€ ì•„ë‹ ìˆ˜ ìˆìŒ
        first_edges = [e for e in edges 
                      if e is not None and isinstance(e, dict) 
                      and e.get('source') in first_node_ids and e.get('target') in first_node_ids]
        second_edges = [e for e in edges 
                       if e is not None and isinstance(e, dict)
                       and e.get('source') in second_node_ids and e.get('target') in second_node_ids]
        
        # ì„œë¸Œ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
        original_id = segment_config.get('id', 'segment')
        
        sub_segment_1 = {
            **segment_config,
            'id': f"{original_id}_sub_1",
            'nodes': first_nodes,
            'edges': first_edges,
            '_kernel_split': True,
            '_split_depth': split_depth + 1,
            '_parent_segment_id': original_id
        }
        
        sub_segment_2 = {
            **segment_config,
            'id': f"{original_id}_sub_2",
            'nodes': second_nodes,
            'edges': second_edges,
            '_kernel_split': True,
            '_split_depth': split_depth + 1,
            '_parent_segment_id': original_id
        }
        
        logger.info(f"[Kernel] [System] Segment '{original_id}' split into 2 sub-segments: "
                   f"{len(first_nodes)} + {len(second_nodes)} nodes")
        
        return [sub_segment_1, sub_segment_2]

    def _execute_with_auto_split(
        self, 
        segment_config: Dict[str, Any], 
        initial_state: Dict[str, Any],
        auth_user_id: str,
        split_depth: int = 0
    ) -> Dict[str, Any]:
        """Pattern 1: Memory-based auto-split execution."""
        # ì‚¬ìš© ê°€ëŠ¥í•œ Lambda ë©”ëª¨ë¦¬
        available_memory = int(os.environ.get('AWS_LAMBDA_FUNCTION_MEMORY_SIZE', 512))
        
        # ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ ì¶”ì •
        estimated_memory = self._estimate_segment_memory(segment_config, initial_state)
        
        # ì•ˆì „ ì„ê³„ê°’ ì²´í¬
        if estimated_memory > available_memory * MEMORY_SAFETY_THRESHOLD:
            logger.info(f"[Kernel] [Warning] Memory pressure detected: {estimated_memory}MB estimated, "
                       f"{available_memory}MB available (threshold: {MEMORY_SAFETY_THRESHOLD*100}%)")
            
            # ë¶„í•  ì‹œë„
            sub_segments = self._split_segment(segment_config, split_depth)
            
            if len(sub_segments) > 1:
                logger.info(f"[Kernel] [System] Executing {len(sub_segments)} sub-segments sequentially")
                
                # ì„œë¸Œ ì„¸ê·¸ë¨¼íŠ¸ ìˆœì°¨ ì‹¤í–‰
                current_state = initial_state.copy()
                all_logs = []
                kernel_actions = []
                
                for i, sub_seg in enumerate(sub_segments):
                    logger.info(f"[Kernel] Executing sub-segment {i+1}/{len(sub_segments)}: {sub_seg.get('id')}")
                    
                    # ì¬ê·€ì ìœ¼ë¡œ ìë™ ë¶„í•  ì ìš©
                    sub_result = self._execute_with_auto_split(
                        sub_seg, current_state, auth_user_id, split_depth + 1
                    )
                    
                    # [System] ë¬´ê²°ì„± ë³´ì¥ ìƒíƒœ ë³‘í•© (ë¦¬ìŠ¤íŠ¸ í‚¤ëŠ” í•©ì¹¨)
                    if isinstance(sub_result, dict):
                        current_state = self._merge_states(
                            current_state, 
                            sub_result,
                            merge_policy=MERGE_POLICY_APPEND_LIST
                        )
                        # all_logsëŠ” ì´ë¯¸ _merge_statesì—ì„œ ì²˜ë¦¬ë¨
                    
                    kernel_actions.append({
                        'action': 'SPLIT_EXECUTE',
                        'sub_segment_id': sub_seg.get('id'),
                        'index': i,
                        'timestamp': time.time()
                    })
                
                # ì»¤ë„ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                current_state['__kernel_actions'] = kernel_actions
                current_state['__new_history_logs'] = all_logs
                
                return current_state
        
        # ì •ìƒ ì‹¤í–‰ (ë¶„í•  ë¶ˆí•„ìš”)
        return run_workflow(
            config_json=segment_config,
            initial_state=initial_state,
            ddb_table_name=os.environ.get("JOB_TABLE"),
            user_api_keys={},
            run_config={"user_id": auth_user_id}
        )

    # ========================================================================
    # [Guard] [Pattern 2] Manifest Mutation: S3 Manifest ë™ì  ìˆ˜ì •
    # ========================================================================
    def _load_manifest_from_s3(self, manifest_s3_path: str) -> Optional[List[Dict[str, Any]]]:
        """S3ì—ì„œ segment_manifest ë¡œë“œ"""
        if not manifest_s3_path or not manifest_s3_path.startswith('s3://'):
            return None
        
        try:
            parts = manifest_s3_path.replace('s3://', '').split('/', 1)
            bucket = parts[0]
            key = parts[1] if len(parts) > 1 else ''
            
            response = self.s3_client.get_object(Bucket=bucket, Key=key)
            content = response['Body'].read().decode('utf-8')
            manifest = self._safe_json_load(content)
            
            logger.info(f"[Kernel] Loaded manifest from S3: {len(manifest)} segments")
            return manifest
            
        except Exception as e:
            logger.error(f"[Kernel] Failed to load manifest from S3: {e}")
            return None

    def _save_manifest_to_s3(self, manifest: List[Dict[str, Any]], manifest_s3_path: str) -> bool:
        """ìˆ˜ì •ëœ segment_manifestë¥¼ S3ì— ì €ì¥"""
        if not manifest_s3_path or not manifest_s3_path.startswith('s3://'):
            return False
        
        try:
            parts = manifest_s3_path.replace('s3://', '').split('/', 1)
            bucket = parts[0]
            key = parts[1] if len(parts) > 1 else ''
            
            self.s3_client.put_object(
                Bucket=bucket,
                Key=key,
                Body=json.dumps(manifest, ensure_ascii=False).encode('utf-8'),
                ContentType='application/json',
                Metadata={
                    'kernel_modified': 'true',
                    'modified_at': str(int(time.time()))
                }
            )
            
            logger.info(f"[Kernel] Saved modified manifest to S3: {len(manifest)} segments")
            return True
            
        except Exception as e:
            logger.error(f"[Kernel] Failed to save manifest to S3: {e}")
            return False

    def _check_segment_status(self, segment_config: Dict[str, Any]) -> str:
        """ì„¸ê·¸ë¨¼íŠ¸ ìƒíƒœ í™•ì¸ (SKIPPED ë“±)"""
        return segment_config.get('status', SEGMENT_STATUS_PENDING)

    def _mark_segments_for_skip(
        self, 
        manifest_s3_path: str, 
        segment_ids_to_skip: List[int], 
        reason: str
    ) -> bool:
        """
        [Guard] [Pattern 2] íŠ¹ì • ì„¸ê·¸ë¨¼íŠ¸ë¥¼ SKIPìœ¼ë¡œ ë§ˆí‚¹
        
        ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
        - ì¡°ê±´ ë¶„ê¸°ì—ì„œ íŠ¹ì • ê²½ë¡œ ë¶ˆí•„ìš”
        - ì„ í–‰ ì„¸ê·¸ë¨¼íŠ¸ ì‹¤íŒ¨ë¡œ í›„ì† ì„¸ê·¸ë¨¼íŠ¸ ì‹¤í–‰ ë¶ˆê°€
        """
        manifest = self._load_manifest_from_s3(manifest_s3_path)
        if not manifest:
            return False
        
        modified = False
        for segment in manifest:
            if segment.get('segment_id') in segment_ids_to_skip:
                segment['status'] = SEGMENT_STATUS_SKIPPED
                segment['skip_reason'] = reason
                segment['skipped_at'] = int(time.time())
                segment['skipped_by'] = 'kernel'
                modified = True
                logger.info(f"[Kernel] Marked segment {segment.get('segment_id')} for skip: {reason}")
        
        if modified:
            return self._save_manifest_to_s3(manifest, manifest_s3_path)
        
        return False

    def _inject_recovery_segments(
        self,
        manifest_s3_path: str,
        after_segment_id: int,
        recovery_segments: List[Dict[str, Any]],
        reason: str
    ) -> bool:
        """
        [Guard] [Pattern 2] ë³µêµ¬ ì„¸ê·¸ë¨¼íŠ¸ ì‚½ì…
        
        ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
        - API ì‹¤íŒ¨ í›„ ë°±ì—… ê²½ë¡œ ì‚½ì…
        - ì—ëŸ¬ í•¸ë“¤ë§ ì„¸ê·¸ë¨¼íŠ¸ ë™ì  ì¶”ê°€
        """
        manifest = self._load_manifest_from_s3(manifest_s3_path)
        if not manifest:
            return False
        
        # ì‚½ì… ìœ„ì¹˜ ì°¾ê¸°
        insert_index = None
        for i, segment in enumerate(manifest):
            if segment.get('segment_id') == after_segment_id:
                insert_index = i + 1
                break
        
        if insert_index is None:
            logger.warning(f"[Kernel] Could not find segment {after_segment_id} for recovery injection")
            return False
        
        # ë³µêµ¬ ì„¸ê·¸ë¨¼íŠ¸ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
        max_segment_id = max(s.get('segment_id', 0) for s in manifest)
        for i, rec_seg in enumerate(recovery_segments):
            rec_seg['segment_id'] = max_segment_id + i + 1
            rec_seg['status'] = SEGMENT_STATUS_PENDING
            rec_seg['injected_by'] = 'kernel'
            rec_seg['injection_reason'] = reason
            rec_seg['injected_at'] = int(time.time())
            rec_seg['type'] = rec_seg.get('type', 'recovery')
        
        # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ì— ì‚½ì…
        new_manifest = manifest[:insert_index] + recovery_segments + manifest[insert_index:]
        
        # í›„ì† ì„¸ê·¸ë¨¼íŠ¸ ID ì¬ì¡°ì •
        for i, segment in enumerate(new_manifest):
            segment['execution_order'] = i
        
        logger.info(f"[Kernel] [System] Injected {len(recovery_segments)} recovery segments after segment {after_segment_id}")
        
        return self._save_manifest_to_s3(new_manifest, manifest_s3_path)

    # ========================================================================
    # [Parallel] [Pattern 3] Parallel Scheduler: ì¸í”„ë¼ ì¸ì§€í˜• ë³‘ë ¬ ìŠ¤ì¼€ì¤„ë§
    # ========================================================================
    
    def _offload_branches_to_s3(
        self,
        branches: List[Dict[str, Any]],
        owner_id: str,
        workflow_id: str,
        segment_id: int
    ) -> Tuple[List[Dict[str, Any]], str]:
        """
        ğŸŒ¿ [Pointer Strategy] ê° ë¸Œëœì¹˜ë¥¼ S3ì— ì—…ë¡œë“œí•˜ê³  ê²½ëŸ‰ í¬ì¸í„° ë°°ì—´ ë°˜í™˜
        
        Map ë‚´ë¶€ Hydrate ì „ëµ:
        - ì „ì²´ branches ë°ì´í„°ë¥¼ S3ì— ì—…ë¡œë“œ (ë‹¨ì¼ íŒŒì¼)
        - pending_branchesì—ëŠ” ì¸ë±ìŠ¤ + S3 ê²½ë¡œë§Œ í¬í•¨ëœ ê²½ëŸ‰ í¬ì¸í„° ë°°ì—´ ì „ë‹¬
        - Map Iterator ì²« ë‹¨ê³„ì—ì„œ ê° ë¸Œëœì¹˜ê°€ S3 ê²½ë¡œë¡œ ìì‹ ì˜ ë°ì´í„° hydrate
        
        Returns:
            (branch_pointers, branches_s3_path)
            - branch_pointers: [{branch_index, branch_id, branches_s3_path, total_branches}, ...]
            - branches_s3_path: ì „ì²´ branches ë°°ì—´ì´ ì €ì¥ëœ S3 ê²½ë¡œ
        """
        if not branches:
            return [], None
        
        if not self.state_bucket:
            logger.warning("[Pointer Strategy] No S3 bucket configured. Returning inline branches (may exceed payload limit)")
            # í´ë°±: ì¸ë¼ì¸ ë°˜í™˜ (ìœ„í—˜í•˜ì§€ë§Œ S3 ì—†ìœ¼ë©´ ì–´ì©” ìˆ˜ ì—†ìŒ)
            return branches, None
        
        try:
            import boto3
            s3_client = boto3.client('s3')
            
            timestamp = int(time.time() * 1000)  # ë°€ë¦¬ì´ˆ ë‹¨ìœ„
            s3_key = f"workflow-states/{owner_id}/{workflow_id}/segments/{segment_id}/branches/{timestamp}/all_branches.json"
            
            # ì „ì²´ branches ë°°ì—´ì„ S3ì— ì—…ë¡œë“œ
            branches_json = json.dumps(branches, default=str)
            s3_client.put_object(
                Bucket=self.state_bucket,
                Key=s3_key,
                Body=branches_json,
                ContentType='application/json'
            )
            
            branches_s3_path = f"s3://{self.state_bucket}/{s3_key}"
            branches_size_kb = len(branches_json) / 1024
            
            logger.info(f"[Pointer Strategy] âœ… Uploaded {len(branches)} branches ({branches_size_kb:.1f}KB) to {branches_s3_path}")
            
            # ê²½ëŸ‰ í¬ì¸í„° ë°°ì—´ ìƒì„±
            # Map Iteratorì—ì„œ ê° í¬ì¸í„°ë¥¼ ë°›ì•„ S3ì—ì„œ ìì‹ ì˜ ë¸Œëœì¹˜ ë°ì´í„°ë¥¼ hydrate
            branch_pointers = []
            for idx, branch in enumerate(branches):
                pointer = {
                    'branch_index': idx,
                    'branch_id': branch.get('id') or branch.get('branch_id') or f'branch_{idx}',
                    'branches_s3_path': branches_s3_path,
                    'total_branches': len(branches),
                    # ğŸ’¡ ê²½ëŸ‰ ë©”íƒ€ë°ì´í„°ë§Œ í¬í•¨ (hydrate ì „ í•„ìš”í•œ ìµœì†Œ ì •ë³´)
                    'segment_count': len(branch.get('partition_map', [])) if branch.get('partition_map') else 0,
                }
                branch_pointers.append(pointer)
            
            pointer_size = len(json.dumps(branch_pointers, default=str))
            logger.info(f"[Pointer Strategy] ğŸ“¦ Created {len(branch_pointers)} pointers ({pointer_size/1024:.2f}KB) - "
                       f"Compression ratio: {branches_size_kb * 1024 / max(pointer_size, 1):.1f}x")
            
            return branch_pointers, branches_s3_path
            
        except Exception as e:
            logger.error(f"[Pointer Strategy] âŒ Failed to offload branches to S3: {e}")
            # í´ë°±: ì¸ë¼ì¸ ë°˜í™˜ (ìœ„í—˜í•˜ì§€ë§Œ S3 ì‹¤íŒ¨ ì‹œ)
            return branches, None
    
    def _estimate_branch_resources(self, branch: Dict[str, Any], state: Dict[str, Any]) -> Dict[str, int]:
        """
        ë¸Œëœì¹˜ì˜ ì˜ˆìƒ ìì› ìš”êµ¬ëŸ‰ ì¶”ì •
        
        Returns:
            {
                'memory_mb': ì˜ˆìƒ ë©”ëª¨ë¦¬ (MB),
                'tokens': ì˜ˆìƒ í† í° ìˆ˜,
                'llm_calls': LLM í˜¸ì¶œ íšŸìˆ˜,
                'has_shared_resource': ê³µìœ  ìì› ì ‘ê·¼ ì—¬ë¶€
            }
        """
        # ğŸ›¡ï¸ [P0 Fix] None ë˜ëŠ” dictê°€ ì•„ë‹Œ ë¸Œëœì¹˜ ë°©ì–´
        if not branch or not isinstance(branch, dict):
            logger.warning(f"[Scheduler] [Warning] Invalid branch object in resource estimation: {type(branch)}")
            return {
                'memory_mb': DEFAULT_BRANCH_MEMORY_MB,
                'tokens': 0,
                'llm_calls': 0,
                'has_shared_resource': False
            }
        
        # [Critical Fix] ìˆ¨ì–´ìˆëŠ” ë…¸ë“œë“¤ê¹Œì§€ íˆ¬ì‹œí•´ì„œ í† í° ê³„ì‚°
        all_nodes = branch.get('nodes', [])
        if not all_nodes and 'partition_map' in branch:
            # íŒŒí‹°ì…”ë‹ëœ ë¸Œëœì¹˜ë¼ë©´ ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ì˜ ë…¸ë“œë¥¼ í•©ì³ì„œ ê³„ì‚° ëŒ€ìƒì— í¬í•¨
            for segment in branch.get('partition_map', []):
                if isinstance(segment, dict):
                    all_nodes.extend(segment.get('nodes', []))
        
        if not all_nodes:
            return {
                'memory_mb': DEFAULT_BRANCH_MEMORY_MB,
                'tokens': 0,
                'llm_calls': 0,
                'has_shared_resource': False
            }
        
        memory_mb = 50  # ê¸°ë³¸ ì˜¤ë²„í—¤ë“œ
        tokens = 0
        llm_calls = 0
        has_shared_resource = False
        
        for node in all_nodes:
            # ğŸ›¡ï¸ [v3.8] None defense in nodes iteration
            if node is None or not isinstance(node, dict):
                continue
            node_type = node.get('type', '')
            config = node.get('config', {})
            
            # ë©”ëª¨ë¦¬ ì¶”ì •
            memory_mb += 10  # ë…¸ë“œë‹¹ ê¸°ë³¸ 10MB
            
            if node_type in ('llm_chat', 'aiModel', 'llm', 'aimodel'):
                memory_mb += 50  # LLM ë…¸ë“œ ì¶”ê°€ ë©”ëª¨ë¦¬
                llm_calls += 1
                # í† í° ì¶”ì •: í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ê¸°ë°˜
                prompt = config.get('prompt', '') or config.get('system_prompt', '') or config.get('prompt_template', '')
                tokens += len(prompt) // 4 + 500  # ëŒ€ëµì  í† í° ì¶”ì • + ì‘ë‹µ ì˜ˆìƒ
                
            elif node_type == 'for_each':
                items_key = config.get('input_list_key', '')
                if items_key and items_key in state:
                    items = state.get(items_key, [])
                    if isinstance(items, list):
                        iteration_count = len(items)
                        memory_mb += iteration_count * 5
                        # for_each ë‚´ë¶€ì— LLMì´ ìˆìœ¼ë©´ í† í° í­ì¦
                        # [Fix] None defense: config['sub_node_config'] ë˜ëŠ” config['sub_workflow']ê°€ Noneì¼ ìˆ˜ ìˆìŒ
                        sub_config = config.get('sub_node_config') or config.get('sub_workflow') or {}
                        sub_nodes = sub_config.get('nodes', []) if isinstance(sub_config, dict) else []
                        for sub_node in sub_nodes:
                            # [Fix] sub_nodeê°€ Noneì¼ ìˆ˜ ìˆìŒ
                            if sub_node and isinstance(sub_node, dict) and sub_node.get('type') in ('llm_chat', 'aiModel'):
                                # [Critical Fix] Multiply by iteration count for accurate token estimation
                                tokens += iteration_count * 5000  # ì•„ì´í…œë‹¹ 5000 í† í° ì˜ˆìƒ
                                llm_calls += iteration_count
                                logger.debug(f"[Scheduler] for_each with LLM: {iteration_count} iterations Ã— 5000 tokens = {iteration_count * 5000} tokens")
            
            # ê³µìœ  ìì› ì ‘ê·¼ ê°ì§€
            if node_type in ('db_write', 's3_write', 'api_call'):
                has_shared_resource = True
            if config.get('write_to_db') or config.get('write_to_s3'):
                has_shared_resource = True
        
        return {
            'memory_mb': memory_mb,
            'tokens': tokens,
            'llm_calls': llm_calls,
            'has_shared_resource': has_shared_resource
        }

    def _bin_pack_branches(
        self,
        branches: List[Dict[str, Any]],
        resource_estimates: List[Dict[str, int]],
        resource_policy: Dict[str, Any]
    ) -> List[List[Dict[str, Any]]]:
        """
        ğŸ¯ Bin Packing ì•Œê³ ë¦¬ì¦˜: ë¸Œëœì¹˜ë¥¼ ì‹¤í–‰ ë°°ì¹˜ë¡œ ê·¸ë£¹í™”
        
        ì „ëµ:
        1. ë¬´ê±°ìš´ ë¸Œëœì¹˜ ë¨¼ì € ë°°ì¹˜ (First Fit Decreasing)
        2. ê° ë°°ì¹˜ì˜ ì´ ìì›ì´ ì œí•œì„ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ êµ¬ì„±
        3. ê³µìœ  ìì› ì ‘ê·¼ ë¸Œëœì¹˜ëŠ” ë³„ë„ ë°°ì¹˜
        
        Returns:
            [[batch1_branches], [batch2_branches], ...]
        """
        # [Fix] Use 'or' to handle None values - .get() returns None if key exists with None value
        max_memory = resource_policy.get('max_concurrent_memory_mb') or DEFAULT_MAX_CONCURRENT_MEMORY_MB
        max_tokens = resource_policy.get('max_concurrent_tokens') or DEFAULT_MAX_CONCURRENT_TOKENS
        max_branches = resource_policy.get('max_concurrent_branches') or DEFAULT_MAX_CONCURRENT_BRANCHES
        strategy = resource_policy.get('strategy') or STRATEGY_RESOURCE_OPTIMIZED
        
        # ë¸Œëœì¹˜ì™€ ìì› ì¶”ì •ì¹˜ ê²°í•© í›„ í¬ê¸°ìˆœ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
        indexed_branches = list(zip(branches, resource_estimates, range(len(branches))))
        
        # ì „ëµì— ë”°ë¥¸ ì •ë ¬ ê¸°ì¤€
        if strategy == STRATEGY_COST_OPTIMIZED:
            # í† í° ë§ì€ ê²ƒ ë¨¼ì € (ë¹„ìš©ì´ í° ì‘ì—… ìˆœì°¨ ì²˜ë¦¬)
            indexed_branches.sort(key=lambda x: x[1]['tokens'], reverse=True)
        else:
            # ë©”ëª¨ë¦¬ ë§ì€ ê²ƒ ë¨¼ì € (ê¸°ë³¸)
            indexed_branches.sort(key=lambda x: x[1]['memory_mb'], reverse=True)
        
        # ê³µìœ  ìì› ì ‘ê·¼ ë¸Œëœì¹˜ ë¶„ë¦¬
        shared_resource_branches = []
        normal_branches = []
        
        for branch, estimate, idx in indexed_branches:
            if estimate['has_shared_resource']:
                shared_resource_branches.append((branch, estimate, idx))
            else:
                normal_branches.append((branch, estimate, idx))
        
        # Bin Packing (First Fit Decreasing)
        batches: List[List[Tuple]] = []
        batch_resources: List[Dict[str, int]] = []
        
        for branch, estimate, idx in normal_branches:
            placed = False
            
            for i, batch in enumerate(batches):
                current = batch_resources[i]
                
                # ì´ ë°°ì¹˜ì— ì¶”ê°€ ê°€ëŠ¥í•œì§€ í™•ì¸
                new_memory = current['memory_mb'] + estimate['memory_mb']
                new_tokens = current['tokens'] + estimate['tokens']
                new_count = len(batch) + 1
                
                if (new_memory <= max_memory and 
                    new_tokens <= max_tokens and 
                    new_count <= max_branches):
                    
                    batch.append((branch, estimate, idx))
                    batch_resources[i] = {
                        'memory_mb': new_memory,
                        'tokens': new_tokens
                    }
                    placed = True
                    break
            
            if not placed:
                # ìƒˆ ë°°ì¹˜ ìƒì„±
                batches.append([(branch, estimate, idx)])
                batch_resources.append({
                    'memory_mb': estimate['memory_mb'],
                    'tokens': estimate['tokens']
                })
        
        # ê³µìœ  ìì› ë¸Œëœì¹˜ëŠ” ê°ê° ë³„ë„ ë°°ì¹˜ (Race Condition ë°©ì§€)
        for branch, estimate, idx in shared_resource_branches:
            batches.append([(branch, estimate, idx)])
            batch_resources.append({
                'memory_mb': estimate['memory_mb'],
                'tokens': estimate['tokens']
            })
        
        # ê²°ê³¼ ë³€í™˜: ë¸Œëœì¹˜ë§Œ ì¶”ì¶œ
        result = []
        for batch in batches:
            result.append([item[0] for item in batch])
        
        return result

    def _schedule_parallel_group(
        self,
        segment_config: Dict[str, Any],
        state: Dict[str, Any],
        segment_id: int,
        owner_id: str = None,
        workflow_id: str = None
    ) -> Dict[str, Any]:
        """
        [Parallel] ë³‘ë ¬ ê·¸ë£¹ ìŠ¤ì¼€ì¤„ë§: resource_policyì— ë”°ë¼ ì‹¤í–‰ ë°°ì¹˜ ê²°ì •
        
        ğŸŒ¿ [Pointer Strategy] Map ë‚´ë¶€ Hydrateë¥¼ ìœ„í•œ S3 ì˜¤í”„ë¡œë”©:
        - ì „ì²´ branches ë°ì´í„°ë¥¼ S3ì— ì—…ë¡œë“œ
        - pending_branchesì—ëŠ” ê²½ëŸ‰ í¬ì¸í„° ë°°ì—´ë§Œ ì „ë‹¬ (branch_index, branch_s3_path)
        - Map Iterator ë‚´ë¶€ì—ì„œ ê° ë¸Œëœì¹˜ê°€ S3 ê²½ë¡œë¡œ ìì‹ ì˜ ë°ì´í„° hydrate
        
        Returns:
            {
                'status': 'PARALLEL_GROUP' | 'SCHEDULED_PARALLEL',
                'branches': [...] (ê²½ëŸ‰ í¬ì¸í„° ë°°ì—´ - S3 ê²½ë¡œ í¬í•¨),
                'branches_s3_path': S3 ê²½ë¡œ (ì „ì²´ branches ë°ì´í„° ìœ„ì¹˜),
                'execution_batches': [[...], [...]] (ë°°ì¹˜ êµ¬ì¡°),
                'scheduling_metadata': {...}
            }
        """
        branches = segment_config.get('branches', [])
        resource_policy = segment_config.get('resource_policy', {})
        
        # resource_policyê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë³‘ë ¬ ì‹¤í–‰
        if not resource_policy:
            logger.info(f"[Scheduler] No resource_policy, using default parallel execution for {len(branches)} branches")
            
            # ğŸŒ¿ [Pointer Strategy] S3ì— ë¸Œëœì¹˜ ì˜¤í”„ë¡œë”©
            branch_pointers, branches_s3_path = self._offload_branches_to_s3(
                branches=branches,
                owner_id=owner_id or 'unknown',
                workflow_id=workflow_id or 'unknown',
                segment_id=segment_id
            )
            
            return {
                'status': 'PARALLEL_GROUP',
                'branches': branch_pointers,  # ê²½ëŸ‰ í¬ì¸í„° ë°°ì—´
                'branches_s3_path': branches_s3_path,  # S3 ê²½ë¡œ
                'execution_batches': [branch_pointers],  # ë‹¨ì¼ ë°°ì¹˜ (í¬ì¸í„°)
                'scheduling_metadata': {
                    'strategy': 'DEFAULT',
                    'total_branches': len(branches),
                    'batch_count': 1,
                    'pointer_strategy': True,
                    'branches_s3_path': branches_s3_path
                }
            }
        
        strategy = resource_policy.get('strategy', STRATEGY_RESOURCE_OPTIMIZED)
        
        # SPEED_OPTIMIZED: ê°€ë“œë ˆì¼ ì²´í¬ í›„ ìµœëŒ€ ë³‘ë ¬ ì‹¤í–‰
        if strategy == STRATEGY_SPEED_OPTIMIZED:
            # [Guard] ê³„ì • ìˆ˜ì¤€ í•˜ë“œ ë¦¬ë°‹ ì²´í¬ (ì‹œìŠ¤í…œ íŒ¨ë‹‰ ë°©ì§€)
            if len(branches) > ACCOUNT_LAMBDA_CONCURRENCY_LIMIT:
                logger.warning(f"[Scheduler] [Warning] SPEED_OPTIMIZED but branch count ({len(branches)}) "
                              f"exceeds account concurrency limit ({ACCOUNT_LAMBDA_CONCURRENCY_LIMIT})")
                # í•˜ë“œ ë¦¬ë°‹ ì ìš©í•˜ì—¬ ë°°ì¹˜ ë¶„í• 
                forced_policy = {
                    'max_concurrent_branches': ACCOUNT_LAMBDA_CONCURRENCY_LIMIT,
                    'max_concurrent_memory_mb': ACCOUNT_MEMORY_HARD_LIMIT_MB,
                    'strategy': STRATEGY_SPEED_OPTIMIZED
                }
                # ìì› ì¶”ì • ë° ë°°ì¹˜ ë¶„í• 
                resource_estimates = [self._estimate_branch_resources(b, state) for b in branches]
                execution_batches = self._bin_pack_branches(branches, resource_estimates, forced_policy)
                
                logger.info(f"[Scheduler] [Guard] Guardrail applied: {len(execution_batches)} batches")
                
                # ğŸŒ¿ [Pointer Strategy] S3ì— ë¸Œëœì¹˜ ì˜¤í”„ë¡œë”©
                branch_pointers, branches_s3_path = self._offload_branches_to_s3(
                    branches=branches,
                    owner_id=owner_id or 'unknown',
                    workflow_id=workflow_id or 'unknown',
                    segment_id=segment_id
                )
                
                return {
                    'status': 'SCHEDULED_PARALLEL',
                    'branches': branch_pointers,  # ê²½ëŸ‰ í¬ì¸í„° ë°°ì—´
                    'branches_s3_path': branches_s3_path,
                    'execution_batches': execution_batches,  # ì›ë³¸ ë°°ì¹˜ êµ¬ì¡° (ìŠ¤ì¼€ì¤„ë§ìš©)
                    'scheduling_metadata': {
                        'strategy': strategy,
                        'total_branches': len(branches),
                        'batch_count': len(execution_batches),
                        'guardrail_applied': True,
                        'reason': 'Account concurrency limit exceeded',
                        'pointer_strategy': True,
                        'branches_s3_path': branches_s3_path
                    }
                }
            
            logger.info(f"[Scheduler] SPEED_OPTIMIZED: All {len(branches)} branches in parallel")
            
            # ğŸŒ¿ [Pointer Strategy] S3ì— ë¸Œëœì¹˜ ì˜¤í”„ë¡œë”©
            branch_pointers, branches_s3_path = self._offload_branches_to_s3(
                branches=branches,
                owner_id=owner_id or 'unknown',
                workflow_id=workflow_id or 'unknown',
                segment_id=segment_id
            )
            
            return {
                'status': 'PARALLEL_GROUP',
                'branches': branch_pointers,  # ê²½ëŸ‰ í¬ì¸í„° ë°°ì—´
                'branches_s3_path': branches_s3_path,
                'execution_batches': [branch_pointers],
                'scheduling_metadata': {
                    'strategy': strategy,
                    'total_branches': len(branches),
                    'batch_count': 1,
                    'guardrail_applied': False,
                    'pointer_strategy': True,
                    'branches_s3_path': branches_s3_path
                }
            }
        
        # ìì› ì¶”ì •
        resource_estimates = []
        total_memory = 0
        total_tokens = 0
        
        for branch in branches:
            estimate = self._estimate_branch_resources(branch, state)
            resource_estimates.append(estimate)
            total_memory += estimate['memory_mb']
            total_tokens += estimate['tokens']
        
        logger.info(f"[Scheduler] Resource estimates: {total_memory}MB memory, {total_tokens} tokens, "
                   f"{len(branches)} branches")
        
        # ì œí•œ í™•ì¸ - [Fix] Use 'or' to handle None values
        max_memory = resource_policy.get('max_concurrent_memory_mb') or DEFAULT_MAX_CONCURRENT_MEMORY_MB
        max_tokens = resource_policy.get('max_concurrent_tokens') or DEFAULT_MAX_CONCURRENT_TOKENS
        
        # ì œí•œ ë‚´ë¼ë©´ ë‹¨ì¼ ë°°ì¹˜
        if total_memory <= max_memory and total_tokens <= max_tokens:
            logger.info(f"[Scheduler] Resources within limits, single batch execution")
            
            # ğŸŒ¿ [Pointer Strategy] S3ì— ë¸Œëœì¹˜ ì˜¤í”„ë¡œë”©
            branch_pointers, branches_s3_path = self._offload_branches_to_s3(
                branches=branches,
                owner_id=owner_id or 'unknown',
                workflow_id=workflow_id or 'unknown',
                segment_id=segment_id
            )
            
            return {
                'status': 'PARALLEL_GROUP',
                'branches': branch_pointers,  # ê²½ëŸ‰ í¬ì¸í„° ë°°ì—´
                'branches_s3_path': branches_s3_path,
                'execution_batches': [branch_pointers],
                'scheduling_metadata': {
                    'strategy': strategy,
                    'total_branches': len(branches),
                    'batch_count': 1,
                    'total_memory_mb': total_memory,
                    'total_tokens': total_tokens,
                    # [Guard] [v3.4] Deep Evidence Metrics
                    'total_tokens_calculated': total_tokens,
                    'actual_concurrency_limit': max_tokens,
                    'pointer_strategy': True,
                    'branches_s3_path': branches_s3_path
                }
            }
        
        # Bin Packingìœ¼ë¡œ ë°°ì¹˜ ìƒì„±
        execution_batches = self._bin_pack_branches(branches, resource_estimates, resource_policy)
        
        logger.info(f"[Scheduler] [System] Created {len(execution_batches)} execution batches from {len(branches)} branches")
        for i, batch in enumerate(execution_batches):
            batch_memory = sum(self._estimate_branch_resources(b, state)['memory_mb'] for b in batch)
            logger.info(f"[Scheduler]   Batch {i+1}: {len(batch)} branches, ~{batch_memory}MB")
        
        # ğŸŒ¿ [Pointer Strategy] S3ì— ë¸Œëœì¹˜ ì˜¤í”„ë¡œë”©
        branch_pointers, branches_s3_path = self._offload_branches_to_s3(
            branches=branches,
            owner_id=owner_id or 'unknown',
            workflow_id=workflow_id or 'unknown',
            segment_id=segment_id
        )
        
        return {
            'status': 'SCHEDULED_PARALLEL',
            'branches': branch_pointers,  # ê²½ëŸ‰ í¬ì¸í„° ë°°ì—´
            'branches_s3_path': branches_s3_path,
            'execution_batches': execution_batches,  # ì›ë³¸ ë°°ì¹˜ êµ¬ì¡° (ìŠ¤ì¼€ì¤„ë§ìš©)
            'scheduling_metadata': {
                'strategy': strategy,
                'total_branches': len(branches),
                'batch_count': len(execution_batches),
                'total_memory_mb': total_memory,
                'total_tokens': total_tokens,
                # [Guard] [v3.4] Deep Evidence Metrics
                'total_tokens_calculated': total_tokens,
                'actual_concurrency_limit': max_tokens,
                'resource_policy': resource_policy,
                'pointer_strategy': True,
                'branches_s3_path': branches_s3_path
            }
        }

    # ========================================================================
    # [Parallel] [Aggregator] ë³‘ë ¬ ë¸Œëœì¹˜ ê²°ê³¼ ì§‘ê³„
    # ========================================================================
    def _handle_aggregator(self, event: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë³‘ë ¬ ë¸Œëœì¹˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì§‘ê³„í•˜ì—¬ ë‹¨ì¼ ìƒíƒœë¡œ ë³‘í•©
        
        ASLì˜ AggregateParallelResultsì—ì„œ í˜¸ì¶œë¨:
        - parallel_results: ê° ë¸Œëœì¹˜ì˜ ì‹¤í–‰ ê²°ê³¼ ë°°ì—´
        - current_state: ë³‘ë ¬ ì‹¤í–‰ ì „ ìƒíƒœ
        - map_error: (ì„ íƒ) Map ì „ì²´ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ì •ë³´
        
        Returns:
            Merged final state + next segment info
        """
        # ğŸ›¡ï¸ [v3.6] Ensure base_state integrity at aggregation start (Entrance to Aggregator)
        from src.common.statebag import ensure_state_bag
        
        # ğŸ” [v3.5 None Trace] Aggregator entry tracing
        _trace_none_access('current_state', 'event', event.get('current_state'), 
                           context=event, caller='Aggregator:Entry')
        _trace_none_access('parallel_results', 'event', event.get('parallel_results'), 
                           context=event, caller='Aggregator:Entry')
        
        # current_state even if null becomes StateBag({})
        current_state = ensure_state_bag(event.get('current_state', {}))
        
        # [Critical Fix] Restore current_state if S3 offloaded
        if isinstance(current_state, dict) and current_state.get('__s3_offloaded') is True:
            offloaded_s3_path = current_state.get('__s3_path')
            if offloaded_s3_path:
                logger.info(f"[Aggregator] current_state is S3 offloaded. Restoring from: {offloaded_s3_path}")
                try:
                    current_state = self.state_manager.download_state_from_s3(offloaded_s3_path)
                    current_state = ensure_state_bag(current_state)
                    logger.info(f"[Aggregator] Successfully restored current_state from S3 "
                               f"({len(json.dumps(current_state, ensure_ascii=False).encode('utf-8'))/1024:.1f}KB)")
                except Exception as e:
                    logger.error(f"[Aggregator] Failed to restore current_state from S3: {e}")
                    # Fallback to empty state
                    current_state = ensure_state_bag({})
        
        # âš ï¸ Keep current_state as local variable only - DO NOT add to event
        
        parallel_results = event.get('parallel_results', [])
        base_state = current_state  # Use local variable instead of event.get
        segment_to_run = event.get('segment_to_run', 0)
        workflow_id = event.get('workflowId') or event.get('workflow_id')
        auth_user_id = event.get('ownerId') or event.get('owner_id')
        map_error = event.get('map_error')  # [Guard] Map overall error info
        
        logger.info(f"[Aggregator] [Parallel] Aggregating {len(parallel_results)} branch results"
                   + (f" (map_error present)" if map_error else ""))
        
        # [Optimization] Parallelize S3 Hydration (Solve N+1 Query problem)
        # Sequential download is timeout risk when many branches (50+)
        # [DEBUG] Log parallel_results structure before processing
        logger.info(f"[Aggregator] ğŸ” DEBUG: parallel_results type: {type(parallel_results)}")
        logger.info(f"[Aggregator] ğŸ” DEBUG: parallel_results length: {len(parallel_results) if parallel_results else 0}")
        if parallel_results:
            for idx, item in enumerate(parallel_results[:3]):  # Log first 3 items
                logger.info(f"[Aggregator] ğŸ” DEBUG: parallel_results[{idx}] keys: {list(item.keys()) if isinstance(item, dict) else 'NOT_DICT'}")
        
        # ThreadPoolExecutorë¡œ ë³‘ë ¬ fetch
        branches_needing_s3 = []
        for i, result in enumerate(parallel_results):
            if not result or not isinstance(result, dict):
                logger.info(f"[Aggregator] âš ï¸ DEBUG: Skipping parallel_results[{i}] - not a dict")
                continue
            
            # [Critical Fix] Unwrap Lambda invoke wrapper if present
            # Distributed Map State returns: {"Payload": {...}}
            logger.info(f"[Aggregator] ğŸ” DEBUG: parallel_results[{i}] has 'Payload' key: {'Payload' in result}")
            if 'Payload' in result and isinstance(result['Payload'], dict):
                logger.info(f"[Aggregator] âœ… DEBUG: Unwrapping Payload for parallel_results[{i}]")
                result = result['Payload']
                parallel_results[i] = result  # Update in-place for later processing
            
            branch_s3_path = result.get('final_state_s3_path') or result.get('state_s3_path')
            branch_state = result.get('final_state') or result.get('state') or {}
            
            # [Critical Fix] __s3_offloaded í”Œë˜ê·¸ í™•ì¸ ì¶”ê°€
            is_offloaded = isinstance(branch_state, dict) and branch_state.get('__s3_offloaded') is True
            if is_offloaded:
                # S3 offloadëœ ìƒíƒœ: __s3_pathì—ì„œ ì‹¤ì œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
                branch_s3_path = branch_state.get('__s3_path')
                logger.info(f"[Aggregator] Branch {i} has __s3_offloaded flag. S3 path: {branch_s3_path}")
            
            is_empty = isinstance(branch_state, dict) and len(branch_state) <= 1  # {} or {"__state_truncated": true}
            
            # S3 ë³µì›ì´ í•„ìš”í•œ ê²½ìš°: ë¹ˆ ìƒíƒœ OR offloaded í”Œë˜ê·¸
            if (is_empty or is_offloaded) and branch_s3_path:
                branches_needing_s3.append((i, branch_s3_path, result))
        
        # ë³‘ë ¬ S3 fetch ì‹¤í–‰
        if branches_needing_s3:
            logger.info(f"[Aggregator] ğŸš€ Parallel S3 fetch for {len(branches_needing_s3)} branches")
            
            def fetch_branch_s3(item: Tuple[int, str, Dict]) -> Tuple[int, Optional[Dict[str, Any]]]:
                idx, s3_path, result = item
                try:
                    bucket = s3_path.replace("s3://", "").split("/")[0]
                    key = "/".join(s3_path.replace("s3://", "").split("/")[1:])
                    obj = self.state_manager.s3_client.get_object(Bucket=bucket, Key=key)
                    content = obj['Body'].read().decode('utf-8')
                    state = self._safe_json_load(content)  # ğŸ›¡ï¸ Use safe loader
                    return (idx, state)
                except Exception as e:
                    logger.error(f"[Aggregator] S3 recovery failed for branch {idx}: {e}")
                    return (idx, {})  # ğŸ›¡ï¸ Return empty dict instead of None
            
            # ë³‘ë ¬ ì‹¤í–‰ (ìµœëŒ€ 10ê°œ ë™ì‹œ)
            with ThreadPoolExecutor(max_workers=10) as executor:
                futures = {executor.submit(fetch_branch_s3, item): item for item in branches_needing_s3}
                
                for future in as_completed(futures, timeout=60):  # 60ì´ˆ ì „ì²´ timeout
                    try:
                        idx, state = future.result(timeout=5)  # ê°œë³„ 5ì´ˆ timeout
                        if state:
                            # ì›ë³¸ ê²°ê³¼ì— hydrated state ì£¼ì…
                            parallel_results[idx]['final_state'] = state
                            parallel_results[idx]['__hydrated_from_s3'] = True
                    except Exception as e:
                        item = futures[future]
                        logger.warning(f"[Aggregator] Future failed for branch {item[0]}: {e}")
            
            logger.info(f"[Aggregator] âœ… Parallel S3 fetch completed")
        
        # 1. ëª¨ë“  ë¸Œëœì¹˜ ê²°ê³¼ ë³‘í•©
        aggregated_state = base_state.copy()
        all_history_logs = []
        branch_errors = []
        successful_branches = 0
        
        # [Guard] Map ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ê¸°ë¡
        if map_error:
            branch_errors.append({
                'branch_id': '__MAP_ERROR__',
                'error': map_error
            })
            logger.warning(f"[Aggregator] [Warning] Map execution failed: {map_error}")
        
        for i, branch_result in enumerate(parallel_results):
            # 1. Null Guard (ë£¨í”„ ì‹œì‘í•˜ìë§ˆì ì²´í¬)
            if branch_result is None or not isinstance(branch_result, dict):
                logger.error(f"[Aggregator] Branch {i} is None or invalid.")
                branch_errors.append({'branch_id': f'branch_{i}', 'error': 'Null Result'})
                continue

            # 2. ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë£¨í”„ ë‚´ë¶€)
            branch_id = branch_result.get('branch_id', f'branch_{i}')
            branch_status = branch_result.get('branch_status') or branch_result.get('status', 'UNKNOWN')
            
            # ğŸ›¡ï¸ [Fix] branch_stateë¥¼ ë£¨í”„ ë‚´ë¶€ì—ì„œ ì•ˆì „í•˜ê²Œ íšë“
            # [Note] ë³‘ë ¬ S3 fetchê°€ ì´ë¯¸ ì™„ë£Œë˜ì–´ hydrated stateê°€ ì£¼ì…ë¨
            branch_state = branch_result.get('final_state') or branch_result.get('state') or {}
            
            branch_logs = branch_result.get('new_history_logs', [])
            error_info = branch_result.get('error_info')
            
            # [Removed] ìˆœì°¨ S3 hydration ë¡œì§ ì œê±° (ì´ë¯¸ ë³‘ë ¬ë¡œ ì²˜ë¦¬ë¨)
            # ë³‘ë ¬ fetchì—ì„œ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ ì—¬ê¸°ì„œ fallback ì‹œë„
            if isinstance(branch_state, dict) and len(branch_state) == 0:
                branch_s3_path = branch_result.get('final_state_s3_path') or branch_result.get('state_s3_path')
                if branch_s3_path and not branch_result.get('__hydrated_from_s3'):
                    # ë³‘ë ¬ fetch ì‹¤íŒ¨ ì‹œ fallback (ìˆœì°¨ ì¬ì‹œë„)
                    logger.warning(f"[Aggregator] Fallback: Sequential fetch for branch {branch_id}")
                    try:
                        bucket = branch_s3_path.replace("s3://", "").split("/")[0]
                        key = "/".join(branch_s3_path.replace("s3://", "").split("/")[1:])
                        obj = self.state_manager.s3_client.get_object(Bucket=bucket, Key=key)
                        content = obj['Body'].read().decode('utf-8')
                        branch_state = self._safe_json_load(content)  # ğŸ›¡ï¸ Use safe loader
                    except Exception as e:
                        logger.error(f"[Aggregator] Fallback failed for branch {branch_id}: {e}")
                        branch_errors.append({
                            'branch_id': branch_id,
                            'error': f"S3 Hydration Failed: {str(e)}"
                        })
            
            # ì‹¤ì œ ìƒíƒœ ë³‘í•© ì‹¤í–‰
            if isinstance(branch_state, dict):
                aggregated_state = self._merge_states(
                    aggregated_state,
                    branch_state,
                    merge_policy=MERGE_POLICY_APPEND_LIST
                )
                
                if branch_status in ('COMPLETE', 'SUCCEEDED', 'COMPLETED', 'CONTINUE'):
                    successful_branches += 1
            
            # ì—ëŸ¬ ìˆ˜ì§‘
            if error_info:
                branch_errors.append({
                    'branch_id': branch_id,
                    'error': error_info
                })
            
            # íˆìŠ¤í† ë¦¬ ë¡œê·¸ ìˆ˜ì§‘ (Memory Safe Truncation)
            if isinstance(branch_logs, list):
                # ğŸ›¡ï¸ [Guard] Prevent unlimited log growth from thousands of branches
                MAX_AGGREGATED_LOGS = 100
                current_log_count = len(all_history_logs)
                
                if current_log_count < MAX_AGGREGATED_LOGS:
                    remaining_slots = MAX_AGGREGATED_LOGS - current_log_count
                    if len(branch_logs) > remaining_slots:
                        all_history_logs.extend(branch_logs[:remaining_slots])
                        all_history_logs.append(f"[Aggregator] Logs truncated: exceeded limit of {MAX_AGGREGATED_LOGS} entries")
                    else:
                        all_history_logs.extend(branch_logs)
        
        # 2. ì§‘ê³„ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        aggregated_state['__aggregator_metadata'] = {
            'total_branches': len(parallel_results),
            'successful_branches': successful_branches,
            'failed_branches': len(branch_errors),
            'aggregated_at': time.time(),
            'logs_truncated': len(all_history_logs) >= 100 
        }
        
        if branch_errors:
            aggregated_state['__branch_errors'] = branch_errors
        
        # [Token Aggregation] ë³‘ë ¬ ë¸Œëœì¹˜ë“¤ì˜ í† í° ì‚¬ìš©ëŸ‰ í•©ì‚°
        from src.handlers.core.token_utils import extract_token_usage
        
        total_input_tokens = 0
        total_output_tokens = 0
        total_tokens = 0
        branch_token_details = []
        
        for branch_result in parallel_results:
            if not isinstance(branch_result, dict):
                continue
                
            branch_id = branch_result.get('branch_id', 'unknown')
            branch_state = branch_result.get('final_state') or branch_result.get('state') or {}
            
            # [DEBUG] ë¸Œëœì¹˜ ìƒíƒœì—ì„œ í† í° ê´€ë ¨ í‚¤ ë¡œê¹…
            token_keys = [k for k in branch_state.keys() if 'token' in k.lower() or k == 'usage']
            logger.info(f"[Aggregator] [DEBUG] Branch {branch_id} token-related keys: {token_keys}")
            if 'usage' in branch_state:
                logger.info(f"[Aggregator] [DEBUG] Branch {branch_id} usage: {branch_state.get('usage')}")
            
            # ë¸Œëœì¹˜ì˜ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
            usage = extract_token_usage(branch_state)
            input_tokens = usage['input_tokens']
            output_tokens = usage['output_tokens']
            branch_total = usage['total_tokens']
            logger.info(f"[Aggregator] [DEBUG] Branch {branch_id} extracted: {usage}")
            
            total_input_tokens += input_tokens
            total_output_tokens += output_tokens
            total_tokens += branch_total
            
            branch_token_details.append({
                'branch_id': branch_id,
                'input_tokens': input_tokens,
                'output_tokens': output_tokens,
                'total_tokens': branch_total
            })
        
        # í•©ì‚°ëœ í† í° ì •ë³´ë¥¼ aggregated_stateì— ê¸°ë¡
        aggregated_state['total_input_tokens'] = total_input_tokens
        aggregated_state['total_output_tokens'] = total_output_tokens
        aggregated_state['total_tokens'] = total_tokens
        aggregated_state['branch_token_details'] = branch_token_details
        
        logger.info(f"[Aggregator] [Token Aggregation] {len(branch_token_details)} branches, "
                   f"total tokens: {total_tokens} ({total_input_tokens} input + {total_output_tokens} output)")
        
        # 3. ìƒíƒœ ì €ì¥ (S3 ì˜¤í”„ë¡œë”© í¬í•¨)
        s3_bucket_raw = os.environ.get("S3_BUCKET") or os.environ.get("SKELETON_S3_BUCKET") or ""
        s3_bucket = s3_bucket_raw.strip() if s3_bucket_raw else None
        
        if not s3_bucket:
            logger.error("[Alert] [CRITICAL] S3_BUCKET/SKELETON_S3_BUCKET not set for aggregation!")
        
        # [Fix] AggregatorëŠ” ë°ì´í„° ìœ ì‹¤ ë°©ì§€ë¥¼ ìœ„í•´ ë” ë³´ìˆ˜ì ì¸ ì„ê³„ê°’ ì ìš©
        # [Update] ê° ë¸Œëœì¹˜ê°€ 50KBë¡œ ì œí•œë˜ë¯€ë¡œ aggregatorëŠ” 120KB threshold ì ìš©
        # ì˜ˆ: 3ê°œ ë¸Œëœì¹˜ x 50KB = 150KB â†’ S3 ì˜¤í”„ë¡œë“œ ë°œìƒ
        # ì°¸ê³ : ë¸Œëœì¹˜ê°€ S3 ë ˆí¼ëŸ°ìŠ¤ë§Œ ë°˜í™˜í•˜ë©´ aggregator ì…ë ¥ì€ ì‘ì§€ë§Œ,
        #       hydration í›„ ë³‘í•©ëœ ê²°ê³¼ëŠ” í´ ìˆ˜ ìˆìŒ
        AGGREGATOR_SAFE_THRESHOLD = 120000  # 120KB (256KB ë¦¬ë°‹ì˜ 47%)
        
        # [Critical] ë³‘í•©ëœ ìƒíƒœ í¬ê¸° ì¸¡ì • (S3 ì˜¤í”„ë¡œë“œ ê²°ì • ì „)
        aggregated_size = len(json.dumps(aggregated_state, ensure_ascii=False).encode('utf-8'))
        logger.info(f"[Aggregator] Merged state size: {aggregated_size/1024:.1f}KB, "
                   f"threshold: {AGGREGATOR_SAFE_THRESHOLD/1024:.0f}KB")
        
        final_state, output_s3_path = self.state_manager.handle_state_storage(
            state=aggregated_state,
            auth_user_id=auth_user_id,
            workflow_id=workflow_id,
            segment_id=segment_to_run,
            bucket=s3_bucket,
            threshold=AGGREGATOR_SAFE_THRESHOLD  # ê°•í™”ëœ ì„ê³„ê°’
        )
        
        # [Critical] ì‘ë‹µ í˜ì´ë¡œë“œ í¬ê¸° ê²€ì¦ (Step Functions 256KB ì œí•œ)
        response_size = len(json.dumps(final_state, ensure_ascii=False).encode('utf-8')) if final_state else 0
        logger.info(f"[Aggregator] Response payload size: {response_size/1024:.1f}KB "
                   f"(S3: {'YES - ' + output_s3_path if output_s3_path else 'NO'})")
        
        if response_size > 250000:  # 250KB warning
            logger.warning(f"[Aggregator] [Alert] Response payload exceeds 250KB! "
                          f"This may fail Step Functions state transition. Size: {response_size/1024:.1f}KB")
        
        
        # 4. ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ê²°ì •
        # aggregator ë‹¤ìŒì€ ì¼ë°˜ì ìœ¼ë¡œ ì›Œí¬í”Œë¡œìš° ì™„ë£Œì´ì§€ë§Œ,
        # partition_mapì—ì„œ next_segmentë¥¼ í™•ì¸
        partition_map = event.get('partition_map', [])
        total_segments = _safe_get_total_segments(event)
        next_segment = segment_to_run + 1
        
        # [P0 Refactoring] HITP Edge Detection via outgoing_edges (O(1) lookup)
        # Uses partition_map.outgoing_edges instead of scanning workflow_config.edges
        hitp_detected = False
        
        if next_segment < total_segments and partition_map and isinstance(partition_map, list):
            current_seg = partition_map[segment_to_run] if segment_to_run < len(partition_map) else None
            next_seg = partition_map[next_segment] if next_segment < len(partition_map) else None
            
            if current_seg and next_seg:
                edge_info = check_inter_segment_edges(current_seg, next_seg)
                if is_hitp_edge(edge_info):
                    hitp_detected = True
                    logger.info(f"[Aggregator] ğŸš¨ HITP edge detected via outgoing_edges: "
                              f"segment {segment_to_run} â†’ {next_segment}, "
                              f"type={edge_info.get('edge_type')}, target={edge_info.get('target_node')}")
        
        # ì™„ë£Œ ì—¬ë¶€ íŒë‹¨
        is_complete = next_segment >= total_segments
        
        logger.info(f"[Aggregator] [Success] Aggregation complete: "
                   f"{successful_branches}/{len(parallel_results)} branches succeeded, "
                   f"next_segment={next_segment if not is_complete else 'COMPLETE'}, "
                   f"hitp_detected={hitp_detected}")
        
        # [Critical] S3 ì¤‘ê°„ íŒŒì¼ ì •ë¦¬ (Garbage Collection)
        # ê° ë¸Œëœì¹˜ê°€ S3ì— ì €ì¥í•œ ì„ì‹œ ê²°ê³¼ íŒŒì¼ë“¤ì„ ì‚­ì œ
        # ë³‘í•© ì™„ë£Œ í›„ì—ëŠ” ë” ì´ìƒ í•„ìš” ì—†ìŒ (ë¹„ìš© & ê´€ë¦¬ ë¶€í•˜ ê°ì†Œ)
        self._cleanup_branch_intermediate_s3(parallel_results, workflow_id, segment_to_run)
        
        # [P0 Refactoring] S3 offload via helper function (DRY principle)
        response_final_state = prepare_response_with_offload(final_state, output_s3_path)
        # [Critical Fix] Safe check - response_final_state is guaranteed non-None by prepare_response_with_offload
        if response_final_state and response_final_state.get('__s3_offloaded'):
            logger.info(f"[Aggregator] [S3 Offload] Replaced final_state with metadata reference. "
                       f"Original: {response_final_state.get('__original_size_kb', 0):.1f}KB â†’ Response: ~0.2KB")
        
        
        # [Guard] [Fix] Handle HITP Detection
        # If HITP edge detected, pause immediately before proceeding to next segment
        if hitp_detected and not is_complete:
            logger.info(f"[Aggregator] ğŸš¨ Pausing execution due to HITP edge. Next segment: {next_segment}")
            return {
                "status": "PAUSED_FOR_HITP",
                "final_state": response_final_state,
                "final_state_s3_path": output_s3_path,
                "current_state": response_final_state,
                "state_s3_path": output_s3_path,
                "next_segment_to_run": next_segment,
                "new_history_logs": all_history_logs,
                "error_info": None,
                "branches": [],
                "segment_type": "hitp_pause",
                "segment_id": segment_to_run,
                "total_segments": total_segments,
                "aggregator_metadata": {
                    'total_branches': len(parallel_results),
                    'successful_branches': successful_branches,
                    'failed_branches': len(branch_errors),
                    'hitp_edge_detected': True
                }
            }
        
        # [Guard] [Fix] Handle Map Error (Loop Limit Exceeded, etc.)
        # If Map failed, we should PAUSE to allow human intervention or analysis
        if map_error:
            logger.warning(f"[Aggregator] Map Error detected. Forcing PAUSE status. Error: {map_error}")
            return {
                "status": "PAUSE",  # Force PAUSE for Map Errors
                "final_state": response_final_state,
                "final_state_s3_path": output_s3_path,
                "current_state": response_final_state,
                "state_s3_path": output_s3_path,
                "next_segment_to_run": segment_to_run, # Retry same segment or let human decide
                "new_history_logs": all_history_logs,
                "error_info": {
                    "error": "MapExecutionFailed",
                    "cause": map_error,
                    "branch_errors": branch_errors
                },
                "branches": [],
                "segment_type": "aggregator",
                "segment_id": segment_to_run,
                "total_segments": total_segments,
                "aggregator_metadata": {
                    'total_branches': len(parallel_results),
                    'successful_branches': successful_branches,
                    'failed_branches': len(branch_errors),
                    'map_error': True
                }
            }

        # [Guard] [v3.9] Core aggregator response
        # ASL passthrough í•„ë“œëŠ” _finalize_responseì—ì„œ ìë™ ì£¼ì…ë¨
        return {
            # Core execution result
            "status": "COMPLETE" if is_complete else "CONTINUE",
            "final_state": response_final_state,
            "final_state_s3_path": output_s3_path,
            "current_state": response_final_state,  # ğŸ›¡ï¸ [P0 Fix] current_stateë„ S3 í¬ì¸í„°ë¡œ ë³€ê²½ (ì¤‘ë³µ ë°ì´í„° ì œê±°)
            "state_s3_path": output_s3_path,  # ASL í˜¸í™˜ìš© ë³„ì¹­
            "next_segment_to_run": None if is_complete else next_segment,
            "new_history_logs": all_history_logs,
            "error_info": branch_errors if branch_errors else None,
            "branches": [],  # ğŸ›¡ï¸ [P0 Fix] None ëŒ€ì‹  ë¹ˆ ë°°ì—´ë¡œ ë³€ê²½ (ASL Map í˜¸í™˜ì„±)
            "segment_type": "aggregator",
            "segment_id": segment_to_run,
            "total_segments": total_segments,
            
            # Aggregator specific metadata
            "aggregator_metadata": {
                'total_branches': len(parallel_results),
                'successful_branches': successful_branches,
                'failed_branches': len(branch_errors)
            }
        }

    def _trigger_child_workflow(self, event: Dict[str, Any], branch_config: Dict[str, Any], auth_user_id: str, quota_id: str) -> Optional[Dict[str, Any]]:
        """
        Triggers a Child Step Function (Standard Orchestrator) for complex branches.
        "Fire and Forget" pattern to avoid Lambda timeouts.
        """
        try:
            import boto3
            import time
            
            sfn_client = boto3.client('stepfunctions')
            
            # 1. Resolve Orchestrator ARN
            orchestrator_arn = os.environ.get('WORKFLOW_ORCHESTRATOR_ARN')
            if not orchestrator_arn:
                logger.error("WORKFLOW_ORCHESTRATOR_ARN not set. Cannot trigger child workflow.")
                return None

            # 2. Construct Payload (Full 23 fields injection)
            payload = event.copy()
            payload['workflow_config'] = branch_config
            
            parent_workflow_id = payload.get('workflowId') or payload.get('workflow_id', 'unknown')
            parent_idempotency_key = payload.get('idempotency_key', str(time.time()))
            
            # 3. Generate Child Idempotency Key
            branch_id = branch_config.get('id') or f"branch_{int(time.time()*1000)}"
            child_idempotency_key = f"{parent_idempotency_key}_{branch_id}"[:80]
            
            payload['idempotency_key'] = child_idempotency_key
            payload['parent_workflow_id'] = parent_workflow_id
            
            # 4. Start Execution with retry
            safe_exec_name = "".join(c for c in child_idempotency_key if c.isalnum() or c in "-_")
            
            logger.info(f"Triggering Child SFN: {safe_exec_name}")
            
            # [v2.1] Step Functions start_executionì— ì¬ì‹œë„ ì ìš©
            def _start_child_execution():
                return sfn_client.start_execution(
                    stateMachineArn=orchestrator_arn,
                    name=safe_exec_name,
                    input=json.dumps(payload)
                )
            
            if RETRY_UTILS_AVAILABLE:
                response = retry_call(
                    _start_child_execution,
                    max_retries=2,
                    base_delay=0.5,
                    max_delay=5.0
                )
            else:
                response = _start_child_execution()
            
            return {
                "status": "ASYNC_CHILD_WORKFLOW_STARTED",
                "executionArn": response['executionArn'],
                "startDate": response['startDate'].isoformat(),
                "executionName": safe_exec_name
            }
            
        except Exception as e:
            logger.error(f"Failed to trigger child workflow: {e}")
            return None

    # ========================================================================
    # [Guard] [v2.2] Ring Protection: í”„ë¡¬í”„íŠ¸ ë³´ì•ˆ ê²€ì¦
    # ========================================================================
    def _apply_ring_protection(
        self,
        segment_config: Dict[str, Any],
        initial_state: Dict[str, Any],
        segment_id: int,
        workflow_id: str
    ) -> List[Dict[str, Any]]:
        """
        [Guard] Ring Protection: ì„¸ê·¸ë¨¼íŠ¸ ë‚´ í”„ë¡¬í”„íŠ¸ ë³´ì•ˆ ê²€ì¦
        
        ëª¨ë“  LLM ë…¸ë“œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ê²€ì¦í•˜ê³ :
        1. Prompt Injection íŒ¨í„´ íƒì§€
        2. Ring 0 íƒœê·¸ ìœ„ì¡° ì‹œë„ íƒì§€
        3. ìœ„í—˜ ë„êµ¬ ì§ì ‘ ì ‘ê·¼ ì‹œë„ íƒì§€
        
        Args:
            segment_config: ì„¸ê·¸ë¨¼íŠ¸ ì„¤ì •
            initial_state: ì´ˆê¸° ìƒíƒœ
            segment_id: ì„¸ê·¸ë¨¼íŠ¸ ID
            workflow_id: ì›Œí¬í”Œë¡œìš° ID
            
        Returns:
            ë³´ì•ˆ ìœ„ë°˜ ëª©ë¡ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ ì•ˆì „)
        """
        violations = []
        
        if not self.security_guard or not RING_PROTECTION_AVAILABLE:
            return violations
        
        nodes = segment_config.get('nodes', [])
        if not nodes:
            return violations
        
        context = {
            'workflow_id': workflow_id,
            'segment_id': segment_id
        }
        
        for node in nodes:
            # ğŸ›¡ï¸ [v3.8] None defense in nodes iteration
            if node is None or not isinstance(node, dict):
                continue
            node_id = node.get('id', 'unknown')
            node_type = node.get('type', '')
            config = node.get('config', {})
            
            # LLM ë…¸ë“œì˜ í”„ë¡¬í”„íŠ¸ ê²€ì¦
            if node_type in ('llm_chat', 'aiModel', 'llm'):
                prompt = config.get('prompt_content') or config.get('prompt') or ''
                system_prompt = config.get('system_prompt', '')
                
                # í”„ë¡¬í”„íŠ¸ ê²€ì¦
                for prompt_type, prompt_content in [('prompt', prompt), ('system_prompt', system_prompt)]:
                    if prompt_content:
                        result = self.security_guard.validate_prompt(
                            content=prompt_content,
                            ring_level=RingLevel.RING_3_USER,
                            context={**context, 'node_id': node_id, 'prompt_type': prompt_type}
                        )
                        
                        if not result.is_safe:
                            for v in result.violations:
                                violations.append({
                                    'node_id': node_id,
                                    'violation_type': v.violation_type.value,
                                    'severity': v.severity,
                                    'message': v.message,
                                    'should_sigkill': result.should_sigkill
                                })
                            
                            # í”„ë¡¬í”„íŠ¸ ì •í™” (in-place)
                            if result.sanitized_content:
                                if prompt_type == 'prompt':
                                    config['prompt_content'] = result.sanitized_content
                                    config['prompt'] = result.sanitized_content
                                else:
                                    config['system_prompt'] = result.sanitized_content
                                logger.info(f"[Ring Protection] [Guard] Sanitized {prompt_type} in node {node_id}")
            
            # ìœ„í—˜ ë„êµ¬ ì ‘ê·¼ ê²€ì¦
            if node_type in ('tool', 'api_call', 'operator'):
                tool_name = config.get('tool') or config.get('method') or node_type
                allowed, violation = self.security_guard.check_tool_permission(
                    tool_name=tool_name,
                    ring_level=RingLevel.RING_3_USER,
                    context={**context, 'node_id': node_id}
                )
                
                if not allowed and violation:
                    violations.append({
                        'node_id': node_id,
                        'violation_type': violation.violation_type.value,
                        'severity': violation.severity,
                        'message': violation.message,
                        'should_sigkill': False  # ë„êµ¬ ì ‘ê·¼ì€ ê²½ê³ ë§Œ
                    })
        
        if violations:
            logger.warning(f"[Ring Protection] [Warning] {len(violations)} security violations detected in segment {segment_id}")
        
        return violations

    # ========================================================================
    # [Guard] [Kernel Defense] Aggressive Retry Helper
    # ========================================================================
    def _is_retryable_error(self, error: Exception) -> bool:
        """
        ì—ëŸ¬ê°€ ì¬ì‹œë„ ê°€ëŠ¥í•œì§€ íŒë‹¨
        """
        error_str = str(error)
        error_type = type(error).__name__
        
        for pattern in RETRYABLE_ERROR_PATTERNS:
            if pattern in error_str or pattern in error_type:
                return True
        
        # Boto3 ClientError ì²´í¬
        if hasattr(error, 'response'):
            # [Fix] None defense: error.response['Error']ê°€ Noneì¼ ìˆ˜ ìˆìŒ
            error_code = (error.response.get('Error') or {}).get('Code', '')
            for pattern in RETRYABLE_ERROR_PATTERNS:
                if pattern in error_code:
                    return True
        
        return False

    def _execute_with_kernel_retry(
        self,
        segment_config: Dict[str, Any],
        initial_state: Dict[str, Any],
        auth_user_id: str,
        event: Dict[str, Any]
    ) -> Tuple[Dict[str, Any], Optional[Dict[str, Any]]]:
        """
        [Guard] ì»¤ë„ ë ˆë²¨ ê³µê²©ì  ì¬ì‹œë„
        
        Step Functions ë ˆë²¨ ì¬ì‹œë„ ì „ì— Lambda ë‚´ë¶€ì—ì„œ ë¨¼ì € í•´ê²° ì‹œë„.
        - ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬, ì¼ì‹œì  ì„œë¹„ìŠ¤ ì¥ì•  ì‹œ ì¬ì‹œë„
        - ì§€ìˆ˜ ë°±ì˜¤í”„ + ì§€í„° ì ìš©
        
        Returns:
            (result_state, error_info) - ì„±ê³µ ì‹œ error_infoëŠ” None
        """
        last_error = None
        retry_history = []
        
        # Check if this is a parallel branch execution (for token aggregation)
        is_parallel_branch = event.get('branch_config') is not None
        
        for attempt in range(KERNEL_MAX_RETRIES + 1):
            try:
                # ì»¤ë„ ë™ì  ë¶„í•  í™œì„±í™” ì—¬ë¶€ í™•ì¸
                enable_kernel_split = os.environ.get('ENABLE_KERNEL_SPLIT', 'true').lower() == 'true'
                
                if enable_kernel_split and isinstance(segment_config, dict):
                    # [Guard] [Pattern 1] ìë™ ë¶„í•  ì‹¤í–‰
                    result_state = self._execute_with_auto_split(
                        segment_config=segment_config,
                        initial_state=initial_state,
                        auth_user_id=auth_user_id,
                        split_depth=segment_config.get('_split_depth', 0)
                    )
                else:
                    # ê¸°ì¡´ ë¡œì§: ì§ì ‘ ì‹¤í–‰
                    logger.info(f"[v3.27 Debug] Calling run_workflow with segment_config: "
                               f"nodes={len(segment_config.get('nodes', []))}, "
                               f"node_ids={[n.get('id') for n in segment_config.get('nodes', [])]}")
                    result_state = run_workflow(
                        config_json=segment_config,
                        initial_state=initial_state,
                        ddb_table_name=os.environ.get("JOB_TABLE"),
                        user_api_keys={},
                        run_config={"user_id": auth_user_id}
                    )
                    logger.info(f"[v3.27 Debug] run_workflow returned state with keys: "
                               f"{list(result_state.keys() if isinstance(result_state, dict) else [])[: 15]}")
                
                # [Guard] [v3.6] Immortal Kernel: Node Result Normalization
                from src.common.statebag import ensure_state_bag
                result_state = ensure_state_bag(result_state)
                
                # Check for empty result (Context Loss)
                # If run_workflow returns empty, it means we lost all state -> Revert to initial
                if not result_state: 
                     logger.error(f"[Kernel] [Alert] Execution yielded empty/null state! Context lost. Reverting to initial_state. Segment: {segment_config.get('id')}")
                     result_state = ensure_state_bag(initial_state)
                     result_state['__execution_null_recovered'] = True
                     # Note: We prefer keep-alive over crash here.
                
                # [Token Aggregation] ASL Parallel Branch Support
                # For parallel branches, extract and accumulate token usage from the branch execution result
                # This ensures _handle_aggregator can extract tokens from branch final_states
                if is_parallel_branch:
                    try:
                        # Extract accumulated token usage by directly searching result_state for token data
                        total_input_tokens = 0
                        total_output_tokens = 0
                        total_tokens = 0
                        
                        def extract_tokens_from_dict(data, prefix=""):
                            """Recursively extract token values from nested dictionaries"""
                            nonlocal total_input_tokens, total_output_tokens, total_tokens
                            
                            if isinstance(data, dict):
                                for key, value in data.items():
                                    full_key = f"{prefix}.{key}" if prefix else key
                                    
                                    if key in ['input_tokens', 'total_input_tokens'] or 'input_tokens' in key:
                                        if isinstance(value, (int, float)):
                                            total_input_tokens += int(value)
                                    elif key in ['output_tokens', 'total_output_tokens'] or 'output_tokens' in key:
                                        if isinstance(value, (int, float)):
                                            total_output_tokens += int(value)
                                    elif key in ['total_tokens'] or 'total_tokens' in key:
                                        if isinstance(value, (int, float)):
                                            total_tokens += int(value)
                                    elif isinstance(value, dict):
                                        # Recursively search nested dictionaries
                                        extract_tokens_from_dict(value, full_key)
                            
                            elif isinstance(data, (int, float)) and 'token' in prefix.lower():
                                # Handle direct token values in keys containing 'token'
                                if 'input' in prefix.lower():
                                    total_input_tokens += int(data)
                                elif 'output' in prefix.lower():
                                    total_output_tokens += int(data)
                                else:
                                    total_tokens += int(data)
                        
                        # Search through all keys in result_state
                        for key, value in result_state.items():
                            extract_tokens_from_dict({key: value})
                        
                        if total_tokens > 0 or total_input_tokens > 0 or total_output_tokens > 0:
                            # Ensure total_tokens is calculated if not directly found
                            if total_tokens == 0 and (total_input_tokens > 0 or total_output_tokens > 0):
                                total_tokens = total_input_tokens + total_output_tokens
                            
                            # Store accumulated usage in final_state for aggregator to extract
                            result_state['usage'] = {
                                'input_tokens': total_input_tokens,
                                'output_tokens': total_output_tokens,
                                'total_tokens': total_tokens
                            }
                            result_state['total_input_tokens'] = total_input_tokens
                            result_state['total_output_tokens'] = total_output_tokens
                            result_state['total_tokens'] = total_tokens
                            logger.info(f"[Parallel Branch] Accumulated token usage in final_state: {total_tokens} tokens ({total_input_tokens} input + {total_output_tokens} output)")
                        else:
                            logger.debug(f"[Parallel Branch] No token usage found in branch result")
                    except Exception as e:
                        logger.warning(f"[Parallel Branch] Failed to extract token usage: {e}")
                
                # ì„±ê³µ
                if attempt > 0:
                    logger.info(f"[Kernel Retry] [Success] Succeeded after {attempt} retries")
                    # ì¬ì‹œë„ ì´ë ¥ ê¸°ë¡
                    if isinstance(result_state, dict):
                        result_state['__kernel_retry_history'] = retry_history
                
                return result_state, None
                
            except Exception as e:
                last_error = e
                retry_info = {
                    'attempt': attempt + 1,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'timestamp': time.time(),
                    'retryable': self._is_retryable_error(e)
                }
                retry_history.append(retry_info)
                
                if attempt < KERNEL_MAX_RETRIES and self._is_retryable_error(e):
                    # ì§€ìˆ˜ ë°±ì˜¤í”„ + ì§€í„°
                    delay = KERNEL_RETRY_BASE_DELAY * (2 ** attempt) + random.uniform(0, 1)
                    logger.warning(
                        f"[Kernel Retry] [Warning] Attempt {attempt + 1}/{KERNEL_MAX_RETRIES + 1} failed: {e}. "
                        f"Retrying in {delay:.2f}s..."
                    )
                    time.sleep(delay)
                else:
                    # ì¬ì‹œë„ ë¶ˆê°€ëŠ¥ ë˜ëŠ” ìµœëŒ€ íšŸìˆ˜ ë„ë‹¬
                    logger.error(
                        f"[Kernel Retry] âŒ All {attempt + 1} attempts failed. "
                        f"Last error: {e}"
                    )
                    break
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ - ì—ëŸ¬ ì •ë³´ ë°˜í™˜
        error_info = {
            'error': str(last_error),
            'error_type': type(last_error).__name__,
            'retry_attempts': len(retry_history),
            'retry_history': retry_history,
            'retryable': self._is_retryable_error(last_error) if last_error else False
        }
        
        return initial_state, error_info

    def execute_segment(self, event: Dict[str, Any]) -> Dict[str, Any]:
        """
        Main execution logic for a workflow segment with StateHydrator integration.
        """
        # ğŸ›¡ï¸ [v3.4] NULL Event Pre-Check (before hydration)
        if event is None:
            logger.error("ğŸš¨ [CRITICAL] execute_segment received None event!")
            return {
                "status": "FAILED",
                "error": "Event is None before hydration",
                "error_type": "NullEventError",
                "final_state": {},
                "new_history_logs": [],
                "segment_type": "ERROR",
                "total_segments": 1,
                "segment_id": 0
            }
        
        # ğŸ›¡ï¸ [v3.11] Unified State Hydration (Input)
        # Hydrate the event (convert to SmartStateBag) using pre-initialized hydrator
        # This handles "__s3_offloaded" restoration automatically
        event = self.hydrator.hydrate(event)
        
        # ğŸ›¡ï¸ [v3.4] Hydration Result Validation
        # hydrator.hydrate() may return None if S3 load fails or input is malformed
        if event is None or (hasattr(event, 'keys') and len(list(event.keys())) == 0):
            logger.error("ğŸš¨ [CRITICAL] Hydration returned empty/None state!")
            return {
                "status": "FAILED",
                "error": "State hydration failed - empty or None result",
                "error_type": "HydrationFailedError",
                "final_state": {},
                "new_history_logs": [],
                "segment_type": "ERROR",
                "total_segments": 1,
                "segment_id": 0
            }
        
        from src.common.statebag import ensure_state_bag
        
        execution_start_time = time.time()
        
        # Check if this is a parallel branch execution (used for token aggregation and payload optimization)
        is_parallel_branch = event.get('branch_config') is not None
        
        # ====================================================================
        # [Guard] [v2.6 P0 Fix] ëª¨ë“  return ê²½ë¡œì—ì„œ ì‚¬ìš©í•  ë©”íƒ€ë°ì´í„° ì‚¬ì „ ê³„ì‚°
        # Step Functions Choice ìƒíƒœì—ì„œ null ì°¸ì¡°ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•¨
        # ====================================================================
        _total_segments = _safe_get_total_segments(event)
        
        # [Guard] [Critical Fix] explicit None handling for segment_id
        # .get('key') returns None if key exists but value is null, which 'or' propagates
        _seg_id_val = event.get('segment_id')
        if _seg_id_val is None:
            _seg_id_val = event.get('segment_to_run')
        _segment_id = _seg_id_val if _seg_id_val is not None else 0
        
        def _finalize_response(res: Dict[str, Any], force_offload: bool = False) -> Dict[str, Any]:
            """
            ğŸ›¡ï¸ [Guard] [v3.12] StateBag Unified Response Wrapper
            
            Uses universal_sync_core to return a proper StateBag format:
            {
                "state_data": { ...merged state... },
                "next_action": "CONTINUE" | "COMPLETE" | "FAILED" | ...
            }
            
            This ensures:
            1. Single Source of Truth: ExecuteSegment returns StateBag directly
            2. No separate SyncStateData call needed for state mutation
            3. ASL always receives consistent {state_data, next_action} format
            """
            # 1. Validation & Safety Defaults
            if not isinstance(res, dict):
                logger.error(f"[Alert] [Kernel] Invalid response type: {type(res)}! Emergency fallback.")
                res = {"status": "FAILED", "error_info": {"error": "KernelTypeMismatch"}}
            
            res.setdefault('status', 'FAILED')
            res.setdefault('segment_type', 'normal')
            res.setdefault('new_history_logs', [])
            res.setdefault('final_state', {})
            res.setdefault('total_segments', _total_segments)
            res.setdefault('segment_id', _segment_id)
            
            # 2. Extract execution result metadata
            original_final_state = res.get('final_state', {})
            if not isinstance(original_final_state, dict): 
                original_final_state = {}
            
            # Inject guardrail metadata
            gv = original_final_state.get('guardrail_verified', False)
            bca = original_final_state.get('batch_count_actual', 1)
            sm = original_final_state.get('scheduling_metadata', {})
            
            original_final_state.update({
                'guardrail_verified': gv,
                'batch_count_actual': bca,
                'scheduling_metadata': sm,
                'state_size_threshold': self.threshold,
            })
            res['final_state'] = original_final_state
            
            # 3. ğŸ¯ [v3.13] Kernel Protocol - The Great Seal
            # Uses seal_state_bag for unified Lambda â†” ASL communication
            if KERNEL_PROTOCOL_AVAILABLE:
                # Build base_state from current event using open_state_bag
                base_state = open_state_bag(event)
                if not isinstance(base_state, dict):
                    base_state = {}
                
                # ğŸ” [Debug] Log loop_counter value for troubleshooting
                logger.info(f"[v3.14 Debug] base_state.loop_counter={base_state.get('loop_counter')}, "
                           f"max_loop_iterations={base_state.get('max_loop_iterations')}, "
                           f"event_keys={list(event.keys())[:10] if isinstance(event, dict) else 'N/A'}")
                
                # Build execution_result for sealing
                status = res.get('status', 'CONTINUE')
                execution_result = {
                    'final_state': original_final_state,
                    'new_history_logs': res.get('new_history_logs', []),
                    'status': status,
                    'segment_id': _segment_id,
                    'segment_type': res.get('segment_type', 'normal'),
                    'next_segment_to_run': res.get('next_segment_to_run'),
                    'error_info': res.get('error_info'),
                    'branches': res.get('branches'),
                    'execution_time': res.get('execution_time'),
                    'kernel_actions': res.get('kernel_actions'),
                    'total_segments': _total_segments,
                    # Token metadata for guardrails
                    'total_tokens': res.get('total_tokens') or original_final_state.get('total_tokens'),
                    'total_input_tokens': res.get('total_input_tokens') or original_final_state.get('total_input_tokens'),
                    'total_output_tokens': res.get('total_output_tokens') or original_final_state.get('total_output_tokens'),
                }
                
                # Context for seal_state_bag
                seal_context = {
                    'segment_id': _segment_id,
                    'force_offload': force_offload,
                    'is_parallel_branch': is_parallel_branch,
                }
                
                # ğŸ¯ [v3.13] Use seal_state_bag - Unified Protocol
                # Returns: { state_data: {...}, next_action: "..." }
                # ASL ResultSelector wraps this into $.state_data.bag
                sealed_result = seal_state_bag(
                    base_state=base_state,
                    result_delta={'execution_result': execution_result},
                    action='sync',
                    context=seal_context
                )
                
                logger.info(f"[v3.13] ğŸ¯ Kernel Protocol: sealed response - "
                           f"next_action={sealed_result.get('next_action')}, "
                           f"state_size={len(json.dumps(sealed_result.get('state_data', {}), default=str))//1024}KB")
                
                return sealed_result
            
            # 3b. USC Fallback (if kernel_protocol not available but USC is)
            if UNIVERSAL_SYNC_CORE_AVAILABLE:
                base_state = event.get('state_data', {})
                if isinstance(base_state, dict) and 'bag' in base_state:
                    base_state = base_state.get('bag', {})
                if not isinstance(base_state, dict):
                    base_state = {}
                
                status = res.get('status', 'CONTINUE')
                execution_result = {
                    'final_state': original_final_state,
                    'new_history_logs': res.get('new_history_logs', []),
                    'status': status,
                    'segment_id': _segment_id,
                    'segment_type': res.get('segment_type', 'normal'),
                    'next_segment_to_run': res.get('next_segment_to_run'),
                    'error_info': res.get('error_info'),
                    'branches': res.get('branches'),
                    'execution_time': res.get('execution_time'),
                    'kernel_actions': res.get('kernel_actions'),
                    'total_segments': _total_segments,
                    'total_tokens': res.get('total_tokens') or original_final_state.get('total_tokens'),
                    'total_input_tokens': res.get('total_input_tokens') or original_final_state.get('total_input_tokens'),
                    'total_output_tokens': res.get('total_output_tokens') or original_final_state.get('total_output_tokens'),
                }
                
                usc_result = universal_sync_core(
                    base_state=base_state,
                    new_result={'execution_result': execution_result},
                    context={'action': 'sync', 'segment_id': _segment_id}
                )
                
                logger.warning("[v3.13] âš ï¸ USC fallback - kernel_protocol not available")
                return usc_result
            
            # 4. Fallback: Legacy mode (if universal_sync_core not available)
            logger.warning("[v3.12] âš ï¸ Fallback to legacy mode - universal_sync_core not available")
            
            # Use StateHydrator to Dehydrate (legacy behavior)
            bag = SmartStateBag(res, hydrator=self.hydrator)
            
            force_fields = set()
            force_fields.add('final_state')
            
            if force_offload or is_parallel_branch:
                force_fields.add('branches')
                force_fields.add('execution_batches')
                if is_parallel_branch:
                    force_fields.add('workflow_config')
                    force_fields.add('partition_map')

            owner_id = event.get('ownerId') or event.get('owner_id', 'unknown')
            workflow_id = event.get('workflowId') or event.get('workflow_id', 'unknown')
            execution_id = event.get('execution_id', 'unknown')

            payload = self.hydrator.dehydrate(
                state=bag,
                owner_id=owner_id,
                workflow_id=workflow_id,
                execution_id=execution_id,
                segment_id=_segment_id,
                force_offload_fields=force_fields,
                return_delta=False
            )
            
            # Restore Critical Metadata to Top Level
            keys_to_preserve = [
                'total_tokens', 'total_input_tokens', 'total_output_tokens',
                'guardrail_verified', 'batch_count_actual', 'scheduling_metadata',
                'usage', 'branch_token_details',
                'inner_partition_map', 'branch_id', 'next_segment_to_run'
            ]
            for k in keys_to_preserve:
                if k in original_final_state and k not in payload:
                    payload[k] = original_final_state[k]
            
            # Generate S3 Path Aliases for ASL Compatibility
            for field in ['final_state', 'current_state', 'workflow_config', 'partition_map', 'segment_manifest', 'branches']:
                val = payload.get(field)
                if isinstance(val, dict) and (val.get('__s3_pointer__') or val.get('bucket')):
                    bucket = val.get('bucket')
                    key = val.get('key')
                    if bucket and key:
                        payload[f"{field}_s3_path"] = f"s3://{bucket}/{key}"
            
            if payload.get('final_state_s3_path'):
                payload['state_s3_path'] = payload['final_state_s3_path']

            # Wrap in StateBag format for consistency
            return {
                'state_data': payload,
                'next_action': res.get('status', 'CONTINUE')
            }
        
        # ====================================================================
        # [Guard] [2ë‹¨ê³„] Pre-Execution Check: ë™ì‹œì„± ë° ì˜ˆì‚° ì²´í¬
        # ====================================================================
        if CONCURRENCY_CONTROLLER_AVAILABLE and self.concurrency_controller:
            pre_check = self.concurrency_controller.pre_execution_check()
            # ğŸ›¡ï¸ [P0 Fix] Null Guard for pre_check return value
            if pre_check and not pre_check.get('can_proceed', True):
                logger.error(f"[Kernel] âŒ Pre-execution check failed: {pre_check.get('reason')}")
                return _finalize_response({
                    "status": "HALTED",
                    "final_state": {},
                    "final_state_s3_path": None,
                    "next_segment_to_run": None,
                    "new_history_logs": [],
                    "error_info": {
                        "error": pre_check.get('reason', 'Unknown'),
                        "error_type": "ConcurrencyControlHalt",
                        "budget_status": pre_check.get('budget_status')
                    },
                    "branches": None,
                    "segment_type": "halted",
                    "kernel_stats": self.concurrency_controller.get_comprehensive_stats()
                })
            
            # ë¡œë“œ ë ˆë²¨ ë¡œê¹…
            snapshot = pre_check.get('snapshot')
            if snapshot and snapshot.load_level.value in ['high', 'critical']:
                logger.warning(f"[Kernel] [Warning] High load detected: {snapshot.load_level.value} "
                             f"({snapshot.active_executions}/{snapshot.reserved_concurrency})")
        
        # [Fix] ì´ë²¤íŠ¸ì—ì„œ MOCK_MODEë¥¼ ì½ì–´ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì£¼ì…
        # MOCK_MODE=falseì¸ ê²½ìš°ì—ë„ ê°•ì œë¡œ í™˜ê²½ë³€ìˆ˜ë¥¼ ë®ì–´ì¨ì„œ ì‹œë®¬ë ˆì´í„°ê°€ ì‹¤ì œ LLM í˜¸ì¶œì„ ê°€ëŠ¥í•˜ê²Œ í•¨
        # [2026-01-26] ê¸°ë³¸ê°’ì„ falseë¡œ ë³€ê²½ (ì‹¤ì œ LLM í˜¸ì¶œ ëª¨ë“œ)
        event_mock_mode = str(event.get('MOCK_MODE', 'false')).lower()
        if event_mock_mode in ('true', '1', 'yes', 'on'):
            os.environ['MOCK_MODE'] = 'true'
            logger.info("ğŸ§ª MOCK_MODE enabled from event payload")
        else:
            os.environ['MOCK_MODE'] = 'false'
            logger.info("ğŸ§ª MOCK_MODE disabled (default: false, Simulator Mode)")
        
        # ====================================================================
        # [Parallel] [Aggregator] ë³‘ë ¬ ê²°ê³¼ ì§‘ê³„ ì²˜ë¦¬
        # ASLì˜ AggregateParallelResultsì—ì„œ í˜¸ì¶œë¨
        # ====================================================================
        segment_type_param = event.get('segment_type')
        if segment_type_param == 'aggregator':
            return _finalize_response(self._handle_aggregator(event), force_offload=True)
        
        # 0. Check for Branch Offloading
        branch_config = event.get('branch_config')
        if branch_config:
            force_child = os.environ.get('FORCE_CHILD_WORKFLOW', 'false').lower() == 'true'
            node_count = len(branch_config.get('nodes', [])) if isinstance(branch_config.get('nodes'), list) else 0
            # ğŸ›¡ï¸ [P0 Fix] nì´ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°©ì–´ ì½”ë“œ ì¶”ê°€
            has_hitp = branch_config.get('hitp', False) or any(
                n.get('hitp') for n in branch_config.get('nodes', []) if n and isinstance(n, dict)
            )
            
            should_offload = force_child or node_count > 20 or has_hitp
            
            if should_offload:
                auth_user_id = event.get('ownerId') or event.get('owner_id')
                quota_id = event.get('quota_reservation_id')
                
                child_result = self._trigger_child_workflow(event, branch_config, auth_user_id, quota_id)
                if child_result:
                    # [Fix] Ensure child workflow result is also wrapped (though small) for consistency
                    return _finalize_response(child_result, force_offload=False)

        # 1. State Bag Normalization
        # [Moved] executed AFTER loading state to prevent data loss
        # normalize_inplace(event, remove_state_data=True) 

        
        # 2. Extract Context
        auth_user_id = event.get('ownerId') or event.get('owner_id') or event.get('user_id')
        workflow_id = event.get('workflowId') or event.get('workflow_id')
        # ğŸš€ [Hybrid Mode] Support both segment_id (hybrid) and segment_to_run (legacy)
        # [Guard] [Critical Fix] explicit None checking to prevent TypeError in comparisons
        _seg_id_cand = event.get('segment_id')
        if _seg_id_cand is None:
            _seg_id_cand = event.get('segment_to_run')
        segment_id = _seg_id_cand if _seg_id_cand is not None else 0
        
        # [Critical Fix] S3 bucket for large payload offloading - ensure non-empty string
        s3_bucket_raw = os.environ.get("S3_BUCKET") or os.environ.get("SKELETON_S3_BUCKET") or ""
        s3_bucket = s3_bucket_raw.strip() if s3_bucket_raw else None
        
        if not s3_bucket:
            logger.error("[Alert] [CRITICAL] S3_BUCKET/SKELETON_S3_BUCKET environment variable is NOT SET or EMPTY! "
                        f"S3_BUCKET='{os.environ.get('S3_BUCKET')}', "
                        f"SKELETON_S3_BUCKET='{os.environ.get('SKELETON_S3_BUCKET')}'. "
                        "Large payloads (>256KB) will FAIL.")
        else:
            logger.debug(f"S3 bucket for state offloading: {s3_bucket}")
        
        # ====================================================================
        # 3. Load State (Inline or S3) - Keep as local variable only
        # âš ï¸ DO NOT add to event to avoid 256KB limit
        # ====================================================================
        # ğŸ›¡ï¸ [v3.14] Kernel Protocol - Use open_state_bag for unified extraction
        # ASL passes $.state_data.bag (bag contents) directly as Payload
        # open_state_bag handles all cases: v3 ASL, legacy, direct invocation
        # ====================================================================
        state_s3_path = event.get('state_s3_path')
        
        # ğŸ’ [v3.14] Use Kernel Protocol for state extraction
        if KERNEL_PROTOCOL_AVAILABLE:
            # open_state_bag handles:
            # 1. event.state_data.bag (if ASL wraps)
            # 2. event.state_data (if flat)
            # 3. event itself (if v3 ASL passes $.state_data.bag directly)
            initial_state = open_state_bag(event)
            resolved_source = "kernel_protocol.open_state_bag"
            logger.debug(f"[v3.14 Kernel Protocol] Used open_state_bag, keys={list(initial_state.keys())[:5] if initial_state else []}")
        else:
            # Legacy fallback
            state_data = event.get('state_data', {})
            
            # ğŸ” [v3.5 None Trace] Entry point tracing
            _trace_none_access('state_data', 'event', event.get('state_data'), 
                               context=event, caller='ExecuteSegment:Entry')
            
            if not isinstance(state_data, dict):
                state_data = {}
            
            # ğŸ›¡ï¸ [v3.4] Search Priority (Deep Lookup):
            bag_in_state_data = state_data.get('bag') if isinstance(state_data.get('bag'), dict) else None
            
            # ğŸ” [v3.5 None Trace] Bag lookup tracing
            _trace_none_access('bag', 'state_data', bag_in_state_data, 
                               context=state_data, caller='ExecuteSegment:BagLookup')
            
            # Compute each candidate for tracing
            candidate_1 = bag_in_state_data.get('current_state') if bag_in_state_data else None
            candidate_2 = state_data.get('current_state') if state_data else None
            candidate_3 = event.get('current_state')
            candidate_4 = event.get('state')
            # ğŸš¨ [v3.14 FIX] If event IS the bag contents (v3 ASL: Payload.$=$.state_data.bag)
            candidate_5 = event if event and not state_data else state_data
            
            initial_state = (
                candidate_1 or candidate_2 or candidate_3 or candidate_4 or candidate_5 or {}
            )
            
            resolved_source = (
                "bag.current_state" if candidate_1 else
                "state_data.current_state" if candidate_2 else
                "event.current_state" if candidate_3 else
                "event.state" if candidate_4 else
                "event_as_bag" if candidate_5 == event else
                "state_data_as_bag" if candidate_5 else
                "empty_fallback"
            )
        if not initial_state or (isinstance(initial_state, dict) and len(initial_state) == 0):
            _trace_none_access('initial_state', 'combined_lookup', None, 
                               context=event, caller='ExecuteSegment:FinalStateResolve')
        
        logger.debug(f"[Deep Lookup] initial_state source: {resolved_source}, "
                    f"initial_state keys={list(initial_state.keys())[:5] if isinstance(initial_state, dict) else 'N/A'}")
        
        # [Critical Fix] S3 Offload Recovery: check __s3_offloaded flag
        # If previous segment did S3 offload, only metadata is included
        # â†’ Full state needs to be restored from S3
        if isinstance(initial_state, dict) and initial_state.get('__s3_offloaded') is True:
            offloaded_s3_path = initial_state.get('__s3_path')
            if offloaded_s3_path:
                logger.info(f"[S3 Offload Recovery] Detected offloaded state. Restoring from S3: {offloaded_s3_path}")
                try:
                    initial_state = self.state_manager.download_state_from_s3(offloaded_s3_path)
                    logger.info(f"[S3 Offload Recovery] Successfully restored state from S3 "
                               f"({len(json.dumps(initial_state, ensure_ascii=False).encode('utf-8'))/1024:.1f}KB)")
                except Exception as e:
                    logger.error(f"[S3 Offload Recovery] Failed to restore state from S3: {e}")
                    return _finalize_response({
                        "status": "FAILED",
                        "error_info": {
                            "error": f"S3 State Recovery Failed: {str(e)}",
                            "error_type": "S3RecoveryError",
                            "s3_path": offloaded_s3_path
                        }
                    })
            else:
                logger.warning("[S3 Offload Recovery] __s3_offloaded=True but no __s3_path found!")
        
        if state_s3_path:
            initial_state = self.state_manager.download_state_from_s3(state_s3_path)
            # [Critical Fix] Double-check for S3 offload in downloaded state
            from src.common.statebag import ensure_state_bag
            initial_state = ensure_state_bag(initial_state)
            if isinstance(initial_state, dict) and initial_state.get('__s3_offloaded') is True:
                offloaded_path = initial_state.get('__s3_path')
                if offloaded_path:
                    logger.info(f"[S3 Offload Recovery] Recursive hydration from: {offloaded_path}")
                    try:
                        initial_state = self.state_manager.download_state_from_s3(offloaded_path)
                        initial_state = ensure_state_bag(initial_state)
                    except Exception as e:
                        logger.error(f"[S3 Offload Recovery] Recursive hydration failed: {e}")
                        # Continue with wrapper (will likely fail downstream but better than crash)
        
        # [Guard] [v3.6 P0] Data Ownership Defense: enforce StateBag
        # StateBag guarantees Safe Access (get(key) != None)
        from src.common.statebag import ensure_state_bag
        initial_state = ensure_state_bag(initial_state)
        
        # [FIX] Propagate MOCK_MODE from payload to state (payload always wins)
        # LLM Simulator passes MOCK_MODE in payload root, but llm_chat_runner reads from state
        # CRITICAL: Payload takes precedence over state to allow runtime override
        if 'MOCK_MODE' in event:
            old_value = initial_state.get('MOCK_MODE', 'not set')
            initial_state['MOCK_MODE'] = event['MOCK_MODE']
            logger.info(f"ğŸ”„ MOCK_MODE override: {old_value} â†’ {event['MOCK_MODE']} (from payload)")

        # ====================================================================
        # [Hydration] [v3.10] Unified State Bag - Single Source of Truth
        # ====================================================================
        # [v3.6] workflow_configëŠ” StateBagì—ì„œ ì§ì ‘ ì¡°íšŒ
        # bag ì „ì²´ê°€ hydrationë˜ë©´ workflow_configë„ í¬í•¨ë¨ (ë³„ë„ S3 ì¡°íšŒ ë¶ˆí•„ìš”)
        # ====================================================================
        
        # ====================================================================
        # [v3.6] Extract ALL bag data BEFORE normalize_inplace removes state_data
        # StateBag as Single Source of Truth - ëª¨ë“  ë°ì´í„°ëŠ” bagì—ì„œ ê°€ì ¸ì˜´
        # ë¡œì»¬ ë³€ìˆ˜ë¡œë§Œ ìœ ì§€ (eventì— ì €ì¥ X - StateBag ì˜¤ì—¼ ë°©ì§€)
        # ====================================================================
        workflow_config = _safe_get_from_bag(event, 'workflow_config')
        partition_map = _safe_get_from_bag(event, 'partition_map')
        partition_map_s3_path = _safe_get_from_bag(event, 'partition_map_s3_path')
        execution_mode = _safe_get_from_bag(event, 'execution_mode')
        distributed_mode = _safe_get_from_bag(event, 'distributed_mode')
        
        # [Fix] [v3.10] Normalize Event AFTER state extraction but BEFORE processing
        # Remove potentially huge state_data from event to save memory
        normalize_inplace(event, remove_state_data=True)

        # 4. Resolve Segment Config - Single Source of Truth
        # workflow_configëŠ” bag ìµœìƒìœ„ì— ìˆìŒ (ìœ„ì—ì„œ ì´ë¯¸ ì¶”ì¶œë¨)
        
        # ğŸ‘‰ [Critical Fix] Branch Execution: partition_map fallback from branch_config
        # ASLì˜ ProcessParallelSegmentsì—ì„œ branch_configì— ì „ì²´ ë¸Œëœì¹˜ ì •ë³´ê°€ ì „ë‹¬ë¨
        # partition_mapì´ nullì´ë©´ branch_config.partition_mapì„ ì‚¬ìš©
        branch_config = event.get('branch_config')
        if not partition_map and branch_config and isinstance(branch_config, dict):
            branch_partition_map = branch_config.get('partition_map')
            if branch_partition_map:
                logger.info(f"[Branch Execution] Using partition_map from branch_config "
                           f"(branch_id: {branch_config.get('branch_id', 'unknown')}, "
                           f"segments: {len(branch_partition_map)})")
                partition_map = branch_partition_map
        
        # ğŸš€ğŸš€ [Hybrid Mode] Direct segment_config support for MAP_REDUCE/BATCHED modes
        direct_segment_config = event.get('segment_config')
        # execution_modeëŠ” ìœ„ì—ì„œ ì´ë¯¸ ì¶”ì¶œë¨
        
        if direct_segment_config and execution_mode in ('MAP_REDUCE', 'BATCHED'):
            logger.info(f"[Hybrid Mode] Using direct segment_config for {execution_mode} mode")
            segment_config = direct_segment_config
        else:
            # [Critical Fix] Support S3 Offloaded Partition Map with retry
            if not partition_map and partition_map_s3_path:
                try:
                    import boto3
                    s3 = boto3.client('s3')
                    bucket_name = partition_map_s3_path.replace("s3://", "").split("/")[0]
                    key_name = "/".join(partition_map_s3_path.replace("s3://", "").split("/")[1:])
                    
                    logger.info(f"Loading partition_map from S3: {partition_map_s3_path}")
                    
                    # [v2.1] S3 get_objectì— ì¬ì‹œë„ ì ìš©
                    def _get_partition_map():
                        obj = s3.get_object(Bucket=bucket_name, Key=key_name)
                        content = obj['Body'].read().decode('utf-8')
                        return self._safe_json_load(content)  # ğŸ›¡ï¸ Use safe loader
                    
                    if RETRY_UTILS_AVAILABLE:
                        partition_map = retry_call(
                            _get_partition_map,
                            max_retries=2,
                            base_delay=0.5,
                            max_delay=5.0
                        )
                    else:
                        partition_map = _get_partition_map()
                        
                except Exception as e:
                    logger.error(f"Failed to load partition_map from S3 after retries: {e}")
                    # Fallback to dynamic partitioning (handled in _resolve_segment_config)
            
            segment_config = self._resolve_segment_config(workflow_config, partition_map, segment_id)

        # [Option A] ì„¸ê·¸ë¨¼íŠ¸ config ì •ê·œí™” - None ê°’ì„ ë¹ˆ dict/listë¡œ ë³€í™˜
        segment_config = _normalize_segment_config(segment_config)

        # [Guard] [v2.6 P0 Fix] 'code' íƒ€ì… ì˜¤ì—¼ ë°©ì§€ Self-Healing
        # ìƒìœ„ ëŒë‹¤(PartitionService ë“±)ì—ì„œ ì˜ëª»ëœ íƒ€ì…ì´ ì£¼ì…ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëŸ°íƒ€ì„ êµì •
        if segment_config and isinstance(segment_config, dict):
            for node in segment_config.get('nodes', []):
                # ğŸ›¡ï¸ [v3.8] None defense
                if node is None or not isinstance(node, dict):
                    continue
                if node.get('type') == 'code':
                    logger.warning(
                        f"[Guard] [Self-Healing] Aliasing 'code' to 'operator' for node {node.get('id')}. "
                        f"This indicates upstream data mutation - investigate PartitionService."
                    )
                    node['type'] = 'operator'
        
        # [Guard] [Critical Fix] segment_configì´ Noneì´ê±°ë‚˜ error íƒ€ì…ì´ë©´ ì¡°ê¸° ì—ëŸ¬ ë°˜í™˜
        if not segment_config or (isinstance(segment_config, dict) and segment_config.get('type') == 'error'):
            error_msg = segment_config.get('error', 'segment_config is None') if isinstance(segment_config, dict) else 'segment_config is None'
            logger.error(f"[Alert] [Critical] segment_config resolution failed: {error_msg}")
            return _finalize_response({
                "status": "FAILED",
                "error": error_msg,
                "error_type": "ConfigurationError",
                "final_state": initial_state,
                "final_state_s3_path": None,
                "next_segment_to_run": None,
                "new_history_logs": [],
                "error_info": {
                    "error": error_msg,
                    "error_type": "ConfigurationError",
                    "workflow_config_present": workflow_config is not None,
                    "partition_map_present": partition_map is not None
                },
                "branches": None,
                "segment_type": "ERROR"
            })
        
        # [Critical Fix] parallel_group íƒ€ì… ì„¸ê·¸ë¨¼íŠ¸ëŠ” ë°”ë¡œ PARALLEL_GROUP status ë°˜í™˜
        # ASLì˜ ProcessParallelSegmentsê°€ branchesë¥¼ ë°›ì•„ì„œ Mapìœ¼ë¡œ ë³‘ë ¬ ì‹¤í–‰í•¨
        # [Parallel] [Pattern 3] ë³‘ë ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©
        segment_type = segment_config.get('type') if isinstance(segment_config, dict) else None
        
        # [Fix] HITP Segment Type Check (Priority: segment type > edge type)
        # If segment itself is marked as 'hitp', pause immediately
        if segment_type == 'hitp':
            logger.info(f"[Kernel] ğŸš¨ HITP segment {segment_id} detected. Pausing for human approval.")
            return _finalize_response({
                "status": "PAUSED_FOR_HITP",
                "final_state": mask_pii_in_state(initial_state),
                "final_state_s3_path": None,
                "next_segment_to_run": segment_id + 1,
                "new_history_logs": [],
                "error_info": None,
                "branches": None,
                "segment_type": "hitp"
            })
        
        # [Fix] Aggregator Interception (Delayed Check)
        # execute_segment ì‹œì‘ ì‹œì ì—ëŠ” segment_type íŒŒë¼ë¯¸í„°ê°€ ì—†ì„ ìˆ˜ ìˆìŒ (partition_mapì—ì„œ resolveëœ ê²½ìš°)
        # ë”°ë¼ì„œ ì—¬ê¸°ì„œ resolveëœ segment_configë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë²ˆ ë” ì²´í¬í•´ì•¼ í•¨
        if segment_type == 'aggregator':
            logger.info(f"[Kernel] ğŸ§© Aggregator segment {segment_id} detected (Resolved). Delegating to _handle_aggregator.")
            return _finalize_response(self._handle_aggregator(event), force_offload=True)

        if segment_type == 'parallel_group':
            branches = segment_config.get('branches', [])
            logger.info(f"[Parallel] Parallel group detected with {len(branches)} branches")
            
            # [Guard] [Critical Fix] Define helper for offloading within parallel block
            def _finalize_with_offload(response_payload: Dict[str, Any]) -> Dict[str, Any]:
                """Helper to ensure S3 offloading is checked before finalizing response."""
                
                # 1. Calculate approximate size
                try:
                    payload_json = json.dumps(response_payload, ensure_ascii=False)
                    payload_size = len(payload_json.encode('utf-8'))
                except:
                    payload_size = 0 # Fallback
                
                # 2. Extract state to potentially offload
                current_final_state = response_payload.get('final_state')
                
                # 3. Offload Logic: If state exists AND (forced offload OR payload > 100KB safe limit)
                # Note: We use 100KB as a safety margin for the 256KB Step Functions limit,
                # considering overhead from packaging, encoding, and ASL wrapper fields.
                # Previous 128KB threshold was too close to the limit.
                should_offload = current_final_state and (payload_size > 100 * 1024)
                
                if should_offload:
                    if not s3_bucket:
                        logger.error("[Parallel] [Critical] S3 Bucket not defined! Cannot offload large payload.")
                        # Proceeding might fail, but we try pruning metadata next
                    else:
                        logger.info(f"[Parallel] Payload size {payload_size} bytes exceeds safety limit. Offloading final_state.")
                        offloaded_state, s3_path = self.state_manager.handle_state_storage(
                            state=current_final_state,
                            auth_user_id=auth_user_id,
                            workflow_id=workflow_id,
                            segment_id=segment_id,
                            bucket=s3_bucket,
                            threshold=0 # Force offload
                        )
                        
                        # [Critical Fix] S3 offload ì‹œ final_state ë¹„ìš°ê¸°
                        if s3_path:
                            offloaded_state = {
                                "__s3_offloaded": True,
                                "__s3_path": s3_path,
                                "__original_size_kb": len(json.dumps(current_final_state, ensure_ascii=False).encode('utf-8')) / 1024 if current_final_state else 0
                            }
                            logger.info(f"[Parallel] Offloaded state to S3: {s3_path}. Response: ~0.2KB")
                        
                        # Update response with offloaded result
                        response_payload['final_state'] = offloaded_state
                        response_payload['final_state_s3_path'] = s3_path
                        response_payload['state_s3_path'] = s3_path # Alias for ASL
                
                # 4. Secondary Pruning: If still too large, prune non-essential metadata
                # Recalculate size
                try:
                    payload_json = json.dumps(response_payload, ensure_ascii=False)
                    payload_size = len(payload_json.encode('utf-8'))
                except:
                    pass
                    
                if payload_size > 200 * 1024: # Still > 200KB
                    logger.warning(f"[Parallel] [Alert] Payload still huge ({payload_size} bytes) after offload. Pruning metadata.")
                    # Prune history logs
                    response_payload['new_history_logs'] = []
                    # Prune metadata details if scheduling is present
                    if 'scheduling_metadata' in response_payload:
                        response_payload['scheduling_metadata'] = {
                            'note': 'Pruned due to size limit',
                            'batch_count': response_payload['scheduling_metadata'].get('batch_count'),
                            'strategy': response_payload['scheduling_metadata'].get('strategy')
                        }
                
                return _finalize_response(response_payload, force_offload=True)

            # [Guard] [Critical Fix] HITP edge ìš°ì„  ì²´í¬ (ë‹¨ì¼ ë¸Œëœì¹˜ ìµœì í™” ì „)
            # HITP edgeê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ PAUSED_FOR_HITPë¡œ ì²˜ë¦¬
            hitp_edge_types = {"hitp", "human_in_the_loop", "pause"}
            has_hitp_edge = False
            
            for branch in branches:
                if isinstance(branch, dict):
                    branch_nodes = branch.get('nodes', [])
                    for node in branch_nodes:
                        if isinstance(node, dict):
                            # ë…¸ë“œì˜ incoming edges ì²´í¬
                            in_edges = node.get('in_edges', [])
                            if any(e.get('type') in hitp_edge_types for e in in_edges if isinstance(e, dict)):
                                has_hitp_edge = True
                                break
                    if has_hitp_edge:
                        break
            
            if has_hitp_edge:
                logger.info(f"[Kernel] ğŸš¨ HITP edge detected in segment {segment_id}. Pausing for human approval.")
                return _finalize_with_offload({
                    "status": "PAUSED_FOR_HITP",
                    "final_state": mask_pii_in_state(initial_state),
                    "final_state_s3_path": None,
                    "next_segment_to_run": segment_id + 1,
                    "new_history_logs": [],
                    "error_info": None,
                    "branches": branches,  # HITP ì´í›„ ì‹¤í–‰í•  ë¸Œëœì¹˜ ì •ë³´ ìœ ì§€
                    "segment_type": "hitp_pause"
                })
            
            # [Guard] [Critical Fix] ë‹¨ì¼ ë¸Œëœì¹˜ + ë‚´ë¶€ partition_map ì¼€ì´ìŠ¤ ì²˜ë¦¬
            # ì´ ê²½ìš° ì‹¤ì œ ë³‘ë ¬ ì‹¤í–‰ì´ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ë¸Œëœì¹˜ ë‚´ë¶€ì˜ ì²« ë²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸ ì§ì ‘ ì‹¤í–‰
            if len(branches) == 1:
                single_branch = branches[0]
                branch_partition_map = single_branch.get('partition_map', [])
                
                if branch_partition_map:
                    logger.info(f"[Kernel] ğŸ“Œ Single branch with internal partition_map detected. "
                               f"Executing {len(branch_partition_map)} segments sequentially instead of parallel.")
                    
                    # ë¸Œëœì¹˜ ë‚´ë¶€ì˜ ì²« ë²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ segment_configë¡œ ì‚¬ìš©
                    first_inner_segment = branch_partition_map[0] if branch_partition_map else None
                    
                    if first_inner_segment:
                        # [System] ë‚´ë¶€ partition_mapì„ ìƒˆë¡œìš´ ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        # ìƒíƒœë¥¼ ìœ ì§€í•˜ë©´ì„œ ë‚´ë¶€ ì„¸ê·¸ë¨¼íŠ¸ ì²´ì¸ ìˆœì°¨ ì‹¤í–‰
                        return _finalize_with_offload({
                            "status": "SEQUENTIAL_BRANCH",
                            "final_state": mask_pii_in_state(initial_state),
                            "final_state_s3_path": None,
                            "next_segment_to_run": segment_id + 1,
                            "new_history_logs": [],
                            "error_info": None,
                            "branches": None,  # ë³‘ë ¬ ì‹¤í–‰ ì•ˆí•¨
                            "segment_type": "sequential_branch",
                            # [Guard] ë‚´ë¶€ partition_map ì •ë³´ ì „ë‹¬ (ASLì´ ìˆœì°¨ ì²˜ë¦¬í•˜ë„ë¡)
                            "inner_partition_map": branch_partition_map,
                            "inner_segment_count": len(branch_partition_map),
                            "branch_id": single_branch.get('branch_id', 'B0'),
                            "scheduling_metadata": {
                                'strategy': 'SEQUENTIAL_SINGLE_BRANCH',
                                'total_inner_segments': len(branch_partition_map),
                                'reason': 'Single branch optimization - parallel execution skipped'
                            }
                        })
            
            # [System] ë¹ˆ ë¸Œëœì¹˜ ë˜ëŠ” ë…¸ë“œê°€ ì—†ëŠ” ë¸Œëœì¹˜ í•„í„°ë§
            valid_branches = []
            for branch in branches:
                # ğŸ›¡ï¸ [P0 Fix] None ë˜ëŠ” dictê°€ ì•„ë‹Œ ë¸Œëœì¹˜ ê°ì²´ ë°©ì–´
                if not branch or not isinstance(branch, dict):
                    logger.warning(f"[Kernel] [Warning] Found invalid branch object (None or not dict): {type(branch)}")
                    continue
                
                branch_nodes = branch.get('nodes', [])
                branch_partition = branch.get('partition_map', [])
                
                # nodesê°€ ìˆê±°ë‚˜ partition_mapì´ ìˆìœ¼ë©´ ìœ íš¨í•œ ë¸Œëœì¹˜
                if branch_nodes or branch_partition:
                    valid_branches.append(branch)
                else:
                    logger.warning(f"[Kernel] [Warning] Skipping empty branch: {branch.get('branch_id', 'unknown')}")
            
            # [Guard] ìœ íš¨í•œ ë¸Œëœì¹˜ê°€ ì—†ìœ¼ë©´ SUCCEEDEDë¡œ ì§„í–‰
            if not valid_branches:
                logger.info(f"[Kernel] â­ï¸ No valid branches to execute, skipping parallel group")
                
                # [P0 Refactoring] HITP Edge Detection via outgoing_edges (O(1) lookup)
                next_segment = segment_id + 1
                total_segments = _safe_get_total_segments(event)
                hitp_detected = False
                
                # Use partition_map.outgoing_edges instead of scanning workflow_config.edges
                if partition_map and isinstance(partition_map, list) and next_segment < total_segments:
                    current_seg = partition_map[segment_id] if segment_id < len(partition_map) else None
                    next_seg = partition_map[next_segment] if next_segment < len(partition_map) else None
                    
                    if current_seg and next_seg:
                        edge_info = check_inter_segment_edges(current_seg, next_seg)
                        if is_hitp_edge(edge_info):
                            hitp_detected = True
                            logger.info(f"[Empty Parallel] ğŸš¨ HITP edge detected via outgoing_edges: "
                                      f"segment {segment_id} â†’ {next_segment}, "
                                      f"type={edge_info.get('edge_type')}, target={edge_info.get('target_node')}")
                
                if hitp_detected:
                    logger.info(f"[Empty Parallel] ğŸš¨ Pausing execution due to HITP edge. Next segment: {next_segment}")
                    return _finalize_with_offload({
                        "status": "PAUSED_FOR_HITP",
                        "final_state": mask_pii_in_state(initial_state),
                        "final_state_s3_path": None,
                        "next_segment_to_run": next_segment,
                        "new_history_logs": [],
                        "error_info": None,
                        "branches": None,
                        "segment_type": "hitp_pause",
                        "hitp_metadata": {
                            'hitp_edge_detected': True,
                            'pause_location': 'empty_parallel_group'
                        }
                    })
                
                return _finalize_with_offload({
                    "status": "CONTINUE",  # [Guard] [Fix] Use CONTINUE for ASL routing
                    "final_state": mask_pii_in_state(initial_state),
                    "final_state_s3_path": None,
                    "next_segment_to_run": next_segment,
                    "new_history_logs": [],
                    "error_info": None,
                    "branches": None,
                    "segment_type": "empty_parallel_group"
                })
            
            # ë³‘ë ¬ ìŠ¤ì¼€ì¤„ëŸ¬ í˜¸ì¶œ
            schedule_result = self._schedule_parallel_group(
                segment_config=segment_config,
                state=initial_state,
                segment_id=segment_id,
                owner_id=auth_user_id,
                workflow_id=workflow_id
            )
            
            # SCHEDULED_PARALLEL: ë°°ì¹˜ë³„ ìˆœì°¨ ì‹¤í–‰ í•„ìš”
            if schedule_result['status'] == 'SCHEDULED_PARALLEL':
                execution_batches = schedule_result['execution_batches']
                # [Guard] [P1 Fix] Inject scheduling_metadata into state for test verification
                meta = schedule_result.get('scheduling_metadata', {})
                batch_count = meta.get('batch_count', 1)
                
                initial_state['scheduling_metadata'] = meta
                initial_state['batch_count_actual'] = batch_count
                
                # [Guard] [P1 Fix] SPEED_GUARDRAIL_TEST requires this flag when splitting occurs
                if meta.get('strategy') == 'SPEED_OPTIMIZED' and batch_count > 1:
                    initial_state['guardrail_verified'] = True
                
                logger.info(f"[Scheduler] [System] Scheduled {meta.get('total_branches', len(valid_branches))} branches into "
                           f"{meta.get('batch_count', 1)} batches (strategy: {meta.get('strategy', 'UNKNOWN')})")
                
                return _finalize_with_offload({
                    "status": "SCHEDULED_PARALLEL",
                    "final_state": mask_pii_in_state(initial_state),
                    "final_state_s3_path": None,
                    "next_segment_to_run": segment_id + 1,
                    "new_history_logs": [],
                    "error_info": None,
                    "branches": None,  # [Optim] Remove redundant branches (use execution_batches)
                    "execution_batches": execution_batches,
                    "segment_type": "scheduled_parallel",
                    "scheduling_metadata": meta
                })
            
            # PARALLEL_GROUP: ê¸°ë³¸ ë³‘ë ¬ ì‹¤í–‰
            # [Guard] [P1 Fix] Inject scheduling_metadata into state for test verification (Consistent with SCHEDULED_PARALLEL)
            meta = schedule_result.get('scheduling_metadata', {})
            initial_state['scheduling_metadata'] = meta
            initial_state['batch_count_actual'] = meta.get('batch_count', 1)
            
            # ğŸŒ¿ [Pointer Strategy] schedule_result.branchesëŠ” ì´ë¯¸ ê²½ëŸ‰ í¬ì¸í„° ë°°ì—´
            # branches_s3_pathë„ ì „ë‹¬í•˜ì—¬ State Bagì— ì €ì¥
            branch_pointers = schedule_result.get('branches', valid_branches)
            branches_s3_path = schedule_result.get('branches_s3_path')
            
            return _finalize_with_offload({
                "status": "PARALLEL_GROUP",
                "final_state": mask_pii_in_state(initial_state),
                "final_state_s3_path": None,
                "next_segment_to_run": segment_id + 1,
                "new_history_logs": [],
                "error_info": None,
                "branches": branch_pointers,  # ğŸŒ¿ ê²½ëŸ‰ í¬ì¸í„° ë°°ì—´
                "branches_s3_path": branches_s3_path,  # ğŸŒ¿ S3 ê²½ë¡œ
                "execution_batches": schedule_result.get('execution_batches', [branch_pointers]),
                "segment_type": "parallel_group",
                "scheduling_metadata": meta
            })
        
        # [Guard] [Pattern 2] ì»¤ë„ ê²€ì¦: ì´ ì„¸ê·¸ë¨¼íŠ¸ê°€ SKIPPED ìƒíƒœì¸ê°€?
        segment_status = self._check_segment_status(segment_config)
        if segment_status == SEGMENT_STATUS_SKIPPED:
            skip_reason = segment_config.get('skip_reason', 'Kernel decision')
            logger.info(f"[Kernel] â­ï¸ Segment {segment_id} SKIPPED: {skip_reason}")
            
            # ì»¤ë„ ì•¡ì…˜ ë¡œê·¸ ê¸°ë¡
            kernel_log = {
                'action': 'SKIP',
                'segment_id': segment_id,
                'reason': skip_reason,
                'skipped_by': segment_config.get('skipped_by', 'kernel'),
                'timestamp': time.time()
            }
            
            return _finalize_response({
                "status": "SKIPPED",
                "final_state": mask_pii_in_state(initial_state),
                "final_state_s3_path": None,
                "next_segment_to_run": segment_id + 1,
                "new_history_logs": [],
                "error_info": None,
                "branches": None,
                "segment_type": "skipped",
                "kernel_action": kernel_log
            })
        
        # 5. Apply Self-Healing (Prompt Injection / Refinement)
        self.healer.apply_healing(segment_config, event.get("_self_healing_metadata"))
        
        # [Guard] [v2.2] Ring Protection: í”„ë¡¬í”„íŠ¸ ë³´ì•ˆ ê²€ì¦
        # ì„¸ê·¸ë¨¼íŠ¸ ë‚´ LLM ë…¸ë“œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ê²€ì¦í•˜ê³  ìœ„í—˜ íŒ¨í„´ íƒì§€
        security_violations = []
        if self.security_guard and RING_PROTECTION_AVAILABLE:
            security_violations = self._apply_ring_protection(
                segment_config=segment_config,
                initial_state=initial_state,
                segment_id=segment_id,
                workflow_id=workflow_id
            )
            
            # CRITICAL ìœ„ë°˜ ì‹œ SIGKILL (ì„¸ê·¸ë¨¼íŠ¸ ê°•ì œ ì¢…ë£Œ)
            critical_violations = [v for v in security_violations if v.get('should_sigkill')]
            if critical_violations:
                logger.error(f"[Kernel] [Guard] SIGKILL triggered by Ring Protection: {len(critical_violations)} critical violations")
                return _finalize_response({
                    "status": "SIGKILL",
                    "final_state": mask_pii_in_state(initial_state),
                    "final_state_s3_path": None,
                    "next_segment_to_run": None,
                    "new_history_logs": [],
                    "error_info": {
                        "error": "Security violation detected",
                        "error_type": "RingProtectionViolation",
                        "violations": critical_violations
                    },
                    "branches": None,
                    "segment_type": "sigkill",
                    "kernel_action": {
                        'action': 'SIGKILL',
                        'segment_id': segment_id,
                        'reason': 'Critical security violation',
                        'violations': critical_violations,
                        'timestamp': time.time()
                    }
                })
        
        # 6. Check User Quota / Secret Resolution (Repo access)
        # Note: In a full refactor, this should move to a UserService or AuthMiddleware
        # For now, we keep it simple.
        if auth_user_id:
            try:
                self.repo.get_user(auth_user_id) # Just validating access/existence
            except Exception as e:
                logger.warning("User check failed, but proceeding if possible: %s", e)

        # 7. Execute Workflow Segment with Kernel Defense
        # [Guard] [Kernel Defense] Aggressive Retry + Partial Success
        start_time = time.time()
        
        result_state, execution_error = self._execute_with_kernel_retry(
            segment_config=segment_config,
            initial_state=initial_state,
            auth_user_id=auth_user_id,
            event=event
        )
        
        execution_time = time.time() - start_time
        
        # [Guard] [Partial Success] ì‹¤íŒ¨í•´ë„ SUCCEEDED ë°˜í™˜ + ì—ëŸ¬ ë©”íƒ€ë°ì´í„° ê¸°ë¡
        if execution_error and ENABLE_PARTIAL_SUCCESS:
            logger.warning(
                f"[Kernel] [Warning] Segment {segment_id} failed but returning PARTIAL_SUCCESS. "
                f"Error: {execution_error['error']}"
            )
            
            # ì—ëŸ¬ ì •ë³´ë¥¼ ìƒíƒœì— ê¸°ë¡
            if isinstance(result_state, dict):
                result_state['__segment_error'] = execution_error
                result_state['__segment_status'] = 'PARTIAL_FAILURE'
                result_state['__failed_segment_id'] = segment_id
            
            # Partial Success ì»¤ë„ ë¡œê·¸
            kernel_log = {
                'action': 'PARTIAL_SUCCESS',
                'segment_id': segment_id,
                'error': execution_error['error'],
                'error_type': execution_error['error_type'],
                'retry_attempts': execution_error['retry_attempts'],
                'timestamp': time.time()
            }
            
            # [Alert] í•µì‹¬: FAILED ëŒ€ì‹  SUCCEEDED ë°˜í™˜ (ToleratedFailureThreshold ë°©ì§€)
            final_state, output_s3_path = self.state_manager.handle_state_storage(
                state=result_state,
                auth_user_id=auth_user_id,
                workflow_id=workflow_id,
                segment_id=segment_id,
                bucket=s3_bucket,
                threshold=self.threshold
            )
            
            # [Critical Fix] S3 offload ì‹œ final_state ë¹„ìš°ê¸° (256KB ì œí•œ íšŒí”¼)
            response_final_state = final_state
            if output_s3_path:
                response_final_state = {
                    "__s3_offloaded": True,
                    "__s3_path": output_s3_path,
                    "__original_size_kb": len(json.dumps(final_state, ensure_ascii=False).encode('utf-8')) / 1024 if final_state else 0
                }
                logger.info(f"[Partial Failure] [S3 Offload] Replaced final_state with metadata. Original: {response_final_state['__original_size_kb']:.1f}KB â†’ Response: ~0.2KB")
            
            total_segments = _safe_get_total_segments(event)
            next_segment = segment_id + 1
            has_more_segments = next_segment < total_segments
            
            # [P0 Refactoring] HITP Edge Detection via outgoing_edges (O(1) lookup)
            hitp_detected = False
            if has_more_segments:
                partition_map = event.get('partition_map')
                if partition_map and isinstance(partition_map, list):
                    current_seg = partition_map[segment_id] if segment_id < len(partition_map) else None
                    next_seg = partition_map[next_segment] if next_segment < len(partition_map) else None
                    
                    if current_seg and next_seg:
                        edge_info = check_inter_segment_edges(current_seg, next_seg)
                        if is_hitp_edge(edge_info):
                            hitp_detected = True
                            logger.info(f"[Partial Success] ğŸš¨ HITP edge detected via outgoing_edges: "
                                      f"segment {segment_id} â†’ {next_segment}, "
                                      f"type={edge_info.get('edge_type')}, target={edge_info.get('target_node')}")
            
            if hitp_detected:
                logger.info(f"[Partial Success] ğŸš¨ Pausing execution due to HITP edge. Next segment: {next_segment}")
                return _finalize_response({
                    "status": "PAUSED_FOR_HITP",
                    "final_state": response_final_state,
                    "final_state_s3_path": output_s3_path,
                    "next_segment_to_run": next_segment,
                    "new_history_logs": [],
                    "error_info": execution_error,
                    "branches": None,
                    "segment_type": "hitp_pause",
                    "kernel_action": kernel_log,
                    "execution_time": execution_time,
                    "_partial_success": True,
                    "total_segments": total_segments,
                    "hitp_metadata": {
                        'hitp_edge_detected': True,
                        'pause_location': 'partial_success'
                    }
                })
            
            return _finalize_response({
                # [Guard] [Fix] Use CONTINUE/COMPLETE instead of SUCCEEDED for ASL routing
                "status": "CONTINUE" if has_more_segments else "COMPLETE",
                "final_state": response_final_state,
                "final_state_s3_path": output_s3_path,
                "next_segment_to_run": next_segment if has_more_segments else None,
                "new_history_logs": [],
                "error_info": execution_error,  # ì—ëŸ¬ ì •ë³´ëŠ” ë©”íƒ€ë°ì´í„°ë¡œ ì „ë‹¬
                "branches": None,
                "segment_type": "partial_failure",
                "kernel_action": kernel_log,
                "execution_time": execution_time,
                "_partial_success": True,  # í´ë¼ì´ì–¸íŠ¸ê°€ ë¶€ë¶„ ì‹¤íŒ¨ ê°ì§€ìš©
                "total_segments": total_segments
            })
        
        execution_time = time.time() - start_time
        
        # [Guard] [Pattern 2] ì¡°ê±´ë¶€ ìŠ¤í‚µ ê²°ì •
        # ì‹¤í–‰ ê²°ê³¼ì—ì„œ ìŠ¤í‚µí•  ì„¸ê·¸ë¨¼íŠ¸ê°€ ì§€ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
        manifest_s3_path = event.get('segment_manifest_s3_path')
        if manifest_s3_path and isinstance(result_state, dict):
            skip_next_segments = result_state.get('_kernel_skip_segments', [])
            if skip_next_segments:
                skip_reason = result_state.get('_kernel_skip_reason', 'Condition not met')
                self._mark_segments_for_skip(manifest_s3_path, skip_next_segments, skip_reason)
                logger.info(f"[Kernel] Marked {len(skip_next_segments)} segments for skip: {skip_reason}")
            
            # ë³µêµ¬ ì„¸ê·¸ë¨¼íŠ¸ ì‚½ì… ìš”ì²­ ì²˜ë¦¬
            recovery_request = result_state.get('_kernel_inject_recovery')
            if recovery_request:
                self._inject_recovery_segments(
                    manifest_s3_path=manifest_s3_path,
                    after_segment_id=segment_id,
                    recovery_segments=recovery_request.get('segments', []),
                    reason=recovery_request.get('reason', 'Recovery injection')
                )
        
        # 8. Handle Output State Storage
        # [Critical] Pre-check result_state size before S3 offload decision
        # Using global json module imported at top of file (Line 5)
        result_state_size = len(json.dumps(result_state, ensure_ascii=False).encode('utf-8')) if result_state else 0
        logger.info(f"[Large Payload Check] result_state size: {result_state_size} bytes ({result_state_size/1024:.1f}KB), "
                   f"s3_bucket: {'SET' if s3_bucket else 'NOT SET'}, threshold: {self.threshold}")
        
        if result_state_size > 250000:  # 250KB - Step Functions limit is 256KB
            logger.warning(f"[Alert] [Large Payload Warning] result_state exceeds 250KB! "
                          f"Size: {result_state_size/1024:.1f}KB. S3 offload REQUIRED.")
        
        # [v3.10] Extract loop_counter for Loop-Safe S3 Paths
        # Check result_state first (dynamic updates), then event (context)
        loop_counter = None
        if isinstance(result_state, dict):
            loop_counter = result_state.get('loop_counter')
        
        if loop_counter is None:
            loop_counter = event.get('loop_counter')
            
        # Ensure proper type
        if loop_counter is not None:
            try:
                loop_counter = int(loop_counter)
            except (ValueError, TypeError):
                loop_counter = None

        # [Critical Fix] Distributed Map ëª¨ë“œì—ì„œëŠ” ë¬´ì¡°ê±´ S3 ì˜¤í”„ë¡œë”©
        # ê° iteration ê²°ê³¼ê°€ ê°œë³„ì ìœ¼ë¡œëŠ” ì‘ì•„ë„ Distributed Mapì´ ëª¨ë“  ê²°ê³¼ë¥¼
        # ë°°ì—´ë¡œ ìˆ˜ì§‘í•˜ë©´ 256KB ì œí•œì„ ì´ˆê³¼í•  ìˆ˜ ìˆìŒ
        # [Fix] distributed_modeê°€ null(JSON)/None(Python)ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëª…ì‹œì  True ì²´í¬
        # ğŸ›¡ï¸ [v3.6] í•¨ìˆ˜ ìŠ¤ì½”í”„ì—ì„œ ì´ë¯¸ ì¶”ì¶œëœ ë¡œì»¬ ë³€ìˆ˜ ì‚¬ìš©
        is_distributed_mode = distributed_mode is True
        
        # [Critical Fix] Map State ë¸Œëœì¹˜ ì‹¤í–‰ë„ ê°•ì œ ì˜¤í”„ë¡œë”© í•„ìš”
        # Map Stateê°€ ëª¨ë“  ë¸Œëœì¹˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•  ë•Œ 256KB ì œí•œ ì´ˆê³¼ ë°©ì§€
        # branch_item ì¡´ì¬ = Map Iteratorì—ì„œ ì‹¤í–‰ ì¤‘ (ê° ë¸Œëœì¹˜ëŠ” ì‘ì€ ë ˆí¼ëŸ°ìŠ¤ë§Œ ë°˜í™˜í•´ì•¼ í•¨)
        is_map_branch = event.get('branch_item') is not None

        # [Critical Fix] ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        # ì„¸ê·¸ë¨¼íŠ¸ê°€ ë” ìˆìœ¼ë©´ ìƒíƒœê°€ ëˆ„ì ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‚®ì€ threshold ì ìš©
        total_segments = _safe_get_total_segments(event)
        has_next_segment = (segment_id + 1) < total_segments
        
        # [Critical Fix] ForEach/Map ê°™ì€ ë°˜ë³µ êµ¬ì¡° ê°ì§€
        # í˜„ì¬ ë˜ëŠ” ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ì— for_each íƒ€ì…ì´ ìˆìœ¼ë©´ ê°•ì œ offload
        has_loop_structure = False
        if isinstance(segment_config, dict):
            # í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ì˜ ë…¸ë“œë“¤ í™•ì¸
            nodes = segment_config.get('nodes', [])
            logger.info(f"[Loop Detection] Checking {len(nodes)} nodes in segment {segment_id} for loop structures")
            for node in nodes:
                if isinstance(node, dict):
                    node_type = node.get('type')
                    node_id = node.get('id', 'unknown')
                    if node_type in ('for_each', 'nested_for_each'):
                        has_loop_structure = True
                        logger.info(f"[Loop Detection] Found loop structure: node_id={node_id}, type={node_type}")
                        break

        if is_distributed_mode:
            # Distributed Map: threshold=0ìœ¼ë¡œ ê°•ì œ ì˜¤í”„ë¡œë”©
            effective_threshold = 0
            logger.info(f"[Distributed Map] Forcing S3 offload for iteration result (distributed_mode=True)")
        elif is_map_branch:
            # [Critical] Map State ë¸Œëœì¹˜: ë¬´ì¡°ê±´ S3 ì˜¤í”„ë¡œë”© (threshold=0)
            # ì´ìœ : ë¸Œëœì¹˜ ê°œìˆ˜ê°€ ê°€ë³€ì  (Nê°œ Ã— 50KB = NÃ—50KB)
            # ì˜ˆì‹œ: 10ê°œ ë¸Œëœì¹˜ Ã— 50KB = 500KB â†’ 256KB ì´ˆê³¼!
            # í•´ê²°: ë¸Œëœì¹˜ í¬ê¸°ì™€ ë¬´ê´€í•˜ê²Œ ëª¨ë“  ê²°ê³¼ë¥¼ S3ë¡œ ì˜¤í”„ë¡œë“œ
            # Mapì€ ì‘ì€ S3 ë ˆí¼ëŸ°ìŠ¤ë§Œ ìˆ˜ì§‘ (Nê°œ Ã— 2KB = 2N KB << 256KB)
            effective_threshold = 0  # ê°•ì œ ì˜¤í”„ë¡œë“œ
            logger.info(f"[Map Branch] Forcing S3 offload for ALL branch results (variable fan-out protection)")
        elif has_loop_structure:
            # [Critical Fix] ForEach/ë°˜ë³µ êµ¬ì¡°ê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ê°•ì œ offload (threshold=0)
            # ì´ìœ : ë°˜ë³µ íšŸìˆ˜ Ã— ê²°ê³¼ í¬ê¸° = ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ëˆ„ì 
            # ì˜ˆ: 40íšŒ Ã— 15KB = 600KB >> 256KB (20KB thresholdë¡œëŠ” ë°©ì–´ ë¶ˆê°€)
            # í•´ê²°: iteration í¬ê¸°ì™€ ë¬´ê´€í•˜ê²Œ ëª¨ë“  ê²°ê³¼ë¥¼ S3ë¡œ ì˜¤í”„ë¡œë“œ
            effective_threshold = 0  # ê°•ì œ ì˜¤í”„ë¡œë“œ
            logger.info(f"[Loop Structure] Forcing S3 offload for ALL iteration results (accumulation prevention)")
        elif has_next_segment and result_state_size > 20000:
            # [Segment Chain] ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ê°€ ìˆê³  20KB ì´ìƒì´ë©´ offload
            # ì´ìœ : ì„¸ê·¸ë¨¼íŠ¸ ì²´ì¸ì—ì„œ ìƒíƒœ ëˆ„ì  ë°©ì§€
            effective_threshold = 20000  # 20KB threshold
            logger.info(f"[Segment Chain] S3 offload for large state: "
                       f"has_next={has_next_segment}, size={result_state_size/1024:.1f}KB")
        else:
            effective_threshold = self.threshold

        final_state, output_s3_path = self.state_manager.handle_state_storage(
            state=result_state,
            auth_user_id=auth_user_id,
            workflow_id=workflow_id,
            segment_id=segment_id,
            bucket=s3_bucket,
            threshold=effective_threshold,
            loop_counter=loop_counter
        )
        
        # [Critical] Map ë¸Œëœì¹˜ ì‘ë‹µ í˜ì´ë¡œë“œ ìµœì†Œí™”
        # Map StateëŠ” ëª¨ë“  ë¸Œëœì¹˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ë¯€ë¡œ ì‘ë‹µ í¬ê¸°ê°€ ì¤‘ìš”
        # S3ì— ì „ì²´ ìƒíƒœë¥¼ ì €ì¥í–ˆìœ¼ë©´ ì‘ë‹µì€ ì‘ì€ ë ˆí¼ëŸ°ìŠ¤ë§Œ í¬í•¨
        if is_map_branch and output_s3_path:
            # [Emergency Payload Pruning] ëŒ€ìš©ëŸ‰ í•„ë“œ ì œê±°
            # documents, queries ê°™ì€ í° ë°°ì—´ì€ S3ì— ìˆìœ¼ë¯€ë¡œ ì‘ë‹µì—ì„œ ì œì™¸
            if isinstance(final_state, dict):
                # ë³´ì¡´í•  í•„ë“œë§Œ ì„ íƒ (step_history, ë©”íƒ€ë°ì´í„°ëŠ” ìœ ì§€)
                pruned_state = {
                    'step_history': final_state.get('step_history', []),
                    '__new_history_logs': final_state.get('__new_history_logs', []),
                    'execution_logs': final_state.get('execution_logs', []),
                    'guardrail_verified': final_state.get('guardrail_verified'),
                    'batch_count_actual': final_state.get('batch_count_actual'),
                    'scheduling_metadata': final_state.get('scheduling_metadata', {}),
                    'state_size_threshold': final_state.get('state_size_threshold'),
                    '__scheduling_metadata': final_state.get('__scheduling_metadata', {}),
                    '__guardrail_verified': final_state.get('__guardrail_verified'),
                    '__batch_count_actual': final_state.get('__batch_count_actual'),
                    # ğŸš€ í† í° ê´€ë ¨ í•„ë“œ ì¶”ê°€ (CRITICAL for aggregation)
                    'total_tokens': final_state.get('total_tokens', 0),
                    'total_input_tokens': final_state.get('total_input_tokens', 0),
                    'total_output_tokens': final_state.get('total_output_tokens', 0),
                    'usage': final_state.get('usage', {}),
                }
                # None ê°’ ì œê±° (ì‘ë‹µ í¬ê¸° ì¶”ê°€ ì ˆê°)
                pruned_state = {k: v for k, v in pruned_state.items() if v is not None}
                
                pruned_size = len(json.dumps(pruned_state, ensure_ascii=False).encode('utf-8'))
                original_size = len(json.dumps(final_state, ensure_ascii=False).encode('utf-8'))
                logger.info(f"[Map Branch Pruning] Reduced payload from {original_size/1024:.1f}KB to {pruned_size/1024:.1f}KB "
                           f"({100*(1-pruned_size/original_size):.1f}% reduction). Full state in S3: {output_s3_path}")
                
                final_state = pruned_state
        
        # [Critical] Log the actual return payload size
        return_payload_size = len(json.dumps(final_state, ensure_ascii=False).encode('utf-8')) if final_state else 0
        logger.info(f"[Large Payload Check] After S3 offload - final_state size: {return_payload_size} bytes ({return_payload_size/1024:.1f}KB), "
                   f"s3_path: {output_s3_path or 'None'}")
        
        # Extract history logs from result_state if available
        new_history_logs = result_state.get('__new_history_logs', []) if isinstance(result_state, dict) else []
        
        # [Critical Fix] ì›Œí¬í”Œë¡œìš° ì™„ë£Œ ì—¬ë¶€ ê²°ì •
        # 1. test_workflow_configê°€ ì£¼ì…ëœ ê²½ìš° (E2E í…ŒìŠ¤íŠ¸): í•œ ë²ˆì— ì „ì²´ ì‹¤í–‰ í›„ ì™„ë£Œ
        # 2. partition_mapì´ ì—†ëŠ” ê²½ìš°: ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ í•œ ë²ˆì— ì‹¤í–‰í–ˆìœ¼ë¯€ë¡œ ì™„ë£Œ
        # 3. partition_mapì´ ìˆëŠ” ê²½ìš°: ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
        is_e2e_test = event.get('test_workflow_config') is not None
        has_partition_map = partition_map is not None and len(partition_map) > 0
        
        # ì»¤ë„ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ìˆëŠ” ê²½ìš°)
        kernel_actions = result_state.get('__kernel_actions', []) if isinstance(result_state, dict) else []
        
        if is_e2e_test or not has_partition_map:
            # E2E í…ŒìŠ¤íŠ¸ ë˜ëŠ” íŒŒí‹°ì…˜ ì—†ëŠ” ë‹¨ì¼ ì‹¤í–‰: ì›Œí¬í”Œë¡œìš° ì™„ë£Œ
            # [Critical Fix] S3 offload ì‹œ final_state ë¹„ìš°ê¸° (256KB ì œí•œ íšŒí”¼)
            response_final_state = final_state
            if output_s3_path:
                response_final_state = {
                    "__s3_offloaded": True,
                    "__s3_path": output_s3_path,
                    "__original_size_kb": len(json.dumps(final_state, ensure_ascii=False).encode('utf-8')) / 1024 if final_state else 0
                }
                logger.info(f"[S3 Offload] Replaced final_state with metadata reference (E2E). Original: {response_final_state['__original_size_kb']:.1f}KB â†’ Response: ~0.2KB")
            
            return _finalize_response({
                "status": "COMPLETE",  # ASLì´ ê¸°ëŒ€í•˜ëŠ” ìƒíƒœê°’
                "final_state": response_final_state,
                "final_state_s3_path": output_s3_path,
                "next_segment_to_run": None,  # ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì—†ìŒ
                "new_history_logs": new_history_logs,
                "error_info": None,
                "branches": None,
                "segment_type": "final",
                "state_s3_path": output_s3_path,
                "execution_time": execution_time,
                "kernel_actions": kernel_actions if kernel_actions else None,
                "total_segments": _total_segments
            })
        
        # íŒŒí‹°ì…˜ ë§µì´ ìˆëŠ” ê²½ìš°: ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        total_segments = _safe_get_total_segments(event)
        
        next_segment = segment_id + 1
        
        if next_segment >= total_segments:
            # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ì™„ë£Œ
            # [Critical Fix] S3 offload ì‹œ final_state ë¹„ìš°ê¸° (256KB ì œí•œ íšŒí”¼)
            response_final_state = final_state
            if output_s3_path:
                response_final_state = {
                    "__s3_offloaded": True,
                    "__s3_path": output_s3_path,
                    "__original_size_kb": len(json.dumps(final_state, ensure_ascii=False).encode('utf-8')) / 1024 if final_state else 0
                }
                logger.info(f"[S3 Offload] Replaced final_state with metadata reference (Final). Original: {response_final_state['__original_size_kb']:.1f}KB â†’ Response: ~0.2KB")
            
            return _finalize_response({
                "status": "COMPLETE",
                "final_state": response_final_state,
                "final_state_s3_path": output_s3_path,
                "next_segment_to_run": None,
                "new_history_logs": new_history_logs,
                "error_info": None,
                "branches": None,
                "segment_type": "final",
                "state_s3_path": output_s3_path,
                "execution_time": execution_time,
                "kernel_actions": kernel_actions if kernel_actions else None,
                "total_segments": total_segments
            })
        
        # ì•„ì§ ì‹¤í–‰í•  ì„¸ê·¸ë¨¼íŠ¸ê°€ ë‚¨ì•„ìˆìŒ
        # [P0 Refactoring] S3 offload via helper function (DRY principle)
        response_final_state = prepare_response_with_offload(final_state, output_s3_path)
        # [Critical Fix] Safe check - response_final_state is guaranteed non-None by prepare_response_with_offload
        if response_final_state and response_final_state.get('__s3_offloaded'):
            logger.info(f"[S3 Offload] Replaced final_state with metadata reference. "
                       f"Original: {response_final_state.get('__original_size_kb', 0):.1f}KB â†’ Response: ~0.2KB")
        
        # [P0 Refactoring] HITP Edge Detection via outgoing_edges (O(1) lookup)
        hitp_detected = False
        if next_segment < total_segments and partition_map and isinstance(partition_map, list):
            current_seg = partition_map[segment_id] if segment_id < len(partition_map) else None
            next_seg = partition_map[next_segment] if next_segment < len(partition_map) else None
            
            if current_seg and next_seg:
                edge_info = check_inter_segment_edges(current_seg, next_seg)
                if is_hitp_edge(edge_info):
                    hitp_detected = True
                    logger.info(f"[General Segment] ğŸš¨ HITP edge detected via outgoing_edges: "
                              f"segment {segment_id} â†’ {next_segment}, "
                              f"type={edge_info.get('edge_type')}, target={edge_info.get('target_node')}")
        
        if hitp_detected:
            logger.info(f"[General Segment] ğŸš¨ Pausing execution due to HITP edge. Next segment: {next_segment}")
            return _finalize_response({
                "status": "PAUSED_FOR_HITP",
                "final_state": response_final_state,
                "final_state_s3_path": output_s3_path,
                "next_segment_to_run": next_segment,
                "new_history_logs": new_history_logs,
                "error_info": None,
                "branches": None,
                "segment_type": "hitp_pause",
                "state_s3_path": output_s3_path,
                "execution_time": execution_time,
                "kernel_actions": kernel_actions if kernel_actions else None,
                "total_segments": total_segments,
                "hitp_metadata": {
                    'hitp_edge_detected': True,
                    'pause_location': 'general_segment_completion'
                }
            })
        
        return _finalize_response({
            "status": "CONTINUE",  # [Guard] [Fix] Explicit status for loop continuation (was 'SUCCEEDED')
            "final_state": response_final_state,
            "final_state_s3_path": output_s3_path,
            "next_segment_to_run": next_segment,
            "new_history_logs": new_history_logs,
            "error_info": None,
            "branches": None,
            "segment_type": "normal",
            "state_s3_path": output_s3_path,
            "execution_time": execution_time,
            "kernel_actions": kernel_actions if kernel_actions else None,
            "total_segments": total_segments
        })

    def _resolve_segment_config(self, workflow_config, partition_map, segment_id):
        """
        Identical logic to original handler for partitioning.
        
        [v3.27] Extract segment_config from partition manifest structure
        """
        # [Critical Fix] workflow_configì´ Noneì´ë©´ ì¡°ê¸° ì²˜ë¦¬
        if not workflow_config:
            logger.error(f"[_resolve_segment_config] [Warning] workflow_config is None! segment_id={segment_id}")
            # partition_mapì—ì„œ ì§ì ‘ ì°¾ê¸° ì‹œë„
            if partition_map:
                if isinstance(partition_map, list) and 0 <= segment_id < len(partition_map):
                    segment = partition_map[segment_id]
                    # [v3.27] Extract segment_config if it's nested
                    if isinstance(segment, dict) and 'segment_config' in segment:
                        return segment['segment_config']
                    return segment
                elif isinstance(partition_map, dict) and str(segment_id) in partition_map:
                    segment = partition_map[str(segment_id)]
                    # [v3.27] Extract segment_config if it's nested
                    if isinstance(segment, dict) and 'segment_config' in segment:
                        return segment['segment_config']
                    return segment
            # ì—ëŸ¬ ì •ë³´ë¥¼ í¬í•¨í•œ ê¸°ë³¸ segment_config ë°˜í™˜
            return {
                "type": "error",
                "error": "workflow_config is None",
                "segment_id": segment_id,
                "nodes": [],
                "edges": []
            }
        
        # Basic full workflow or pre-chunked
        # If we are strictly running a segment, we might need to simulate partitioning if map is missing
        # For simplicity, we assume workflow_config IS the segment config if partition_map is missing
        # OR we call the dynamic partitioner.
        if not partition_map:
            # Fallback to dynamic partitioning logic
            parts = _partition_workflow_dynamically(workflow_config) # arbitrary chunks removed
            if 0 <= segment_id < len(parts):
                return parts[segment_id]
            return workflow_config # Fallback

        # [Critical Fix] partition_mapì´ list ë˜ëŠ” dictì¼ ìˆ˜ ìˆìŒ
        if partition_map:
            if isinstance(partition_map, list):
                # listì¸ ê²½ìš°: ì¸ë±ìŠ¤ë¡œ ì ‘ê·¼
                if 0 <= segment_id < len(partition_map):
                    segment = partition_map[segment_id]
                    # [v3.27 CRITICAL FIX] Extract segment_config from manifest structure
                    # Partition manifest has: {segment_id, segment_config: {nodes, edges}, type, ...}
                    # run_workflow expects: {nodes, edges, ...}
                    if isinstance(segment, dict) and 'segment_config' in segment:
                        extracted_config = segment['segment_config']
                        logger.info(f"[v3.27] âœ“ Extracted segment_config from manifest for segment {segment_id}")
                        logger.info(f"[v3.27] Config has {len(extracted_config.get('nodes', []))} nodes: "
                                   f"{[n.get('id') for n in extracted_config.get('nodes', [])]}")
                        return extracted_config
                    logger.warning(f"[v3.27] âœ— No segment_config key found in manifest segment {segment_id}, "
                                 f"available keys: {list(segment.keys() if isinstance(segment, dict) else [])}")
                    return segment
            elif isinstance(partition_map, dict):
                # dictì¸ ê²½ìš°: ë¬¸ìì—´ í‚¤ë¡œ ì ‘ê·¼
                if str(segment_id) in partition_map:
                    segment = partition_map[str(segment_id)]
                    # [v3.27] Extract segment_config if it's nested
                    if isinstance(segment, dict) and 'segment_config' in segment:
                        extracted_config = segment['segment_config']
                        logger.info(f"[v3.27] Extracted segment_config from manifest for segment {segment_id}")
                        return extracted_config
                    return segment
            
        # Simplified fallback - workflow_config ë˜ëŠ” ì—ëŸ¬ ìƒíƒœ
        if workflow_config:
            logger.warning(f"[_resolve_segment_config] [Warning] partition_map unavailable, falling back to workflow_config. "
                          f"This may cause issues for large/complex workflows. segment_id={segment_id}")
            return workflow_config
        
        # [Critical Fix] ëª¨ë“  fallback ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ìƒíƒœ ë°˜í™˜ (None ë°˜í™˜ ë°©ì§€)
        logger.error(f"[_resolve_segment_config] [Alert] All fallbacks failed! segment_id={segment_id}")
        return {
            "type": "error",
            "error": "Failed to resolve segment config - both workflow_config and partition_map are invalid",
            "segment_id": segment_id,
            "nodes": [],
            "edges": []
        }
