import os
import json
import logging
import threading
from typing import Dict, Any, List, Set, Optional, Tuple, FrozenSet
from src.services.workflow.repository import WorkflowRepository

logger = logging.getLogger(__name__)

# ============================================================================
# [v2.0 Production Hardening] ìƒìˆ˜ ë° ì„¤ì •
# ============================================================================

# ìµœëŒ€ ì¬ê·€ ê¹Šì´ ì œí•œ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
MAX_PARTITION_DEPTH = int(os.environ.get("MAX_PARTITION_DEPTH", "50"))

# ìµœëŒ€ ë…¸ë“œ ìˆ˜ ì œí•œ (ëŒ€ê·œëª¨ ê·¸ë˜í”„ ë³´í˜¸)
MAX_NODES_LIMIT = int(os.environ.get("MAX_NODES_LIMIT", "500"))

# LLM ë…¸ë“œ íƒ€ì…ë“¤ - ì´ íƒ€ì…ë“¤ì„ ë§Œë‚  ë•Œë§ˆë‹¤ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤
LLM_NODE_TYPES: FrozenSet[str] = frozenset({
    "llm_chat",
    "openai_chat", 
    "anthropic_chat",
    "bedrock_chat",
    "claude_chat",
    "gpt_chat",
    "aiModel"  # ë²”ìš© AI ëª¨ë¸ ë…¸ë“œ íƒ€ì… ì¶”ê°€
})

# HITP (Human in the Loop) ì—£ì§€ íƒ€ì…ë“¤
HITP_EDGE_TYPES: FrozenSet[str] = frozenset({"hitp", "human_in_the_loop", "pause"})

# ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì…ë“¤
SEGMENT_TYPES: FrozenSet[str] = frozenset({
    "normal", "llm", "hitp", "isolated", "complete", "parallel_group", "aggregator"
})


# ============================================================================
# [Critical Fix #1] ì‚¬ì´í´ ê°ì§€ ì˜ˆì™¸ ë° DAG ê²€ì¦
# ============================================================================

class CycleDetectedError(Exception):
    """ê·¸ë˜í”„ì—ì„œ ì‚¬ì´í´(ìˆœí™˜ ì°¸ì¡°)ì´ ê°ì§€ë˜ì—ˆì„ ë•Œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
    def __init__(self, cycle_path: List[str]):
        self.cycle_path = cycle_path
        super().__init__(
            f"Cycle detected in workflow graph: {' -> '.join(cycle_path)}. "
            f"Workflows must be DAGs (Directed Acyclic Graphs)."
        )


class PartitionDepthExceededError(Exception):
    """íŒŒí‹°ì…”ë‹ ì¬ê·€ ê¹Šì´ ì´ˆê³¼ ì‹œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
    def __init__(self, depth: int, max_depth: int):
        self.depth = depth
        self.max_depth = max_depth
        super().__init__(
            f"Partition recursion depth ({depth}) exceeded maximum ({max_depth}). "
            f"Consider simplifying the workflow or increasing MAX_PARTITION_DEPTH."
        )


class BranchTerminationError(Exception):
    """ë¸Œëœì¹˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì¢…ë£Œë˜ì§€ ì•Šì•˜ì„ ë•Œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
    def __init__(self, branch_id: str, message: str):
        self.branch_id = branch_id
        super().__init__(f"Branch '{branch_id}' termination error: {message}")


def validate_dag(
    nodes: Dict[str, Any], 
    outgoing_edges: Dict[str, List[Dict[str, Any]]]
) -> Tuple[bool, Optional[List[str]]]:
    """
    [Critical Fix #1] ê·¸ë˜í”„ê°€ DAG(Directed Acyclic Graph)ì¸ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Kahn's Algorithm (ìœ„ìƒ ì •ë ¬) ê¸°ë°˜ ì‚¬ì´í´ ê°ì§€.
    
    Args:
        nodes: ë…¸ë“œ ID -> ë…¸ë“œ ì •ì˜ ë§µ
        outgoing_edges: ë…¸ë“œ ID -> ë‚˜ê°€ëŠ” ì—£ì§€ ë¦¬ìŠ¤íŠ¸ ë§µ
        
    Returns:
        Tuple[is_dag, cycle_path]: DAGì´ë©´ (True, None), ì•„ë‹ˆë©´ (False, cycle_path)
    """
    if not nodes:
        return True, None
    
    # ì§„ì… ì°¨ìˆ˜(in-degree) ê³„ì‚°
    in_degree: Dict[str, int] = {nid: 0 for nid in nodes}
    
    for source_id, edges in outgoing_edges.items():
        for edge in edges:
            target = edge.get("target")
            if target and target in in_degree:
                in_degree[target] += 1
    
    # ì§„ì… ì°¨ìˆ˜ê°€ 0ì¸ ë…¸ë“œë“¤ë¡œ ì‹œì‘
    queue = [nid for nid, deg in in_degree.items() if deg == 0]
    visited_count = 0
    
    while queue:
        node_id = queue.pop(0)
        visited_count += 1
        
        for edge in outgoing_edges.get(node_id, []):
            target = edge.get("target")
            if target and target in in_degree:
                in_degree[target] -= 1
                if in_degree[target] == 0:
                    queue.append(target)
    
    # ëª¨ë“  ë…¸ë“œë¥¼ ë°©ë¬¸í•˜ì§€ ëª»í–ˆë‹¤ë©´ ì‚¬ì´í´ ì¡´ì¬
    if visited_count < len(nodes):
        # ì‚¬ì´í´ ê²½ë¡œ ì¶”ì  (DFSë¡œ ì‹¤ì œ ì‚¬ì´í´ ì°¾ê¸°)
        cycle_path = _find_cycle_path(nodes, outgoing_edges)
        return False, cycle_path
    
    return True, None


def _find_cycle_path(
    nodes: Dict[str, Any], 
    outgoing_edges: Dict[str, List[Dict[str, Any]]]
) -> List[str]:
    """DFSë¡œ ì‹¤ì œ ì‚¬ì´í´ ê²½ë¡œë¥¼ ì¶”ì í•©ë‹ˆë‹¤."""
    WHITE, GRAY, BLACK = 0, 1, 2
    color: Dict[str, int] = {nid: WHITE for nid in nodes}
    parent: Dict[str, Optional[str]] = {nid: None for nid in nodes}
    
    def dfs(node_id: str, path: List[str]) -> Optional[List[str]]:
        color[node_id] = GRAY
        path.append(node_id)
        
        for edge in outgoing_edges.get(node_id, []):
            target = edge.get("target")
            if target and target in color:
                if color[target] == GRAY:
                    # ì‚¬ì´í´ ë°œê²¬ - ê²½ë¡œ ì¶”ì¶œ
                    cycle_start = path.index(target)
                    return path[cycle_start:] + [target]
                elif color[target] == WHITE:
                    result = dfs(target, path)
                    if result:
                        return result
        
        color[node_id] = BLACK
        path.pop()
        return None
    
    for nid in nodes:
        if color[nid] == WHITE:
            result = dfs(nid, [])
            if result:
                return result
    
    return ["unknown_cycle"]  # í´ë°±


def partition_workflow_advanced(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    ê³ ê¸‰ ì›Œí¬í”Œë¡œìš° ë¶„í• : HITP ì—£ì§€ì™€ LLM ë…¸ë“œ ê¸°ë°˜ìœ¼ë¡œ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    [v2.0 Production Hardening]
    - DAG(Directed Acyclic Graph) ì‚¬ì „ ê²€ì¦
    - ì¬ê·€ ê¹Šì´ ì œí•œ (MAX_PARTITION_DEPTH)
    - í•©ë¥˜ì (Convergence Node) ê°•ì œ ë¶„í• 
    - ë¸Œëœì¹˜ ì¢…ë£Œ ê²€ì¦
    - Thread-safe ID ìƒì„±
    
    ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜:
    - ë³‘í•© ì§€ì (Merge Point) ê°ì§€ ë° ì²˜ë¦¬
    - ë³‘ë ¬ ê·¸ë£¹(Parallel Group) ìƒì„± ë° ì¬ê·€ì  íŒŒí‹°ì…”ë‹
    - Convergence Node ì°¾ê¸° ë° ë¸Œëœì¹˜ ì œí•œ
    - ì¬ê·€ì  Node-to-Segment ë§¤í•‘
    
    Raises:
        CycleDetectedError: ê·¸ë˜í”„ì— ì‚¬ì´í´ì´ ìˆëŠ” ê²½ìš°
        PartitionDepthExceededError: ì¬ê·€ ê¹Šì´ ì´ˆê³¼ ì‹œ
        ValueError: ë…¸ë“œ ìˆ˜ ì œí•œ ì´ˆê³¼ ì‹œ
    """
    # ğŸ›¡ï¸ [v3.8] None defense: filter out None elements from nodes list
    raw_nodes = config.get("nodes", [])
    nodes = {n["id"]: n for n in raw_nodes if n is not None and isinstance(n, dict) and "id" in n}
    edges = config.get("edges", []) if config.get("edges") else []
    
    # [Critical Fix] ë…¸ë“œ ìˆ˜ ì œí•œ ê²€ì¦
    if len(nodes) > MAX_NODES_LIMIT:
        raise ValueError(
            f"Workflow has {len(nodes)} nodes, exceeding maximum limit of {MAX_NODES_LIMIT}. "
            f"Consider splitting into subgraphs."
        )
    
    # [Performance Optimization] ì—£ì§€ ë§µ ìƒì„± (Pre-indexed)
    # í–¥í›„ ì›Œí¬í”Œë¡œìš° ì €ì¥ ì‹œì ì— ë©”íƒ€ë°ì´í„°ë¡œ ì¶”ì¶œí•˜ì—¬ ì¬ì‚¬ìš© ê°€ëŠ¥
    incoming_edges: Dict[str, List[Dict[str, Any]]] = {}
    outgoing_edges: Dict[str, List[Dict[str, Any]]] = {}
    
    for edge in edges:
        source = edge.get("source")
        target = edge.get("target") 
        if source:
            outgoing_edges.setdefault(source, []).append(edge)
        if target:
            incoming_edges.setdefault(target, []).append(edge)
    
    # [Critical Fix #1] DAG ê²€ì¦ - ì‚¬ì´í´ ê°ì§€
    is_dag, cycle_path = validate_dag(nodes, outgoing_edges)
    if not is_dag:
        raise CycleDetectedError(cycle_path or ["unknown"])
    
    # [Critical Fix #2] í•©ë¥˜ì  ì§‘í•© - ì´ ë…¸ë“œë“¤ì€ ë°˜ë“œì‹œ ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ì ì´ ë¨
    # find_convergence_nodeë¡œ ì°¾ì€ ëª¨ë“  í•©ë¥˜ì ì„ ë¯¸ë¦¬ ìˆ˜ì§‘
    forced_segment_starts: Set[str] = set()
    
    # [Performance Optimization] Thread-safe ID ìƒì„±ê¸°
    class ThreadSafeIdGenerator:
        def __init__(self): 
            self._val = -1
            self._lock = threading.Lock()
        
        def next(self) -> int:
            with self._lock:
                self._val += 1
                return self._val
        
        @property
        def current(self) -> int:
            return self._val
    
    seg_id_gen = ThreadSafeIdGenerator()
    stats = {"llm": 0, "hitp": 0, "parallel_groups": 0, "branches": 0}
    
    # --- Helper: í•©ë¥˜ ì§€ì (Convergence Node) ì°¾ê¸° ---
    def find_convergence_node(start_nodes: List[str]) -> Optional[str]:
        """
        ë¸Œëœì¹˜ë“¤ì´ ê³µí†µìœ¼ë¡œ ë„ë‹¬í•˜ëŠ” ì²« ë²ˆì§¸ Merge Pointë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        in-degree > 1ì¸ ë…¸ë“œë¥¼ í›„ë³´ë¡œ ë´…ë‹ˆë‹¤.
        
        [Critical Fix #2] ì°¾ì€ í•©ë¥˜ì ì€ forced_segment_startsì— ë“±ë¡ë˜ì–´
        ë°˜ë“œì‹œ ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ì˜ ì‹œì‘ì ì´ ë©ë‹ˆë‹¤.
        """
        queue = list(start_nodes)
        seen = set(queue)
        
        while queue:
            node_id = queue.pop(0)
            # Merge Point í›„ë³´ í™•ì¸
            if len(incoming_edges.get(node_id, [])) > 1:
                if node_id not in start_nodes:
                    # [Critical Fix #2] í•©ë¥˜ì ì€ ë°˜ë“œì‹œ ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ì 
                    forced_segment_starts.add(node_id)
                    logger.debug(f"Convergence node registered as forced segment start: {node_id}")
                    return node_id
            
            for out_edge in outgoing_edges.get(node_id, []):
                target = out_edge.get("target")
                if target and target not in seen:
                    seen.add(target)
                    queue.append(target)
        return None
    
    # --- [Critical Fix] ìœ„ìƒ ì •ë ¬ í—¬í¼ ---
    def _topological_sort_nodes(nodes_map: Dict[str, Any], edges_list: List[Dict]) -> List[Dict[str, Any]]:
        """
        ì„¸ê·¸ë¨¼íŠ¸ ë‚´ ë…¸ë“œë“¤ì„ ìœ„ìƒ ì •ë ¬í•˜ì—¬ ì‹¤í–‰ ìˆœì„œëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        DynamicWorkflowBuilderëŠ” nodes[0]ì„ entry pointë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ,
        ì²« ë²ˆì§¸ ë…¸ë“œê°€ ì‹¤ì œ ì‹œì‘ ë…¸ë“œê°€ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
        
        Args:
            nodes_map: {node_id: node_config} ë§¤í•‘
            edges_list: ì„¸ê·¸ë¨¼íŠ¸ ë‚´ë¶€ ì—£ì§€ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ìœ„ìƒ ì •ë ¬ëœ ë…¸ë“œ ì„¤ì • ë¦¬ìŠ¤íŠ¸
        """
        if len(nodes_map) <= 1:
            return list(nodes_map.values())
        
        # ì„¸ê·¸ë¨¼íŠ¸ ë‚´ ë…¸ë“œ ID ì§‘í•©
        node_ids = set(nodes_map.keys())
        
        # ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ ë° ì§„ì… ì°¨ìˆ˜(in-degree) ê³„ì‚°
        in_degree = {nid: 0 for nid in node_ids}
        adj = {nid: [] for nid in node_ids}
        
        for edge in edges_list:
            src = edge.get("source")
            tgt = edge.get("target")
            if src in node_ids and tgt in node_ids:
                adj[src].append(tgt)
                in_degree[tgt] += 1
        
        # Kahn's Algorithm: ì§„ì… ì°¨ìˆ˜ê°€ 0ì¸ ë…¸ë“œë¶€í„° ì‹œì‘
        queue = [nid for nid in node_ids if in_degree[nid] == 0]
        sorted_ids = []
        
        while queue:
            # ì•ˆì •ì ì¸ ìˆœì„œë¥¼ ìœ„í•´ ì •ë ¬ (ì•ŒíŒŒë²³ ìˆœ)
            queue.sort()
            node_id = queue.pop(0)
            sorted_ids.append(node_id)
            
            for neighbor in adj[node_id]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)
        
        # ì •ë ¬ë˜ì§€ ì•Šì€ ë…¸ë“œê°€ ìˆìœ¼ë©´ (ì‚¬ì´í´ ë˜ëŠ” ì—°ê²° ì•ˆë¨) ì›ë˜ ìˆœì„œë¡œ ì¶”ê°€
        if len(sorted_ids) < len(node_ids):
            remaining = [nid for nid in nodes_map.keys() if nid not in sorted_ids]
            logger.warning(f"Some nodes not topologically sorted, appending in original order: {remaining}")
            sorted_ids.extend(remaining)
        
        result = [nodes_map[nid] for nid in sorted_ids]
        logger.debug(f"Topological sort result: {[n.get('id') for n in result]}")
        return result
    
    # --- Segment ìƒì„± í—¬í¼ ---
    def create_segment(nodes_map, edges_list, s_type="normal", override_id=None, config=None):
        # ğŸ›¡ï¸ [v2.6 P0 Fix] 'code' íƒ€ì… ê°•ì œ ì •ì • - ValueError ë°©ì§€
        # ìƒìœ„ ë ˆì´ì–´(í”„ë¡ íŠ¸ì—”ë“œ, DB ë“±)ì—ì„œ ì˜ëª»ëœ íƒ€ì…ì´ ë“¤ì–´ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œ êµì •
        for node_id, node in nodes_map.items():
            if isinstance(node, dict) and node.get("type") == "code":
                logger.warning(
                    f"ğŸ›¡ï¸ [Kernel Defense] Fixing 'code' type to 'operator' for node {node_id} "
                    f"in partition_service.create_segment"
                )
                node["type"] = "operator"
        
        # ì„¸ê·¸ë¨¼íŠ¸ ë‚´ë¶€ ì—£ì§€ ì¶”ê°€
        if config:
            all_edges = config.get("edges", [])
            for edge in all_edges:
                source = edge.get("source")
                target = edge.get("target")
                if source in nodes_map and target in nodes_map:
                    if edge not in edges_list:  # ì¤‘ë³µ ë°©ì§€
                        edges_list.append(edge)
        
        # [Critical Fix] ë…¸ë“œ ìˆœì„œë¥¼ ìœ„ìƒ ì •ë ¬í•˜ì—¬ ì²« ë²ˆì§¸ ë…¸ë“œê°€ ì‹¤ì œ ì‹œì‘ ë…¸ë“œê°€ ë˜ë„ë¡ ë³´ì¥
        # DynamicWorkflowBuilderëŠ” nodes[0]ì„ entry pointë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ ìˆœì„œê°€ ì¤‘ìš”í•¨
        sorted_nodes = _topological_sort_nodes(nodes_map, edges_list)
        
        final_type = s_type
        if s_type == "normal":
            if any(n.get("hitp") in [True, "true"] for n in nodes_map.values()):
                final_type = "hitp"
        
        if final_type == "llm": 
            stats["llm"] += 1
        elif final_type == "hitp": 
            stats["hitp"] += 1
            
        return {
            "id": override_id if override_id is not None else seg_id_gen.next(),
            "nodes": sorted_nodes,  # [Critical Fix] ìœ„ìƒ ì •ë ¬ëœ ë…¸ë“œ ì‚¬ìš©
            "edges": list(edges_list),
            "type": final_type,
            "node_ids": [n["id"] for n in sorted_nodes]  # ì •ë ¬ëœ ìˆœì„œ ë°˜ì˜
        }
    
    # --- ì¬ê·€ì  íŒŒí‹°ì…”ë‹ ë¡œì§ ---
    visited_nodes: Set[str] = set()
    
    def run_partitioning(
        start_node_ids: List[str], 
        stop_at_nodes: Set[str] = None, 
        config=None,
        depth: int = 0  # [Critical Fix #1] ì¬ê·€ ê¹Šì´ ì¶”ì 
    ) -> List[Dict[str, Any]]:
        """
        ì¬ê·€ì  íŒŒí‹°ì…”ë‹ ë¡œì§.
        
        [Critical Fix #1] depth íŒŒë¼ë¯¸í„°ë¡œ ì¬ê·€ ê¹Šì´ ì œí•œ
        [Critical Fix #2] forced_segment_startsë¡œ í•©ë¥˜ì  ê°•ì œ ë¶„í• 
        """
        # [Critical Fix #1] ì¬ê·€ ê¹Šì´ ì œí•œ ê²€ì‚¬
        if depth > MAX_PARTITION_DEPTH:
            raise PartitionDepthExceededError(depth, MAX_PARTITION_DEPTH)
        
        local_segments = []
        local_current_nodes = {}
        local_current_edges = []
        queue = list(start_node_ids)
        
        # [Critical Fix #1] ë¬´í•œ ë£¨í”„ ë°©ì§€ìš© ë°˜ë³µ ì¹´ìš´í„°
        max_iterations = len(nodes) * 2  # ì•ˆì „ ë§ˆì§„
        iteration_count = 0
        
        def flush_local(seg_type="normal"):
            nonlocal local_current_nodes, local_current_edges
            if local_current_nodes or local_current_edges:
                seg = create_segment(local_current_nodes, local_current_edges, seg_type, config=config)
                local_segments.append(seg)
                local_current_nodes = {}
                local_current_edges = []
        
        while queue:
            # [Critical Fix #1] ë¬´í•œ ë£¨í”„ ë°©ì§€
            iteration_count += 1
            if iteration_count > max_iterations:
                logger.error(
                    f"Partition iteration limit exceeded ({max_iterations}). "
                    f"Possible infinite loop. Queue: {queue[:5]}..."
                )
                raise PartitionDepthExceededError(iteration_count, max_iterations)
            
            node_id = queue.pop(0)
            
            # Stop Condition
            if node_id in visited_nodes: 
                continue
            if stop_at_nodes and node_id in stop_at_nodes: 
                continue
            
            # [Safety] ë…¸ë“œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if node_id not in nodes:
                logger.warning(f"Node '{node_id}' referenced but not found in nodes map. Skipping.")
                continue
            
            node = nodes[node_id]
            
            # íŠ¸ë¦¬ê±° ì¡°ê±´ ê³„ì‚°
            in_edges = incoming_edges.get(node_id, [])
            non_hitp_in = [e for e in in_edges if e.get("type") not in HITP_EDGE_TYPES]
            
            is_hitp_start = any(e.get("type") in HITP_EDGE_TYPES for e in in_edges)
            is_llm = node.get("type") in LLM_NODE_TYPES
            is_merge = len(non_hitp_in) > 1
            is_branch = len(outgoing_edges.get(node_id, [])) > 1
            
            # ğŸ›¡ï¸ [v3.8] ì¸ë¼ì¸ parallel_group ë…¸ë“œ ê°ì§€
            # ë…¸ë“œ ìì²´ê°€ type="parallel_group"ì´ê³  branchesë¥¼ í¬í•¨í•˜ëŠ” ê²½ìš°
            is_inline_parallel = (
                node.get("type") == "parallel_group" and 
                isinstance(node.get("branches"), list) and
                len(node.get("branches", [])) > 0
            )
            
            # [Critical Fix #2] í•©ë¥˜ì ì€ ë°˜ë“œì‹œ ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘
            is_forced_start = node_id in forced_segment_starts
            
            # ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  íŠ¸ë¦¬ê±° (is_forced_start ì¶”ê°€)
            if (is_hitp_start or is_llm or is_merge or is_branch or is_forced_start or is_inline_parallel) and local_current_nodes:
                if node_id not in local_current_nodes:
                    flush_local("normal")
            
            # ğŸ›¡ï¸ [v3.8] ì¸ë¼ì¸ parallel_group ë…¸ë“œ ì²˜ë¦¬ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
            if is_inline_parallel:
                flush_local("normal")  # í˜„ì¬ê¹Œì§€ ì €ì¥
                visited_nodes.add(node_id)
                
                # ì¸ë¼ì¸ branchesë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                inline_branches = node.get("branches", [])
                branches_data = []
                
                for i, branch in enumerate(inline_branches):
                    if branch is None:
                        logger.warning(f"ğŸ›¡ï¸ [Self-Healing] Skipping None branch in inline parallel_group {node_id}")
                        continue
                    
                    branch_id = branch.get("id", f"B{i}")
                    branch_nodes = branch.get("nodes", [])
                    branch_edges = branch.get("edges", [])
                    
                    # ë¸Œëœì¹˜ ë‚´ë¶€ë¥¼ ì„œë¸Œ íŒŒí‹°ì…˜ìœ¼ë¡œ ì²˜ë¦¬
                    branch_partition = []
                    if branch_nodes:
                        # ë¸Œëœì¹˜ ë‚´ë¶€ ë…¸ë“œë“¤ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
                        branch_nodes_map = {}
                        for bn in branch_nodes:
                            if bn is not None and isinstance(bn, dict) and "id" in bn:
                                branch_nodes_map[bn["id"]] = bn
                        
                        if branch_nodes_map:
                            branch_seg = create_segment(
                                branch_nodes_map, 
                                branch_edges, 
                                "normal",
                                config={"nodes": branch_nodes, "edges": branch_edges}
                            )
                            branch_partition.append(branch_seg)
                    
                    branch_data = {
                        "branch_id": branch_id,
                        "partition_map": branch_partition,
                        "has_end": False,
                        "target_node": branch_nodes[0].get("id") if branch_nodes else None
                    }
                    branches_data.append(branch_data)
                    stats["branches"] += 1
                
                # Parallel Group ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
                if branches_data:
                    stats["parallel_groups"] += 1
                    p_seg_id = seg_id_gen.next()
                    parallel_seg = {
                        "id": p_seg_id,
                        "type": "parallel_group",
                        "branches": branches_data,
                        "node_ids": [node_id],
                        "branch_count": len(branches_data),
                        "resource_policy": node.get("resource_policy", {}),  # ì›ë³¸ resource_policy ë³´ì¡´
                        "label": node.get("label", "")
                    }
                    local_segments.append(parallel_seg)
                    
                    # Aggregator ìƒì„±
                    agg_seg_id = seg_id_gen.next()
                    aggregator_seg = {
                        "id": agg_seg_id,
                        "type": "aggregator",
                        "nodes": [],
                        "edges": [],
                        "node_ids": [],
                        "source_parallel_group": p_seg_id
                    }
                    local_segments.append(aggregator_seg)
                    
                    # next ì„¤ì •
                    parallel_seg["next_mode"] = "default"
                    parallel_seg["default_next"] = agg_seg_id
                
                # ë‹¤ìŒ ë…¸ë“œ íƒìƒ‰
                for out_edge in outgoing_edges.get(node_id, []):
                    tgt = out_edge.get("target")
                    if tgt and tgt not in visited_nodes and tgt not in queue:
                        if not (stop_at_nodes and tgt in stop_at_nodes):
                            queue.append(tgt)
                continue
            
            # ë³‘ë ¬ ê·¸ë£¹ ì²˜ë¦¬ (ê·¸ë˜í”„ ë¶„ê¸°ì  ê¸°ë°˜)
            if is_branch:
                flush_local("normal")  # í˜„ì¬ê¹Œì§€ ì €ì¥
                
                # ë¶„ê¸°ì  ë…¸ë“œ ì²˜ë¦¬
                seg = create_segment({node_id: node}, [], "normal", config=config) 
                local_segments.append(seg)
                visited_nodes.add(node_id)
                
                out_edges = outgoing_edges.get(node_id, [])
                branch_targets = [e.get("target") for e in out_edges if e.get("target")]
                
                # í•©ë¥˜ì  ì°¾ê¸° - [Critical Fix #2] í•©ë¥˜ì ì´ forced_segment_startsì— ë“±ë¡ë¨
                convergence_node = find_convergence_node(branch_targets)
                stop_set = {convergence_node} if convergence_node else set()
                
                # ê° ë¸Œëœì¹˜ ì‹¤í–‰
                branches_data = []
                for i, target in enumerate(branch_targets):
                    if target:
                        # [Critical Fix #1] ì¬ê·€ ê¹Šì´ ì „ë‹¬
                        branch_segs = run_partitioning(
                            [target], 
                            stop_at_nodes=stop_set, 
                            config=config,
                            depth=depth + 1
                        )
                        
                        # [Critical Fix #3] ë¸Œëœì¹˜ ì¢…ë£Œ ê²€ì¦
                        if branch_segs:
                            last_seg = branch_segs[-1]
                            # ë¸Œëœì¹˜ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                            branch_data = {
                                "branch_id": f"B{i}",
                                "partition_map": branch_segs,
                                "has_end": last_seg.get("next_mode") == "end",
                                "target_node": target
                            }
                            branches_data.append(branch_data)
                            stats["branches"] += 1
                        else:
                            # ë¹ˆ ë¸Œëœì¹˜ ê²½ê³ 
                            logger.warning(f"Branch {i} starting at {target} produced no segments")
                
                # Parallel Group ìƒì„±
                if branches_data:
                    stats["parallel_groups"] += 1
                    p_seg_id = seg_id_gen.next()
                    parallel_seg = {
                        "id": p_seg_id,
                        "type": "parallel_group",
                        "branches": branches_data,
                        "node_ids": [],
                        "branch_count": len(branches_data)  # [ì¶”ê°€] ë¸Œëœì¹˜ ìˆ˜ ë©”íƒ€ë°ì´í„°
                    }
                    local_segments.append(parallel_seg)
                    
                    # Aggregator ìƒì„±
                    agg_seg_id = seg_id_gen.next()
                    aggregator_seg = {
                        "id": agg_seg_id,
                        "type": "aggregator",
                        "nodes": [],
                        "edges": [],
                        "node_ids": [],
                        "convergence_node": convergence_node,  # í•©ë¥˜ ë…¸ë“œ ì €ì¥
                        "source_parallel_group": p_seg_id  # [ì¶”ê°€] ì›ë³¸ parallel_group ì°¸ì¡°
                    }
                    local_segments.append(aggregator_seg)
                    
                    # Parallel Groupì˜ next ì„¤ì •
                    parallel_seg["next_mode"] = "default"
                    parallel_seg["default_next"] = agg_seg_id
                
                # í•©ë¥˜ì ì´ ìˆë‹¤ë©´ íì— ì¶”ê°€
                if convergence_node and convergence_node not in visited_nodes:
                    queue.append(convergence_node)
                continue
            
            # ì¼ë°˜ ë…¸ë“œ ì²˜ë¦¬
            local_current_nodes[node_id] = node
            visited_nodes.add(node_id)
            
            # íŠ¹ìˆ˜ íƒ€ì… ì²˜ë¦¬
            if is_llm:
                flush_local("llm")
            elif is_hitp_start:
                flush_local("hitp")
            
            # ë‹¤ìŒ ë…¸ë“œ íƒìƒ‰
            for out_edge in outgoing_edges.get(node_id, []):
                tgt = out_edge.get("target")
                if tgt and tgt not in visited_nodes and tgt not in queue:
                    if not (stop_at_nodes and tgt in stop_at_nodes):
                        queue.append(tgt)
        
        flush_local()  # ë‚¨ì€ ê²ƒ ì²˜ë¦¬
        return local_segments
    
    # ì‹œì‘ ë…¸ë“œ ì°¾ê¸° ë° ì‹¤í–‰
    start_nodes = [nid for nid in nodes if not incoming_edges.get(nid)]
    if not start_nodes and nodes: 
        start_nodes = [list(nodes.keys())[0]]
    
    segments = run_partitioning(start_nodes, config=config)
    
    # --- Pass 2: ì¬ê·€ì  Node-to-Segment ë§¤í•‘ ---
    node_to_seg_map = {}
    
    def map_nodes_recursive(seg_list):
        for seg in seg_list:
            for nid in seg.get("node_ids", []):
                node_to_seg_map[nid] = seg["id"]
            
            if seg["type"] == "parallel_group":
                for branch in seg["branches"]:
                    map_nodes_recursive(branch["partition_map"])
    
    map_nodes_recursive(segments)
    
    # --- Next Mode ì„¤ì • (ì¬ê·€ì ) ---
    def process_links_recursive(seg_list: List[Dict[str, Any]], parent_aggregator_id: Optional[int] = None):
        """
        ì„¸ê·¸ë¨¼íŠ¸ ê°„ ì—°ê²°(next_mode) ì„¤ì •.
        
        [Critical Fix #3] ë¸Œëœì¹˜ ë‚´ë¶€ ì„¸ê·¸ë¨¼íŠ¸ê°€ ì˜¬ë°”ë¥´ê²Œ ì¢…ë£Œë˜ëŠ”ì§€ ê²€ì¦.
        parent_aggregator_idê°€ ì£¼ì–´ì§€ë©´, ë¸Œëœì¹˜ ë‚´ ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ì´ aggregatorë¡œ ì—°ê²°ë˜ì–´ì•¼ í•¨.
        """
        # Aggregator ì„¸ê·¸ë¨¼íŠ¸ë“¤ì˜ ID ì§‘í•©ì„ ë¯¸ë¦¬ íŒŒì•…
        aggregator_ids = {s["id"] for s in seg_list if s.get("type") == "aggregator"}
        
        for idx, seg in enumerate(seg_list):
            if seg["type"] == "parallel_group":
                # parallel_group ë‹¤ìŒì˜ aggregator ID ì°¾ê¸°
                next_agg_id = seg.get("default_next")
                
                for branch in seg["branches"]:
                    branch_segs = branch.get("partition_map", [])
                    
                    # [Critical Fix #3] ë¸Œëœì¹˜ ë‚´ë¶€ ì¬ê·€ ì²˜ë¦¬ - aggregator ID ì „ë‹¬
                    process_links_recursive(branch_segs, parent_aggregator_id=next_agg_id)
                    
                    # [Critical Fix #3] ë¸Œëœì¹˜ ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ê²€ì¦
                    if branch_segs:
                        last_branch_seg = branch_segs[-1]
                        
                        # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ê°€ ëª…ì‹œì  ENDê°€ ì•„ë‹ˆê³  nextê°€ ì—†ìœ¼ë©´
                        # aggregatorë¡œ ì•”ë¬µì  ì—°ê²° ì„¤ì •
                        if last_branch_seg.get("next_mode") == "end" and next_agg_id:
                            # ë¹„ëŒ€ì¹­ ë¸Œëœì¹˜: í•œ ìª½ì€ ëë‚˜ê³  ë‹¤ë¥¸ ìª½ì€ í•©ë¥˜
                            # aggregatorì—ì„œ ì´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                            last_branch_seg["implicit_aggregator_target"] = next_agg_id
                            branch["terminates_early"] = True
                            logger.debug(
                                f"Branch {branch['branch_id']} terminates early. "
                                f"Implicit aggregator target: {next_agg_id}"
                            )
                continue
            
            # Aggregatorì˜ ê²½ìš° convergence_nodeë¥¼ ì‚¬ìš©í•´ ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì—°ê²°
            if seg.get("type") == "aggregator":
                convergence_node = seg.get("convergence_node")
                if convergence_node and convergence_node in node_to_seg_map:
                    next_seg_id = node_to_seg_map[convergence_node]
                    seg["next_mode"] = "default"
                    seg["default_next"] = next_seg_id
                else:
                    # [Critical Fix #2] í•©ë¥˜ì ì´ ë§µì— ì—†ìœ¼ë©´ ê°•ì œë¡œ ì°¾ê¸° ì‹œë„
                    if convergence_node:
                        # í•©ë¥˜ì ì´ forced_segment_startsì— ìˆëŠ”ì§€ í™•ì¸
                        if convergence_node in forced_segment_starts:
                            logger.error(
                                f"Aggregator {seg['id']} has convergence node '{convergence_node}' "
                                f"which is a forced segment start but not mapped. "
                                f"This indicates a partitioning logic error."
                            )
                        else:
                            logger.warning(
                                f"Aggregator {seg['id']} has convergence node '{convergence_node}' "
                                f"but it is not mapped to any segment. Treating as workflow end."
                            )
                    
                    seg["next_mode"] = "end"
                    seg["default_next"] = None
                continue
            
            exit_edges = []
            for nid in seg.get("node_ids", []):
                for out_edge in outgoing_edges.get(nid, []):
                    tgt = out_edge.get("target")
                    if tgt and tgt in node_to_seg_map:
                        tgt_seg = node_to_seg_map[tgt]
                        
                        # íƒ€ê²Ÿì´ í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ì™€ ë‹¤ë¥´ê³ 
                        if tgt_seg != seg["id"]:
                            # ë§Œì•½ íƒ€ê²Ÿì´ Aggregatorë¼ë©´, ë¸Œëœì¹˜ ë‚´ë¶€ì—ì„œëŠ” ì´ë¥¼ ì—°ê²°í•˜ì§€ ì•ŠìŒ
                            # (ASL Map Stateê°€ ëë‚˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë„˜ì–´ê°€ë„ë¡ í•¨)
                            if tgt_seg in aggregator_ids:
                                continue 
                                
                            exit_edges.append({"edge": out_edge, "target_segment": tgt_seg})
            
            if not exit_edges:
                # [Critical Fix #3] ë¶€ëª¨ aggregatorê°€ ìˆìœ¼ë©´ ì•”ë¬µì  ì—°ê²°
                if parent_aggregator_id is not None:
                    seg["next_mode"] = "implicit_aggregator"
                    seg["default_next"] = None
                    seg["parent_aggregator"] = parent_aggregator_id
                else:
                    seg["next_mode"] = "end"
                    seg["default_next"] = None
            elif len(exit_edges) == 1:
                seg["next_mode"] = "default"
                seg["default_next"] = exit_edges[0]["target_segment"]
            else:
                seg["next_mode"] = "conditional"
                seg["branches"] = [
                    {"condition": e["edge"].get("condition", "default"), "next": e["target_segment"]}
                    for e in exit_edges
                ]
    
    process_links_recursive(segments)
    
    # --- ì¬ê·€ì  ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ ê³„ì‚° ---
    def count_segments_recursive(seg_list):
        total = 0
        for seg in seg_list:
            total += 1
            if seg.get("type") == "parallel_group":
                for branch in seg.get("branches", []):
                    total += count_segments_recursive(branch.get("partition_map", []))
        return total
    
    total_segments_recursive = count_segments_recursive(segments)
    
    # ğŸ›¡ï¸ [Critical Fix] Step Functions Loop Control requires Top-Level Count
    # total_segments returned here drives the main execution loop (0..N-1).
    # It must match len(partition_map), otherwise loop will try to access non-existent indices.
    execution_segments_count = len(segments)
    
    # ğŸ›¡ï¸ [P2 Fix] execution_segments_countê°€ 0ì´ë©´ ìµœì†Œ 1ë¡œ ë³´ì¥ (ë¹ˆ ì›Œí¬í”Œë¡œìš° ë°©ì–´)
    if execution_segments_count < 1:
        logger.warning(f"execution_segments_count calculated as {execution_segments_count}, forcing to 1")
        execution_segments_count = 1
    
    # [Performance Optimization] Pre-indexed ë©”íƒ€ë°ì´í„° ë°˜í™˜
    return {
        "partition_map": segments,
        "total_segments": execution_segments_count,  # [Fix] Use top-level count for execution loop
        "llm_segments": stats["llm"],
        "hitp_segments": stats["hitp"],
        # [v2.0] ì¶”ê°€ í†µê³„
        "parallel_groups": stats["parallel_groups"],
        "total_branches": stats["branches"],
        "forced_segment_starts": list(forced_segment_starts),
        # [Performance] Pre-indexed ë°ì´í„° (ì¬ì‚¬ìš© ê°€ëŠ¥)
        "node_to_segment_map": node_to_seg_map,
        "metadata": {
            "max_partition_depth": MAX_PARTITION_DEPTH,
            "max_nodes_limit": MAX_NODES_LIMIT,
            "nodes_processed": len(visited_nodes),
            "total_nodes": len(nodes),
            "total_segments_recursive": total_segments_recursive  # [Fix] Store recursive count in metadata
        }
    }


def lambda_handler(event: Dict[str, Any], context: Any = None) -> Dict[str, Any]:
    """
    PartitionWorkflow Lambda: ì›Œí¬í”Œë¡œìš°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    
    [v2.0 Production Hardening]
    - DAG ê²€ì¦ ì‹¤íŒ¨ ì‹œ ëª…í™•í•œ ì—ëŸ¬ ë°˜í™˜
    - ì¬ê·€ ê¹Šì´ ì´ˆê³¼ ì‹œ ì—ëŸ¬ ë°˜í™˜
    - ë…¸ë“œ ìˆ˜ ì œí•œ ì´ˆê³¼ ì‹œ ì—ëŸ¬ ë°˜í™˜
    
    Input event:
        - workflow_config: ë¶„í• í•  ì›Œí¬í”Œë¡œìš° ì„¤ì •
        - ownerId: ì†Œìœ ì ID (ë³´ì•ˆ/ë¡œê¹…ìš©)
        
    Output:
        - partition_result: partition_workflow_advanced() ê²°ê³¼
        - status: "success" | "error"
    """
    try:
        workflow_config = event.get("workflow_config")
        if not workflow_config:
            raise ValueError("workflow_config is required")
        
        owner_id = event.get("ownerId") or event.get("owner_id") or event.get("user_id")
        
        # ì›Œí¬í”Œë¡œìš° ë¶„í•  ì‹¤í–‰
        partition_result = partition_workflow_advanced(workflow_config)
        
        logger.info(
            "Partitioned workflow for owner=%s: %d total segments "
            "(%d LLM, %d HITP, %d parallel groups, %d branches)", 
            owner_id,
            partition_result["total_segments"],
            partition_result["llm_segments"], 
            partition_result["hitp_segments"],
            partition_result.get("parallel_groups", 0),
            partition_result.get("total_branches", 0)
        )
        
        # ğŸ›¡ï¸ [Critical Fix] ë°˜í™˜ êµ¬ì¡° í‰íƒ„í™” - Step Functions ASLì´ $.Payload.total_segmentsë¥¼ ì§ì ‘ ì°¸ì¡°í•  ìˆ˜ ìˆë„ë¡
        # ê¸°ì¡´: {"status": "success", "partition_result": {...}} â†’ ASLì—ì„œ $.Payload.partition_result.total_segmentsë¡œ ì ‘ê·¼ í•„ìš”
        # ìˆ˜ì •: {"status": "success", "total_segments": N, ...} â†’ ASLì—ì„œ $.Payload.total_segmentsë¡œ ì§ì ‘ ì ‘ê·¼ ê°€ëŠ¥
        return {
            "status": "success",
            **partition_result  # ğŸ›¡ï¸ ê²°ê³¼ë¥¼ í‰íƒ„í™”í•˜ì—¬ ASL ë§¤í•‘ ì˜¤ë¥˜ í•´ê²°
        }
    
    except CycleDetectedError as e:
        logger.error(f"Cycle detected in workflow: {e.cycle_path}")
        return {
            "status": "error",
            "error_type": "CycleDetectedError",
            "error_message": str(e),
            "cycle_path": e.cycle_path,
            "total_segments": 1,  # ğŸ›¡ï¸ [P0] ASL null ì°¸ì¡° ë°©ì§€
            "partition_map": []
        }
    
    except PartitionDepthExceededError as e:
        logger.error(f"Partition depth exceeded: {e.depth}/{e.max_depth}")
        return {
            "status": "error",
            "error_type": "PartitionDepthExceededError",
            "error_message": str(e),
            "depth": e.depth,
            "max_depth": e.max_depth,
            "total_segments": 1,  # ğŸ›¡ï¸ [P0] ASL null ì°¸ì¡° ë°©ì§€
            "partition_map": []
        }
    
    except ValueError as e:
        logger.error(f"Validation error: {e}")
        return {
            "status": "error",
            "error_type": "ValidationError",
            "error_message": str(e),
            "total_segments": 1,  # ğŸ›¡ï¸ [P0] ASL null ì°¸ì¡° ë°©ì§€
            "partition_map": []
        }
        
    except Exception as e:
        logger.exception("Failed to partition workflow")
        return {
            "status": "error",
            "error_type": type(e).__name__,
            "error_message": str(e),
            "total_segments": 1,  # ğŸ›¡ï¸ [P0] ASL null ì°¸ì¡° ë°©ì§€
            "partition_map": []
        }
