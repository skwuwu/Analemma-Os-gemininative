"""
DLQ Recovery Pessimistic Tests

Tests for Dead Letter Queue recovery functionality with emphasis on:
- Context Retention (preserve execution_id, owner_id)
- Idempotency (prevent duplicate data)
- Redrive Throttling (backoff during bulk recovery)
- Poison Pill Detection (prevent infinite retries)
- Error Type Filtering (Throttle vs Logic Error)

Reference: dlq_redrive_handler.py
"""

import pytest
import json
import sys
import os
import uuid
import time
from datetime import datetime, timezone
from unittest.mock import patch, MagicMock, Mock
from typing import Dict, Any, List

# Add backend to path
sys.path.insert(0, os.path.abspath("backend"))


# =============================================================================
# Mock Services for Testing
# =============================================================================

class MockDynamoDBTable:
    """Mock DynamoDB table for testing"""
    
    def __init__(self, items: Dict[str, Dict] = None):
        self.items = items or {}
        self.update_calls = []
        self.delete_calls = []
    
    def get_item(self, Key: Dict) -> Dict:
        key_value = list(Key.values())[0]
        item = self.items.get(key_value)
        return {"Item": item} if item else {}
    
    def update_item(self, **kwargs):
        self.update_calls.append(kwargs)
        key_value = list(kwargs["Key"].values())[0]
        # Simulate update
        if key_value in self.items:
            # Update would happen here
            pass
    
    def delete_item(self, Key: Dict):
        key_value = list(Key.values())[0]
        self.delete_calls.append(key_value)
        if key_value in self.items:
            del self.items[key_value]
    
    def query(self, **kwargs):
        # Simple mock for ownerId-index GSI
        owner_id = None
        if "KeyConditionExpression" in kwargs:
            # Extract owner_id from mock
            pass
        return {"Items": []}


class MockAPIGateway:
    """Mock API Gateway Management API for WebSocket"""
    
    def __init__(self, stale_connections: List[str] = None):
        self.stale_connections = stale_connections or []
        self.sent_messages = []
        self.post_call_count = 0
    
    def post_to_connection(self, ConnectionId: str, Data: str):
        self.post_call_count += 1
        
        if ConnectionId in self.stale_connections:
            from botocore.exceptions import ClientError
            raise ClientError(
                {"Error": {"Code": "GoneException", "Message": "Connection gone"}},
                "PostToConnection"
            )
        
        self.sent_messages.append({
            "connectionId": ConnectionId,
            "data": json.loads(Data)
        })


class MockStepFunctionsClient:
    """Mock Step Functions client for task token callbacks"""
    
    def __init__(self, expired_tokens: List[str] = None, throttled: bool = False):
        self.expired_tokens = expired_tokens or []
        self.throttled = throttled
        self.success_calls = []
        self.failure_calls = []
    
    def send_task_success(self, taskToken: str, output: str):
        if self.throttled:
            from botocore.exceptions import ClientError
            raise ClientError(
                {"Error": {"Code": "ThrottlingException", "Message": "Rate exceeded"}},
                "SendTaskSuccess"
            )
        
        if taskToken in self.expired_tokens:
            from botocore.exceptions import ClientError
            raise ClientError(
                {"Error": {"Code": "TaskTimedOut", "Message": "Task timed out"}},
                "SendTaskSuccess"
            )
        
        self.success_calls.append({"taskToken": taskToken, "output": output})
    
    def send_task_failure(self, taskToken: str, error: str, cause: str):
        if taskToken in self.expired_tokens:
            from botocore.exceptions import ClientError
            raise ClientError(
                {"Error": {"Code": "InvalidToken", "Message": "Token invalid"}},
                "SendTaskFailure"
            )
        
        self.failure_calls.append({"taskToken": taskToken, "error": error, "cause": cause})


# =============================================================================
# DLQ Message Type Detection Tests
# =============================================================================

class TestDLQMessageTypeDetection:
    """Test message type detection logic"""
    
    def test_detect_websocket_notification_type(self):
        """Detect WebSocket notification event type"""
        event = {
            "source": "aws.states",
            "detail": {
                "executionArn": "arn:aws:states:us-east-1:123456789:execution:MyMachine:exec-123"
            }
        }
        
        # Import and test
        from src.handlers.utils.dlq_redrive_handler import determine_message_type
        
        message_type = determine_message_type(event)
        assert message_type == "websocket_notification"
    
    def test_detect_task_token_callback_type(self):
        """Detect Task Token callback event type"""
        event = {
            "taskToken": "AQAEAAAAAAAA...",
            "result": {"output": "success"}
        }
        
        from src.handlers.utils.dlq_redrive_handler import determine_message_type
        
        message_type = determine_message_type(event)
        assert message_type == "task_token_callback"
    
    def test_detect_execution_update_type(self):
        """Detect execution update event type"""
        event = {
            "executionId": "exec-123",
            "status": "COMPLETED"
        }
        
        from src.handlers.utils.dlq_redrive_handler import determine_message_type
        
        message_type = determine_message_type(event)
        assert message_type == "execution_update"
    
    def test_unknown_message_type(self):
        """Handle unknown event type"""
        event = {"random": "data"}
        
        from src.handlers.utils.dlq_redrive_handler import determine_message_type
        
        message_type = determine_message_type(event)
        assert message_type == "unknown"


# =============================================================================
# Context Retention Tests (pessimistic reinforcement)
# =============================================================================

class TestDLQContextRetention:
    """
    ğŸ”´ Pessimistic reinforcement: Verify that messages recovered from DLQ maintain their original context
    
    Risk: If execution_id, owner_id, segment_index etc. are lost, recovery is impossible
    """
    
    def test_original_event_extraction_from_sqs_body(self):
        """
        Verify that the original event is correctly extracted from SQS body
        """
        # SQS record in the form stored in DLQ
        sqs_body = json.dumps({
            "originalEvent": {
                "execution_id": "exec-123",
                "segment_to_run": 5,
                "ownerId": "user-test-001",
                "workflowId": "wf-abc"
            },
            "error_info": "ThrottlingException"
        })
        
        # Extract original event
        body = json.loads(sqs_body)
        original_event = body.get("originalEvent", body)
        
        # Verify context preservation
        assert original_event["execution_id"] == "exec-123"
        assert original_event["ownerId"] == "user-test-001"
        assert original_event["segment_to_run"] == 5
    
    def test_nested_original_event_fallback(self):
        """
        Verify that when originalEvent key is missing, the entire body is used as the original event
        """
        # Some failures may be saved without originalEvent wrapping
        sqs_body = json.dumps({
            "executionId": "exec-456",
            "status": "FAILED"
        })
        
        body = json.loads(sqs_body)
        original_event = body.get("originalEvent", body)
        
        # Verify fallback operation
        assert original_event["executionId"] == "exec-456"
        assert original_event["status"] == "FAILED"
    
    def test_execution_arn_to_execution_id_extraction(self):
        """
        Verify that execution_id is correctly extracted from executionArn
        """
        execution_arn = "arn:aws:states:us-east-1:123456789012:execution:AnalemmaStateMachine:exec-unique-id-789"
        
        # Same as handler's extraction logic
        execution_id = execution_arn.split(":")[-1]
        
        assert execution_id == "exec-unique-id-789"


# =============================================================================
# Idempotency Tests (Idempotency)
# =============================================================================

class TestDLQIdempotency:
    """
    ğŸ”´ Pessimistic reinforcement: No duplicate data even if same message is reprocessed multiple times
    """
    
    def test_websocket_payload_includes_unique_message_id(self):
        """
        WebSocket payload includes unique messageId so client can filter duplicates
        """
        from src.handlers.utils.dlq_redrive_handler import create_websocket_payload
        
        event = {
            "detail": {
                "stateEnteredEventDetails": {
                    "name": "ProcessingState",
                    "output": {"progress": 50}
                }
            }
        }
        execution_info = {
            "executionId": "exec-123",
            "workflowId": "wf-abc"
        }
        
        payload1 = create_websocket_payload(event, execution_info)
        payload2 = create_websocket_payload(event, execution_info)
        
        # Each payload includes unique messageId
        assert "messageId" in payload1
        assert "messageId" in payload2
        # The messageId of the two payloads are different (UUID)
        assert payload1["messageId"] != payload2["messageId"]
    
    def test_execution_update_is_idempotent(self):
        """
        Execution update remains the same final state even if called multiple times
        """
        mock_table = MockDynamoDBTable({
            "exec-123": {
                "executionId": "exec-123",
                "status": "RUNNING",
                "updatedAt": 1000
            }
        })
        
        # Apply the same update twice
        update_event = {
            "executionId": "exec-123",
            "status": "COMPLETED",
            "updates": {"finalOutput": {"result": "success"}}
        }
        
        with patch.dict(os.environ, {
            "EXECUTIONS_TABLE": "test-table"
        }):
            with patch("src.handlers.utils.dlq_redrive_handler.executions_table", mock_table):
                from src.handlers.utils.dlq_redrive_handler import retry_execution_update
                
                result1 = retry_execution_update(update_event)
                result2 = retry_execution_update(update_event)
                
                # Both succeed
                assert result1 is True
                assert result2 is True
                # Update is called twice (idempotency guaranteed at DB level)
                assert len(mock_table.update_calls) == 2


# =============================================================================
# Poison Pill Detection Tests (Poison Apple Detection)
# =============================================================================

class TestDLQPoisonPillDetection:
    """
    ğŸ”´ Pessimistic Reinforcement: Prevent messages that can never succeed from infinite retries
    
    Risk: Resource waste if messages due to logic bugs keep returning to DLQ
    """
    
    @pytest.fixture
    def retry_count_tracker(self):
        """Retry count tracking utility"""
        class RetryTracker:
            MAX_RETRIES = 3
            
            def __init__(self):
                self.retry_counts = {}
            
            def should_retry(self, message_id: str, error_type: str) -> bool:
                """
                ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •
                
                ë¡œì§ ì—ëŸ¬ (ValueError, KeyError ë“±): ì¦‰ì‹œ ì¤‘ë‹¨
                ì¸í”„ë¼ ì—ëŸ¬ (Throttling, Timeout ë“±): ì¬ì‹œë„
                """
                # ë¡œì§ ì—ëŸ¬ëŠ” ì¬ì‹œë„í•´ë„ ì†Œìš©ì—†ìŒ
                if error_type in ["ValueError", "KeyError", "TypeError", "PermissionError"]:
                    return False
                
                # ì¸í”„ë¼ ì—ëŸ¬ëŠ” ì¬ì‹œë„ íšŸìˆ˜ í™•ì¸
                current_count = self.retry_counts.get(message_id, 0)
                if current_count >= self.MAX_RETRIES:
                    return False
                
                self.retry_counts[message_id] = current_count + 1
                return True
            
            def get_retry_count(self, message_id: str) -> int:
                return self.retry_counts.get(message_id, 0)
        
        return RetryTracker()
    
    def test_logic_error_not_retried(self, retry_count_tracker):
        """
        ë¡œì§ ì—ëŸ¬ (ValueError, KeyError)ëŠ” ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ
        """
        message_id = "msg-logic-error-001"
        
        # ValueErrorëŠ” ì¬ì‹œë„ ë¶ˆê°€
        assert retry_count_tracker.should_retry(message_id, "ValueError") is False
        assert retry_count_tracker.get_retry_count(message_id) == 0
        
        # KeyErrorë„ ì¬ì‹œë„ ë¶ˆê°€
        assert retry_count_tracker.should_retry("msg-002", "KeyError") is False
    
    def test_infrastructure_error_retried_with_limit(self, retry_count_tracker):
        """
        ì¸í”„ë¼ ì—ëŸ¬ (Throttling, Timeout)ëŠ” ìµœëŒ€ íšŸìˆ˜ê¹Œì§€ë§Œ ì¬ì‹œë„
        """
        message_id = "msg-throttle-001"
        
        # ì²« 3ë²ˆì€ ì¬ì‹œë„ í—ˆìš©
        assert retry_count_tracker.should_retry(message_id, "ThrottlingException") is True
        assert retry_count_tracker.should_retry(message_id, "ThrottlingException") is True
        assert retry_count_tracker.should_retry(message_id, "ThrottlingException") is True
        
        # 4ë²ˆì§¸ë¶€í„°ëŠ” ê±°ë¶€
        assert retry_count_tracker.should_retry(message_id, "ThrottlingException") is False
    
    def test_expired_token_is_not_retried(self):
        """
        ë§Œë£Œëœ Task Tokenì€ ì¬ì‹œë„í•´ë„ ì†Œìš©ì—†ìœ¼ë¯€ë¡œ ì„±ê³µ ì²˜ë¦¬í•˜ì—¬ DLQì—ì„œ ì œê±°
        """
        mock_sf = MockStepFunctionsClient(expired_tokens=["expired-token-123"])
        
        with patch("boto3.client", return_value=mock_sf):
            from src.handlers.utils.dlq_redrive_handler import retry_task_token_callback
            
            event = {
                "taskToken": "expired-token-123",
                "result": {"output": "test"}
            }
            
            # ë§Œë£Œëœ í† í°ì€ True ë°˜í™˜ (DLQì—ì„œ ì œê±°)
            result = retry_task_token_callback(event)
            assert result is True


# =============================================================================
# Redrive Throttling Tests (ëŒ€ëŸ‰ ë³µêµ¬ ì‹œ ë°±ì˜¤í”„)
# =============================================================================

class TestDLQRedriveThrottling:
    """
    ğŸ”´ ë¹„ê´€ì  ê°•í™”: ëŒ€ëŸ‰ ë©”ì‹œì§€ ë³µêµ¬ ì‹œ íƒ€ê²Ÿ ì‹œìŠ¤í…œì„ ë§ˆë¹„ì‹œí‚¤ì§€ ì•ŠìŒ
    """
    
    @pytest.fixture
    def throttled_redrive_service(self):
        """ì§€ìˆ˜ ë°±ì˜¤í”„ê°€ ì ìš©ëœ Redrive ì„œë¹„ìŠ¤"""
        import random
        
        class ThrottledRedriveService:
            BASE_DELAY = 0.1  # 100ms
            MAX_DELAY = 5.0   # 5ì´ˆ
            BATCH_SIZE = 10   # í•œ ë²ˆì— ì²˜ë¦¬í•  ë©”ì‹œì§€ ìˆ˜
            
            def __init__(self):
                self.processed_count = 0
                self.delays_applied = []
            
            def calculate_backoff(self, attempt: int) -> float:
                """ì§€ìˆ˜ ë°±ì˜¤í”„ ê³„ì‚°"""
                delay = min(self.BASE_DELAY * (2 ** attempt), self.MAX_DELAY)
                jitter = delay * random.uniform(0, 0.5)
                return delay + jitter
            
            def process_batch(self, messages: List[Dict], attempt: int = 0) -> Dict:
                """ë°°ì¹˜ ì²˜ë¦¬ (ë°±ì˜¤í”„ ì ìš©)"""
                if attempt > 0:
                    delay = self.calculate_backoff(attempt)
                    self.delays_applied.append(delay)
                    time.sleep(delay)  # ì‹¤ì œë¡œëŠ” ì§§ê²Œ
                
                success_count = 0
                failed_ids = []
                
                for i, msg in enumerate(messages):
                    if i < self.BATCH_SIZE:
                        # ë°°ì¹˜ í¬ê¸° ë‚´ ì²˜ë¦¬
                        self.processed_count += 1
                        if msg.get("will_fail"):
                            failed_ids.append(msg["id"])
                        else:
                            success_count += 1
                
                return {
                    "processed": success_count,
                    "failed": failed_ids,
                    "remaining": len(messages) - self.BATCH_SIZE if len(messages) > self.BATCH_SIZE else 0
                }
        
        return ThrottledRedriveService()
    
    def test_batch_processing_respects_size_limit(self, throttled_redrive_service):
        """
        ëŒ€ëŸ‰ ë©”ì‹œì§€ëŠ” ë°°ì¹˜ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        """
        # 30ê°œ ë©”ì‹œì§€ ìƒì„±
        messages = [{"id": f"msg-{i}"} for i in range(30)]
        
        result = throttled_redrive_service.process_batch(messages)
        
        # í•œ ë²ˆì— 10ê°œë§Œ ì²˜ë¦¬
        assert result["processed"] == 10
        assert result["remaining"] == 20
    
    def test_exponential_backoff_increases_delay(self, throttled_redrive_service):
        """
        ì¬ì‹œë„ ì‹œ ì§€ìˆ˜ì ìœ¼ë¡œ ì§€ì—° ì‹œê°„ ì¦ê°€
        """
        delays = [
            throttled_redrive_service.calculate_backoff(0),
            throttled_redrive_service.calculate_backoff(1),
            throttled_redrive_service.calculate_backoff(2),
            throttled_redrive_service.calculate_backoff(3),
        ]
        
        # ì§€ìˆ˜ì  ì¦ê°€ (ì§€í„° ë•Œë¬¸ì— ì •í™•í•˜ì§€ëŠ” ì•Šì§€ë§Œ ì¶”ì„¸ í™•ì¸)
        # delay[1] > delay[0], delay[2] > delay[1]
        assert delays[1] > delays[0] * 1.5  # ìµœì†Œ 1.5ë°° ì´ìƒ ì¦ê°€
        assert delays[2] > delays[1] * 1.5
        
        # ìµœëŒ€ê°’ ì œí•œ
        assert delays[3] <= throttled_redrive_service.MAX_DELAY * 1.5  # ì§€í„° í¬í•¨


# =============================================================================
# Batch Item Failures Tests (SQS ë¶€ë¶„ ì¬ì‹œë„)
# =============================================================================

class TestDLQBatchItemFailures:
    """
    SQS Lambda í†µí•©ì˜ batchItemFailures ì‘ë‹µ í˜•ì‹ ê²€ì¦
    """
    
    def test_partial_failure_returns_correct_format(self):
        """
        ì¼ë¶€ ë©”ì‹œì§€ë§Œ ì‹¤íŒ¨ ì‹œ ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ì‹¤íŒ¨ ë©”ì‹œì§€ ID ë°˜í™˜
        """
        # í˜¼í•©ëœ ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜
        records = [
            {"messageId": "msg-1", "body": json.dumps({"executionId": "exec-1", "status": "OK"})},
            {"messageId": "msg-2", "body": json.dumps({"invalid": "data"})},  # ì‹¤íŒ¨ ì˜ˆìƒ
            {"messageId": "msg-3", "body": json.dumps({"executionId": "exec-3", "status": "OK"})},
        ]
        
        batch_item_failures = []
        
        for record in records:
            try:
                body = json.loads(record["body"])
                if "executionId" not in body:
                    raise ValueError("Missing executionId")
            except Exception:
                batch_item_failures.append({"itemIdentifier": record["messageId"]})
        
        # msg-2ë§Œ ì‹¤íŒ¨
        assert len(batch_item_failures) == 1
        assert batch_item_failures[0]["itemIdentifier"] == "msg-2"
    
    def test_all_success_returns_empty_failures(self):
        """
        ëª¨ë“  ë©”ì‹œì§€ ì„±ê³µ ì‹œ ë¹ˆ ì‹¤íŒ¨ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        """
        batch_item_failures = []
        
        result = {"batchItemFailures": batch_item_failures}
        
        assert result["batchItemFailures"] == []
    
    def test_all_failure_returns_all_message_ids(self):
        """
        ëª¨ë“  ë©”ì‹œì§€ ì‹¤íŒ¨ ì‹œ ì „ì²´ ë©”ì‹œì§€ ID ë°˜í™˜
        """
        message_ids = ["msg-1", "msg-2", "msg-3"]
        
        batch_item_failures = [{"itemIdentifier": msg_id} for msg_id in message_ids]
        
        assert len(batch_item_failures) == 3


# =============================================================================
# Stale Connection Cleanup Tests
# =============================================================================

class TestDLQStaleConnectionCleanup:
    """
    WebSocket ì—°ê²° ëŠê¹€ ì‹œ ìë™ ì •ë¦¬
    """
    
    def test_gone_exception_triggers_connection_removal(self):
        """
        GoneException ë°œìƒ ì‹œ ì—°ê²°ì´ ìë™ ì‚­ì œë¨
        """
        mock_table = MockDynamoDBTable({
            "conn-stale-123": {"connectionId": "conn-stale-123", "ownerId": "user-1"}
        })
        
        with patch("src.handlers.utils.dlq_redrive_handler.connections_table", mock_table):
            from src.handlers.utils.dlq_redrive_handler import remove_stale_connection
            
            remove_stale_connection("conn-stale-123")
            
            # ì‚­ì œ í˜¸ì¶œ í™•ì¸
            assert "conn-stale-123" in mock_table.delete_calls


# =============================================================================
# Error Type Filtering Tests (ì—ëŸ¬ íƒ€ì…ë³„ í•„í„°ë§)
# =============================================================================

class TestDLQErrorTypeFiltering:
    """
    ğŸ”´ ë¹„ê´€ì  ê°•í™”: ì—ëŸ¬ íƒ€ì…ì— ë”°ë¥¸ Redrive ì •ì±…
    
    Throttling, Timeout, NetworkError â†’ ìë™ Redrive ëŒ€ìƒ
    ValueError, KeyError, PermissionError â†’ ìˆ˜ë™ ê²€í†  ëŒ€ìƒ
    """
    
    @pytest.fixture
    def error_classifier(self):
        """ì—ëŸ¬ ë¶„ë¥˜ê¸°"""
        class ErrorClassifier:
            # ìë™ Redrive ëŒ€ìƒ ì—ëŸ¬
            RETRIABLE_ERRORS = {
                "ThrottlingException",
                "ServiceUnavailableException",
                "InternalServerError",
                "TimeoutError",
                "NetworkError",
                "503 Slow Down",
                "TooManyRequestsException",
            }
            
            # ìˆ˜ë™ ê²€í†  ëŒ€ìƒ ì—ëŸ¬ (ë¡œì§ ë²„ê·¸)
            NON_RETRIABLE_ERRORS = {
                "ValueError",
                "KeyError",
                "TypeError",
                "PermissionError",
                "ValidationError",
                "AccessDeniedException",
            }
            
            def classify(self, error_type: str) -> str:
                """
                ì—ëŸ¬ ë¶„ë¥˜
                
                Returns:
                    "auto_retry": ìë™ Redrive
                    "manual_review": ìˆ˜ë™ ê²€í†  í•„ìš”
                    "unknown": ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬ (ê¸°ë³¸ê°’: 1íšŒ ì¬ì‹œë„ í›„ ìˆ˜ë™ ê²€í† )
                """
                if error_type in self.RETRIABLE_ERRORS:
                    return "auto_retry"
                elif error_type in self.NON_RETRIABLE_ERRORS:
                    return "manual_review"
                else:
                    return "unknown"
            
            def should_alert_admin(self, error_type: str, retry_count: int) -> bool:
                """ê´€ë¦¬ì ì•Œë¦¼ í•„ìš” ì—¬ë¶€"""
                classification = self.classify(error_type)
                
                # ìˆ˜ë™ ê²€í†  ëŒ€ìƒì€ ì¦‰ì‹œ ì•Œë¦¼
                if classification == "manual_review":
                    return True
                
                # ìë™ ì¬ì‹œë„ë„ 3íšŒ ì´ˆê³¼ ì‹œ ì•Œë¦¼
                if retry_count > 3:
                    return True
                
                return False
        
        return ErrorClassifier()
    
    def test_throttling_error_classified_as_auto_retry(self, error_classifier):
        """Throttling ì—ëŸ¬ëŠ” ìë™ ì¬ì‹œë„ ëŒ€ìƒ"""
        assert error_classifier.classify("ThrottlingException") == "auto_retry"
        assert error_classifier.classify("ServiceUnavailableException") == "auto_retry"
        assert error_classifier.classify("503 Slow Down") == "auto_retry"
    
    def test_logic_error_classified_as_manual_review(self, error_classifier):
        """ë¡œì§ ì—ëŸ¬ëŠ” ìˆ˜ë™ ê²€í†  ëŒ€ìƒ"""
        assert error_classifier.classify("ValueError") == "manual_review"
        assert error_classifier.classify("KeyError") == "manual_review"
        assert error_classifier.classify("PermissionError") == "manual_review"
    
    def test_admin_alert_on_logic_error(self, error_classifier):
        """ë¡œì§ ì—ëŸ¬ ë°œìƒ ì‹œ ì¦‰ì‹œ ê´€ë¦¬ì ì•Œë¦¼"""
        assert error_classifier.should_alert_admin("ValueError", retry_count=0) is True
        assert error_classifier.should_alert_admin("KeyError", retry_count=0) is True
    
    def test_admin_alert_after_max_retries(self, error_classifier):
        """ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ì‹œ ê´€ë¦¬ì ì•Œë¦¼"""
        # 3íšŒê¹Œì§€ëŠ” ì•Œë¦¼ ì—†ìŒ
        assert error_classifier.should_alert_admin("ThrottlingException", retry_count=1) is False
        assert error_classifier.should_alert_admin("ThrottlingException", retry_count=3) is False
        
        # 4íšŒë¶€í„° ì•Œë¦¼
        assert error_classifier.should_alert_admin("ThrottlingException", retry_count=4) is True


# =============================================================================
# Integration Test: Full DLQ Redrive Flow
# =============================================================================

class TestDLQRedriveIntegration:
    """
    ì „ì²´ DLQ Redrive íë¦„ í†µí•© í…ŒìŠ¤íŠ¸
    """
    
    def test_full_redrive_flow_preserves_context(self):
        """
        ğŸ”´ ë¹„ê´€ì  í…ŒìŠ¤íŠ¸: API í­ì£¼ë¡œ ì¸í•œ ì›Œì»¤ ì§‘ë‹¨ íì‚¬ ë° ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤
        
        ìƒí™©: 100ê°œì˜ ë§µ ì›Œì»¤ ì¤‘ 30ê°œê°€ Bedrock í• ë‹¹ëŸ‰ ì´ˆê³¼ë¡œ ì‹¤íŒ¨í•˜ì—¬ DLQì— ìŒ“ì„
        ê²€ì¦: ë³µêµ¬ ì‹œ ì›ë˜ì˜ execution_id, owner_id, segment_index ìœ ì§€
        """
        # DLQì— ì €ì¥ëœ í˜•íƒœì˜ SQS ì´ë²¤íŠ¸
        sqs_event = {
            "Records": [{
                "messageId": f"msg-{i}",
                "body": json.dumps({
                    "originalEvent": {
                        "execution_id": f"exec-batch-{i}",
                        "segment_to_run": i % 10,
                        "ownerId": "user-batch-test",
                        "workflowId": "wf-multimodal"
                    },
                    "error_info": {
                        "error": "ThrottlingException",
                        "cause": "Bedrock TPS exceeded"
                    }
                }),
                "messageAttributes": {"RetryCount": {"stringValue": str(i % 3 + 1)}}
            } for i in range(30)]
        }
        
        # ëª¨ë“  ë ˆì½”ë“œì˜ ì»¨í…ìŠ¤íŠ¸ ê²€ì¦
        for record in sqs_event["Records"]:
            body = json.loads(record["body"])
            original_event = body.get("originalEvent", body)
            
            # í•„ìˆ˜ ì»¨í…ìŠ¤íŠ¸ í•„ë“œ ì¡´ì¬
            assert "execution_id" in original_event
            assert "ownerId" in original_event
            assert "segment_to_run" in original_event
            
            # ê°’ì´ ì˜¬ë°”ë¥¸ í˜•ì‹
            assert original_event["execution_id"].startswith("exec-batch-")
            assert original_event["ownerId"] == "user-batch-test"
            assert 0 <= original_event["segment_to_run"] < 10


# =============================================================================
# Clock Skew Tests (íƒ€ì„ìŠ¤íƒ¬í”„ ì—­ì „ ë°©ì§€)
# =============================================================================

class TestDLQClockSkew:
    """
    ğŸ”´ ë¹„ê´€ì  ê°•í™”: DLQì—ì„œ ë³µêµ¬ëœ ë©”ì‹œì§€ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ì—­ì „ ë°©ì§€
    
    Risk: DLQì—ì„œ ë³µêµ¬ëœ ì˜¤ë˜ëœ ë©”ì‹œì§€ê°€ ìµœì‹  ìƒíƒœë¥¼ ë®ì–´ì“¸ ìˆ˜ ìˆìŒ
    """
    
    @pytest.fixture
    def timestamp_protected_updater(self):
        """íƒ€ì„ìŠ¤íƒ¬í”„ ë³´í˜¸ ê¸°ëŠ¥ì´ ìˆëŠ” ì—…ë°ì´í„°"""
        class TimestampProtectedUpdater:
            def __init__(self):
                self.updates = []
                self.rejections = []
            
            def update_with_timestamp_guard(
                self, 
                execution_id: str, 
                new_data: Dict, 
                new_timestamp: int,
                current_timestamp: int = None
            ) -> Dict:
                """
                íƒ€ì„ìŠ¤íƒ¬í”„ ì¡°ê±´ë¶€ ì—…ë°ì´íŠ¸
                
                DynamoDBì˜ ConditionExpressionê³¼ ë™ì¼í•œ ë¡œì§:
                í˜„ì¬ ì €ì¥ëœ updatedAtë³´ë‹¤ ìƒˆë¡œìš´ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
                """
                if current_timestamp is not None and new_timestamp <= current_timestamp:
                    self.rejections.append({
                        "execution_id": execution_id,
                        "new_timestamp": new_timestamp,
                        "current_timestamp": current_timestamp,
                        "reason": "timestamp_inversion"
                    })
                    return {
                        "updated": False,
                        "reason": "New timestamp is not newer than current"
                    }
                
                self.updates.append({
                    "execution_id": execution_id,
                    "data": new_data,
                    "timestamp": new_timestamp
                })
                
                return {"updated": True}
            
            def generate_condition_expression(self) -> str:
                """
                DynamoDB ConditionExpression ìƒì„±
                
                Expected ì¡°ê±´: í˜„ì¬ updatedAtì´ ì—†ê±°ë‚˜, newTimestampë³´ë‹¤ ì‘ì€ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
                """
                return "attribute_not_exists(updatedAt) OR updatedAt < :newTimestamp"
        
        return TimestampProtectedUpdater()
    
    def test_old_message_does_not_overwrite_new_data(self, timestamp_protected_updater):
        """
        ğŸ”´ ì˜¤ë˜ëœ DLQ ë©”ì‹œì§€ê°€ ìµœì‹  ë°ì´í„°ë¥¼ ë®ì–´ì“°ì§€ ì•ŠìŒ
        
        ì‹œë‚˜ë¦¬ì˜¤: 
        - ë©”ì‹œì§€ Aê°€ 1ì›” 1ì¼ì— ì‹¤íŒ¨í•˜ì—¬ DLQë¡œ ì´ë™
        - ë©”ì‹œì§€ Bê°€ 1ì›” 2ì¼ì— ê°™ì€ ì‹¤í–‰ì„ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ
        - 1ì›” 3ì¼ì— DLQ Redriveê°€ ë©”ì‹œì§€ Aë¥¼ ë³µêµ¬ ì‹œë„
        - ê²°ê³¼: ë©”ì‹œì§€ Aì˜ ì˜¤ë˜ëœ ìƒíƒœëŠ” ë¬´ì‹œë˜ì–´ì•¼ í•¨
        """
        execution_id = "exec-clock-test-001"
        
        # í˜„ì¬ ì €ì¥ëœ ìƒíƒœ (1ì›” 2ì¼ì— ì—…ë°ì´íŠ¸ë¨)
        current_timestamp = 1704153600  # 2024-01-02
        
        # DLQì—ì„œ ë³µêµ¬ëœ ì˜¤ë˜ëœ ë©”ì‹œì§€ (1ì›” 1ì¼ ë°ì´í„°)
        old_message_timestamp = 1704067200  # 2024-01-01
        old_data = {"status": "FAILED", "error": "Throttling"}
        
        result = timestamp_protected_updater.update_with_timestamp_guard(
            execution_id=execution_id,
            new_data=old_data,
            new_timestamp=old_message_timestamp,
            current_timestamp=current_timestamp
        )
        
        # ì—…ë°ì´íŠ¸ ê±°ë¶€ë¨
        assert result["updated"] is False
        assert len(timestamp_protected_updater.rejections) == 1
        assert timestamp_protected_updater.rejections[0]["reason"] == "timestamp_inversion"
    
    def test_new_message_updates_old_data(self, timestamp_protected_updater):
        """
        ìƒˆë¡œìš´ ë©”ì‹œì§€ëŠ” ì •ìƒì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨
        """
        execution_id = "exec-clock-test-002"
        
        current_timestamp = 1704067200  # 2024-01-01
        new_timestamp = 1704153600  # 2024-01-02 (ë” ìµœì‹ )
        new_data = {"status": "COMPLETED"}
        
        result = timestamp_protected_updater.update_with_timestamp_guard(
            execution_id=execution_id,
            new_data=new_data,
            new_timestamp=new_timestamp,
            current_timestamp=current_timestamp
        )
        
        # ì—…ë°ì´íŠ¸ ì„±ê³µ
        assert result["updated"] is True
        assert len(timestamp_protected_updater.updates) == 1
    
    def test_condition_expression_format(self, timestamp_protected_updater):
        """
        DynamoDB ConditionExpressionì´ ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ì§€ í™•ì¸
        """
        expr = timestamp_protected_updater.generate_condition_expression()
        
        # í•„ìˆ˜ ì¡°ê±´ í¬í•¨
        assert "attribute_not_exists" in expr or "updatedAt" in expr
        assert "<" in expr  # íƒ€ì„ìŠ¤íƒ¬í”„ ë¹„êµ


# =============================================================================
# IAM Permission Alignment Tests (ê¶Œí•œ ì†Œì™¸ ë°©ì§€)
# =============================================================================

class TestDLQIAMPermissionAlignment:
    """
    ğŸ”´ ë¹„ê´€ì  ê°•í™”: DLQ Redrive í•¸ë“¤ëŸ¬ê°€ í•„ìš”í•œ ëª¨ë“  ê¶Œí•œì„ ê°€ì§€ëŠ”ì§€ í™•ì¸
    
    Risk: segment_runnerê°€ ì‚¬ìš©í•˜ëŠ” ë¦¬ì†ŒìŠ¤(S3, DynamoDB, Bedrock)ì— ëŒ€í•œ 
          ê¶Œí•œì´ dlq_redrive_handlerì— ì—†ìœ¼ë©´ ë³µêµ¬ ì‹¤íŒ¨
    """
    
    @pytest.fixture
    def required_permissions(self):
        """DLQ Redriveì— í•„ìš”í•œ ê¶Œí•œ ëª©ë¡"""
        return {
            # DynamoDB ê¶Œí•œ
            "dynamodb:GetItem",
            "dynamodb:UpdateItem",
            "dynamodb:DeleteItem",
            "dynamodb:Query",
            # Step Functions ê¶Œí•œ
            "states:SendTaskSuccess",
            "states:SendTaskFailure",
            # API Gateway (WebSocket) ê¶Œí•œ
            "execute-api:ManageConnections",
            # (ì„ íƒ) S3 ê¶Œí•œ - ìƒíƒœ ë³µêµ¬ ì‹œ í•„ìš”í•  ìˆ˜ ìˆìŒ
            "s3:GetObject",
            "s3:PutObject",
            # CloudWatch Logs ê¶Œí•œ
            "logs:CreateLogStream",
            "logs:PutLogEvents",
        }
    
    @pytest.fixture
    def segment_runner_permissions(self):
        """segment_runnerê°€ ì‚¬ìš©í•˜ëŠ” ê¶Œí•œ"""
        return {
            "dynamodb:GetItem",
            "dynamodb:UpdateItem",
            "dynamodb:PutItem",
            "dynamodb:Query",
            "s3:GetObject",
            "s3:PutObject",
            "states:SendTaskSuccess",
            "states:SendTaskFailure",
            "bedrock:InvokeModel",
            "execute-api:ManageConnections",
            "logs:CreateLogStream",
            "logs:PutLogEvents",
        }
    
    def test_dlq_handler_has_all_required_permissions(self, required_permissions):
        """
        DLQ í•¸ë“¤ëŸ¬ì— í•„ìš”í•œ ëª¨ë“  ê¶Œí•œì´ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        """
        # í•µì‹¬ ê¶Œí•œ ê²€ì¦
        essential_permissions = {
            "dynamodb:GetItem",  # ì‹¤í–‰ ì •ë³´ ì¡°íšŒ
            "dynamodb:UpdateItem",  # ìƒíƒœ ì—…ë°ì´íŠ¸
            "states:SendTaskSuccess",  # Task Token ì½œë°±
            "states:SendTaskFailure",
        }
        
        assert essential_permissions.issubset(required_permissions)
    
    def test_dlq_permissions_cover_segment_runner_resources(
        self, 
        required_permissions, 
        segment_runner_permissions
    ):
        """
        DLQ í•¸ë“¤ëŸ¬ ê¶Œí•œì´ segment_runnerê°€ ì‚¬ìš©í•˜ëŠ” ë¦¬ì†ŒìŠ¤ë¥¼ ì»¤ë²„í•˜ëŠ”ì§€ í™•ì¸
        
        Redrive ì‹œ segment_runnerê°€ ì‹¤íŒ¨í•œ ì‘ì—…ì„ ì¬ì‹¤í–‰í•˜ë ¤ë©´
        ë™ì¼í•œ ë¦¬ì†ŒìŠ¤ì— ì ‘ê·¼í•  ìˆ˜ ìˆì–´ì•¼ í•¨
        """
        # ê³µí†µ ë¦¬ì†ŒìŠ¤ì— ëŒ€í•œ ê¶Œí•œ ë¹„êµ
        common_resources_permissions = {
            "dynamodb:GetItem",
            "dynamodb:UpdateItem",
            "states:SendTaskSuccess",
            "states:SendTaskFailure",
        }
        
        # DLQ í•¸ë“¤ëŸ¬ê°€ ìµœì†Œí•œ ì´ ê¶Œí•œë“¤ì€ ê°€ì ¸ì•¼ í•¨
        dlq_has_common = common_resources_permissions.issubset(required_permissions)
        runner_has_common = common_resources_permissions.issubset(segment_runner_permissions)
        
        assert dlq_has_common, "DLQ handler missing common permissions"
        assert runner_has_common, "Segment runner missing common permissions"
    
    def test_managed_policy_recommended_for_alignment(self):
        """
        ê¶Œí•œ ë™ê¸°í™”ë¥¼ ìœ„í•´ ê³µìœ  Managed Policy ì‚¬ìš© ê¶Œì¥
        """
        # SAM templateì—ì„œ ë™ì¼í•œ Managed Policyë¥¼ ì°¸ì¡°í•˜ë„ë¡ ê¶Œì¥
        shared_policy_name = "AnalemmaSharedExecutionPolicy"
        
        # í…ŒìŠ¤íŠ¸ëŠ” ê¶Œì¥ ì‚¬í•­ë§Œ ë¬¸ì„œí™”
        recommendation = {
            "policy_name": shared_policy_name,
            "includes": [
                "DynamoDB access for state tables",
                "Step Functions task token callbacks",
                "API Gateway WebSocket management",
                "CloudWatch Logs",
            ],
            "used_by": ["SegmentRunnerFunction", "DLQRedriveFunction"]
        }
        
        assert recommendation["policy_name"] == shared_policy_name


# =============================================================================
# Correlation ID Traceability Tests (ì¶”ì ì„± ë³´ì¥)
# =============================================================================

class TestDLQCorrelationIDTraceability:
    """
    ğŸ”´ ë¹„ê´€ì  ê°•í™”: DLQ Redrive ê³¼ì •ì—ì„œ ì¶”ì ì„± ìœ ì§€
    
    Risk: Redriveëœ ë©”ì‹œì§€ëŠ” ìƒˆë¡œìš´ SQS messageIdë¥¼ ê°€ì§€ë¯€ë¡œ 
          ì›ë˜ ì‹¤í–‰ê³¼ì˜ ì—°ê²°ê³ ë¦¬ê°€ ëŠì–´ì§ˆ ìˆ˜ ìˆìŒ
    """
    
    @pytest.fixture
    def correlation_aware_logger(self):
        """Correlation IDë¥¼ í¬í•¨í•˜ëŠ” ë¡œê±°"""
        class CorrelationAwareLogger:
            def __init__(self):
                self.log_entries = []
            
            def log(
                self, 
                level: str, 
                message: str, 
                correlation_id: str = None,
                execution_id: str = None,
                original_message_id: str = None,
                redrive_message_id: str = None
            ):
                """
                ì¶”ì  ì •ë³´ê°€ í¬í•¨ëœ ë¡œê·¸ ì¶œë ¥
                
                ëª¨ë“  ë¡œê·¸ì— correlation_idë¥¼ í¬í•¨í•˜ì—¬ 
                ì‹¤íŒ¨ â†’ DLQ â†’ ë³µêµ¬ ê³¼ì •ì„ ì¶”ì  ê°€ëŠ¥
                """
                entry = {
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "level": level,
                    "message": message,
                    "correlation_id": correlation_id or execution_id,
                    "execution_id": execution_id,
                    "original_message_id": original_message_id,
                    "redrive_message_id": redrive_message_id,
                }
                self.log_entries.append(entry)
                return entry
            
            def get_trace(self, correlation_id: str) -> List[Dict]:
                """íŠ¹ì • correlation_idì˜ ëª¨ë“  ë¡œê·¸ í•­ëª© ì¡°íšŒ"""
                return [
                    entry for entry in self.log_entries 
                    if entry["correlation_id"] == correlation_id
                ]
        
        return CorrelationAwareLogger()
    
    def test_correlation_id_preserved_in_original_event(self):
        """
        originalEventì— correlation_idê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        """
        # DLQ ë©”ì‹œì§€ì— ì €ì¥ë˜ëŠ” í˜•íƒœ
        dlq_message = {
            "originalEvent": {
                "execution_id": "exec-trace-001",
                "correlation_id": "corr-abc-123",  # ì¶”ì ì„ ìœ„í•œ ID
                "ownerId": "user-test",
                "segment_to_run": 3
            },
            "error_info": {
                "error": "ThrottlingException",
                "cause": "API rate limit exceeded"
            },
            "original_message_id": "sqs-msg-original-456"  # ì›ë³¸ SQS ë©”ì‹œì§€ ID
        }
        
        # correlation_id ì¡´ì¬ í™•ì¸
        assert "correlation_id" in dlq_message["originalEvent"]
        assert dlq_message["originalEvent"]["correlation_id"] == "corr-abc-123"
        
        # original_message_id ì¡´ì¬ í™•ì¸
        assert "original_message_id" in dlq_message
    
    def test_log_entry_includes_all_trace_ids(self, correlation_aware_logger):
        """
        ë¡œê·¸ í•­ëª©ì— ëª¨ë“  ì¶”ì  IDê°€ í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
        """
        entry = correlation_aware_logger.log(
            level="INFO",
            message="Redrive attempt started",
            correlation_id="corr-abc-123",
            execution_id="exec-trace-001",
            original_message_id="sqs-original-456",
            redrive_message_id="sqs-redrive-789"
        )
        
        # ëª¨ë“  ì¶”ì  í•„ë“œ ì¡´ì¬
        assert entry["correlation_id"] == "corr-abc-123"
        assert entry["execution_id"] == "exec-trace-001"
        assert entry["original_message_id"] == "sqs-original-456"
        assert entry["redrive_message_id"] == "sqs-redrive-789"
    
    def test_can_trace_full_lifecycle(self, correlation_aware_logger):
        """
        ì‹¤íŒ¨ â†’ DLQ â†’ ë³µêµ¬ ì „ì²´ ìˆ˜ëª…ì£¼ê¸°ë¥¼ í•˜ë‚˜ì˜ íƒ€ì„ë¼ì¸ìœ¼ë¡œ ì¶”ì  ê°€ëŠ¥
        """
        correlation_id = "corr-lifecycle-test"
        execution_id = "exec-lifecycle-001"
        
        # 1. ì›ë³¸ ì‹¤í–‰ ì‹¤íŒ¨
        correlation_aware_logger.log(
            level="ERROR",
            message="Segment execution failed",
            correlation_id=correlation_id,
            execution_id=execution_id,
            original_message_id="sqs-1"
        )
        
        # 2. DLQë¡œ ì´ë™
        correlation_aware_logger.log(
            level="WARN",
            message="Message moved to DLQ",
            correlation_id=correlation_id,
            execution_id=execution_id,
            original_message_id="sqs-1"
        )
        
        # 3. DLQ Redrive ì‹œì‘
        correlation_aware_logger.log(
            level="INFO",
            message="DLQ redrive initiated",
            correlation_id=correlation_id,
            execution_id=execution_id,
            original_message_id="sqs-1",
            redrive_message_id="sqs-2"  # ìƒˆë¡œìš´ SQS ë©”ì‹œì§€ ID
        )
        
        # 4. ë³µêµ¬ ì„±ê³µ
        correlation_aware_logger.log(
            level="INFO",
            message="Redrive successful",
            correlation_id=correlation_id,
            execution_id=execution_id,
            redrive_message_id="sqs-2"
        )
        
        # ì „ì²´ ìˆ˜ëª…ì£¼ê¸° ì¶”ì 
        trace = correlation_aware_logger.get_trace(correlation_id)
        
        assert len(trace) == 4
        assert trace[0]["message"] == "Segment execution failed"
        assert trace[1]["message"] == "Message moved to DLQ"
        assert trace[2]["message"] == "DLQ redrive initiated"
        assert trace[3]["message"] == "Redrive successful"
    
    def test_correlation_id_fallback_to_execution_id(self, correlation_aware_logger):
        """
        correlation_idê°€ ì—†ìœ¼ë©´ execution_idë¥¼ ëŒ€ì²´ ì‚¬ìš©
        """
        entry = correlation_aware_logger.log(
            level="INFO",
            message="Processing without explicit correlation_id",
            execution_id="exec-fallback-001"  # correlation_id ì—†ìŒ
        )
        
        # execution_idê°€ correlation_idë¡œ ì‚¬ìš©ë¨
        assert entry["correlation_id"] == "exec-fallback-001"


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
